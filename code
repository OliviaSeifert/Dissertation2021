library(readr)
#brides 18
urlfile18="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides18full.csv"
bride18data=read_csv(url(urlfile18))
head(bride18data)
#brides 19
urlfile19="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides19full.csv"
bride19data=read_csv(url(urlfile19))
head(bride19data)
#brides 20
urlfile20="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides20full.csv"
bride20data=read_csv(url(urlfile20))
head(bride20data)
#brides 21
urlfile21="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides21full.csv"
bride21data=read_csv(url(urlfile21))
head(bride21data)
#brides 22
urlfile22="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides22full.csv"
bride22data=read_csv(url(urlfile22))
head(bride22data)
#brides 23
urlfile23="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides23full.csv"
bride23data=read_csv(url(urlfile23))
head(bride23data)
#brides 24
urlfile24="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides24full.csv"
bride24data=read_csv(url(urlfile24))
head(bride24data)
#bride 25
urlfile25="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides25full.csv"
bride25data=read_csv(url(urlfile25))
head(bride25data)
#bride 26
urlfile26="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides26full.csv"
bride26data=read_csv(url(urlfile26))
head(bride26data)
#bride 27
urlfile27="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides27full.csv"
bride27data=read_csv(url(urlfile27))
head(bride27data)
#bride 28
urlfile28="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides28full.csv"
bride28data=read_csv(url(urlfile28))
head(bride28data)
#bride 29
urlfile29="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides29full.csv"
bride29data=read_csv(url(urlfile29))
head(bride29data)
#bride 30
urlfile30="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides30full.csv"
bride30data=read_csv(url(urlfile30))
head(bride30data)
#bride 31
urlfile31="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides31full.csv"
bride31data=read_csv(url(urlfile31))
head(bride31data)
#bride 32
urlfile32="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides32full.csv"
bride32data=read_csv(url(urlfile32))
head(bride32data)
#bride 33
urlfile33="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides33full.csv"
bride33data=read_csv(url(urlfile33))
head(bride33data)
#bride 34
urlfile34="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides34full.csv"
bride34data=read_csv(url(urlfile34))
head(bride34data)
#bride 35
urlfile35="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides35full.csv"
bride35data=read_csv(url(urlfile35))
head(bride35data)
#bride 36
urlfile36="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides36full.csv"
bride36data=read_csv(url(urlfile36))
head(bride36data)
#bride 37
urlfile37="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides37full.csv"
bride37data=read_csv(url(urlfile37))
head(bride37data)
#bride 38
urlfile38="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides38full.csv"
bride38data=read_csv(url(urlfile38))
head(bride38data)
#bride 39
urlfile39="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides39full.csv"
bride39data=read_csv(url(urlfile39))
head(bride39data)
#bride 40
urlfile40="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides40full.csv"
bride40data=read_csv(url(urlfile40))
head(bride40data)
#bride 41
urlfile41="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides41full.csv"
bride41data=read_csv(url(urlfile41))
head(bride41data)
#bride 42
urlfile42="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides42full.csv"
bride42data=read_csv(url(urlfile42))
head(bride42data)
#bride 43
urlfile43="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides43full.csv"
bride43data=read_csv(url(urlfile43))
head(bride43data)
#bride 44
urlfile44="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides44full.csv"
bride44data=read_csv(url(urlfile44))
head(bride44data)
#bride 45
urlfile45="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides45full.csv"
bride45data=read_csv(url(urlfile45))
head(bride45data)
#bride 46
urlfile46="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides46full.csv"
bride46data=read_csv(url(urlfile46))
head(bride46data)
#bride 47
urlfile47="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides47full.csv"
bride47data=read_csv(url(urlfile47))
head(bride47data)
#bride 48
urlfile48="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides48full.csv"
bride48data=read_csv(url(urlfile48))
head(bride48data)
#bride 49
urlfile49="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides49full.csv"
bride49data=read_csv(url(urlfile49))
head(bride49data)
#bride 50
urlfile50="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides50full.csv"
bride50data=read_csv(url(urlfile50))
head(bride50data)
#bride 51
urlfile51="https://raw.githubusercontent.com/deadofied/ac299r/master/data/brides51full.csv"
bride51data=read_csv(url(urlfile51))
head(bride51data)
#groom 18
urlfileG18="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms18full.csv"
groom18data=read_csv(url(urlfileG18))
head(groom18data)
#groom 19
urlfileG19="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms19full.csv"
groom19data=read_csv(url(urlfileG19))
head(groom19data)
#groom 20
urlfileG20="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms20full.csv"
groom20data=read_csv(url(urlfileG20))
head(groom20data)
#groom 21
urlfileG21="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms21full.csv"
groom21data=read_csv(url(urlfileG21))
head(groom21data)
#groom 22
urlfileG22="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms22full.csv"
groom22data=read_csv(url(urlfileG22))
head(groom22data)
#groom 23
urlfileG23="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms23full.csv"
groom23data=read_csv(url(urlfileG23))
head(groom23data)
#groom 24
urlfileG24="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms24full.csv"
groom24data=read_csv(url(urlfileG24))
head(groom24data)
#groom 25
urlfileG25="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms25full.csv"
groom25data=read_csv(url(urlfileG25))
head(groom25data)
#groom 26
urlfileG26="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms26full.csv"
groom26data=read_csv(url(urlfileG26))
head(groom26data)
#groom 27
urlfileG27="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms27full.csv"
groom27data=read_csv(url(urlfileG27))
head(groom27data)
#groom 28
urlfileG28="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms28full.csv"
groom28data=read_csv(url(urlfileG28))
head(groom28data)
#groom 29
urlfileG29="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms29full.csv"
groom29data=read_csv(url(urlfileG29))
head(groom29data)
#groom 30
urlfileG30="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms30full.csv"
groom30data=read_csv(url(urlfileG30))
head(groom30data)
#groom 31
urlfileG31="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms31full.csv"
groom31data=read_csv(url(urlfileG31))
head(groom31data)
#groom 32
urlfileG32="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms32full.csv"
groom32data=read_csv(url(urlfileG32))
head(groom32data)
#groom 33
urlfileG33="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms33full.csv"
groom33data=read_csv(url(urlfileG33))
head(groom33data)
#groom 34
urlfileG34="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms34full.csv"
groom34data=read_csv(url(urlfileG34))
head(groom34data)
#groom 35
urlfileG35="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms35full.csv"
groom35data=read_csv(url(urlfileG35))
head(groom35data)
#groom 36
urlfileG36="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms36full.csv"
groom36data=read_csv(url(urlfileG36))
head(groom36data)
#groom 37
urlfileG37="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms37full.csv"
groom37data=read_csv(url(urlfileG37))
head(groom37data)
#groom 38
urlfileG38="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms38full.csv"
groom38data=read_csv(url(urlfileG38))
head(groom38data)
#groom 39
urlfileG39="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms39full.csv"
groom39data=read_csv(url(urlfileG39))
head(groom39data)
#groom 40
urlfileG40="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms40full.csv"
groom40data=read_csv(url(urlfileG40))
head(groom40data)
#groom 41
urlfileG41="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms41full.csv"
groom41data=read_csv(url(urlfileG41))
head(groom41data)
#groom 42
urlfileG42="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms42full.csv"
groom42data=read_csv(url(urlfileG42))
head(groom42data)
#groom 43
urlfileG43="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms43full.csv"
groom43data=read_csv(url(urlfileG43))
head(groom43data)
#groom 44
urlfileG44="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms44full.csv"
groom44data=read_csv(url(urlfileG44))
head(groom44data)
#groom 45
urlfileG45="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms45full.csv"
groom45data=read_csv(url(urlfileG45))
head(groom45data)
#groom 46
urlfileG46="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms46full.csv"
groom46data=read_csv(url(urlfileG46))
head(groom46data)
#groom 47
urlfileG47="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms47full.csv"
groom47data=read_csv(url(urlfileG47))
head(groom47data)
#groom 48
urlfileG48="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms48full.csv"
groom48data=read_csv(url(urlfileG48))
head(groom48data)
#groom 49
urlfileG49="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms49full.csv"
groom49data=read_csv(url(urlfileG49))
head(groom49data)
#groom 50
urlfileG50="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms50full.csv"
groom50data=read_csv(url(urlfileG50))
head(groom50data)
#groom 51
urlfileG51="https://raw.githubusercontent.com/deadofied/ac299r/master/data/grooms51full.csv"
groom51data=read_csv(url(urlfileG51))
head(groom51data)

#make 2 matrimonal datasets containing all females and all males 
allbrides=rbind(bride18data, bride19data, bride20data, bride21data, bride22data, bride23data, bride24data, bride25data, bride26data, bride27data, bride28data, bride29data, bride30data, bride31data, bride32data, bride33data, bride34data, bride35data, bride36data, bride37data, bride38data, bride39data, bride40data, bride41data, bride42data, bride43data, bride44data, bride45data, bride46data, bride47data, bride48data, bride49data, bride50data, bride51data)
allgrooms=rbind(groom18data, groom19data, groom20data, groom21data, groom22data, groom23data, groom24data, groom25data, groom26data, groom27data, groom28data, groom29data, groom30data, groom31data, groom32data, groom33data, groom34data, groom35data, groom36data, groom37data, groom38data, groom39data, groom40data, groom41data, groom42data, groom43data, groom44data, groom45data, groom46data, groom47data, groom48data, groom49data, groom50data, groom51data)
#cut down male and female datasets to only include caste,complexion,city,age,etc
cutdownbrides=allbrides[, c(4, 5, 6, 7, 8, 12, 13, 14, 16, 17, 18, 32, 52, 53, 59, 60, 61)]
cutdowngrooms=allgrooms[, c(4, 5, 6, 7, 8, 12, 13, 14, 16, 17, 18, 32, 52, 53, 59, 60, 61)]
#add together 
cutdown_bridesandgrooms=rbind(cutdownbrides, cutdowngrooms)

#create df with only castes with 30+ datapoints
#before doing any tables etc. remove certain individuals
#first see if bad idea to remove individuals not born in India by checking prop of people putting NA
cutdown_bridesandgrooms$birth3 <- as.character(cutdown_bridesandgrooms$birth3)
cutdown_bridesandgrooms$birth3[cutdown_bridesandgrooms$birth3==""] <- NA
cutdown_bridesandgrooms$birth3 <- as.factor(cutdown_bridesandgrooms$birth3)
sum(is.na(cutdown_bridesandgrooms$birth3))/nrow(cutdown_bridesandgrooms)
#30% have "NA" for birth3, can't remove all those without birth3=="India"
#now see if should remove all those without birth3=India, excluding all those that have NA
cutdown_bridesandgrooms_India=cutdown_bridesandgrooms[with(cutdown_bridesandgrooms, birth3=="India"|is.na(birth3)), ]
(nrow(cutdown_bridesandgrooms)-nrow(cutdown_bridesandgrooms_India))/nrow(cutdown_bridesandgrooms)
#3.2% of data removed, ok to proceed 
##now need to remove all individuals with more than one caste1 or caste2 preference 
#first need to split caste2 into caste2_1 etc.
library(data.table) 
cutdown_bridesandgrooms_caste1split=setDT(cutdown_bridesandgrooms_India)[, paste0("caste1", 1:99) := tstrsplit(caste1, " - |,")]
#repeat for caste2
cutdown_bridesandgrooms_caste1and2split=setDT(cutdown_bridesandgrooms_caste1split)[, paste0("caste2", 100:199) := tstrsplit(caste2, " - |,")]
#remove individuals who have info for more than one caste for caste2
cutdown_bridesandgrooms_caste1and2split_new=cutdown_bridesandgrooms_caste1and2split[is.na(cutdown_bridesandgrooms_caste1and2split$caste2102),]
#remove individuals who have info for more than one caste for caste1
cutdown_bridesandgrooms_caste1and2split_new2=cutdown_bridesandgrooms_caste1and2split_new[is.na(cutdown_bridesandgrooms_caste1and2split_new$caste13),]
#check removal of individuals with more than one caste1 or caste2 answer isn't too damaging
(nrow(cutdown_bridesandgrooms_caste1and2split)-nrow(cutdown_bridesandgrooms_caste1and2split_new2))/nrow(cutdown_bridesandgrooms_caste1and2split)
#--> removes 3.12% of data: ok to proceed
#now make freq table for caste1 with new dataset with 3.12% individuals removed
library(dplyr)
cutdown_bridesandgrooms_cleaned_freqtable = cutdown_bridesandgrooms_caste1and2split_new2 %>% dplyr:: count(caste1)
#now add pop size of freq table to main dataset through merging
cutdown_bridesandgrooms_cleaned_withfreq=merge(cutdown_bridesandgrooms_caste1and2split_new2, cutdown_bridesandgrooms_cleaned_freqtable, by="caste1")
#then create subset containing only individuals from castes with 30plus datapoints
castes30plus_only=subset(cutdown_bridesandgrooms_cleaned_withfreq, n>=30)
#remove unecessary split columns
castes30plus_only_new=castes30plus_only[, c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 118, 119, 218)]
#rename column titles
colnames(castes30plus_only_new) <- c("caste1", "age2", "birth1", "birth2", "birth3", "birth4", "caste2", "city2", "complexion1", "complexion2", "created_for", "gotra", "religion1", "religion2", "status1", "status2", "subcaste","source", "caste1_1", "caste1_2", "caste2_1", "caste2_2", "n_caste1")

###now merge all cities and states spreadsheet with castes30plus_only_new datatable to give state for each individual 
alltownsandstates_India=read.csv("all_towns_and_states_India.csv", na.strings = " ", header=TRUE)
#rename column names 
colnames(alltownsandstates_India)=c("Sl. No.", "birth4", "Urban Status", "State Code", "State", "District Code", "District")
#remove all towns/cities which have rows with duplicates
alltownsandstates_India_new = alltownsandstates_India %>% distinct(birth4, .keep_all = TRUE) 
#rename New Delhi Municipal Council as Delhi in townsandstates dataframe to match matrimonial and rename state from Delhi* to Delhi
alltownsandstates_India_new$birth4[alltownsandstates_India_new$birth4=="New Delhi Municipal Council"]="Delhi"
alltownsandstates_India_new$State[alltownsandstates_India_new$State=="Delhi*"]="Delhi"
#rename Greater Mumbai as Mumbai in townsandstates dataframe to match matrimonial name
alltownsandstates_India_new$birth4[alltownsandstates_India_new$birth4=="Greater Mumbai"]="Mumbai"
#rename Udhagamandalam to Ooty
alltownsandstates_India_new$birth4[alltownsandstates_India_new$birth4=="Udhagamandalam"]="Ooty"
#set birth4 to NA for blanks in matrimonial dataset
castes30plus_only_new$birth4 <- as.character(castes30plus_only_new$birth4)
castes30plus_only_new$birth4[castes30plus_only_new$birth4==""] <- NA
castes30plus_only_new$birth4 <- as.factor(castes30plus_only_new$birth4)
#now merge to have castes30plus group with states
castes30plus_only_new_andstates=merge(castes30plus_only_new, alltownsandstates_India_new, by="birth4", all.x=TRUE)
#remove useless columns 
castes30plus_only_new_andstates2=castes30plus_only_new_andstates[, c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,27)]

##to assign state to those without birth4 info, create list of modal State for each caste
#blanks in castes30plus_only_new_andstates already been put as NA for birth4, but need to do for caste1
castes30plus_only_new_andstates2$caste1 <- as.character(castes30plus_only_new_andstates2$caste1)
castes30plus_only_new_andstates2$caste1[castes30plus_only_new_andstates2$caste1==""] <- NA
castes30plus_only_new_andstates2$caste1 <- as.factor(castes30plus_only_new_andstates2$caste1)
#can proceed for finding mode of birth4 for each caste1
# Mode function
calculate_mode <- function(x) {
  uniqx <- na.omit(unique(x))
  uniqx[which.max(tabulate(match(x, uniqx)))]
}
#run mode function for state
library(dplyr)
library(modeest)

castes30plus_only_new_andstates2_caste1_by_State_mode=castes30plus_only_new_andstates2 %>%  
  dplyr:: group_by(caste1) %>%
  dplyr:: summarise(State = calculate_mode(State))

colnames(castes30plus_only_new_andstates2_caste1_by_State_mode)=c("caste1", "State")

#now merge state mode datatable, with main dataframe
castes30plus_only_new_andstates3=merge(castes30plus_only_new_andstates2, castes30plus_only_new_andstates2_caste1_by_State_mode, by="caste1", all.x = TRUE)
#rename columns 
colnames(castes30plus_only_new_andstates3)=c("caste1", "birth4", "age2", "birth1","birth2", "birth3", "caste2", "city2", "complexion1", "complexion2", "created_for", "gotra,", "religion1", "religion2", "status1", "status2", "subcaste", "source", "caste1_1", "caste1_2", "caste2_1", "caste2_2", "n_caste1", "State", "Modal_State_by_caste1")
#now assign State as Modal_State_by_caste1 when state=NA
#first ascribe NA to blanks in State
castes30plus_only_new_andstates3$State <- as.character(castes30plus_only_new_andstates3$State)
castes30plus_only_new_andstates3$State[castes30plus_only_new_andstates3$State==""] <- NA
castes30plus_only_new_andstates3$State <- as.factor(castes30plus_only_new_andstates3$State)

#now do assigning of states when state is NA
library(dplyr)

castes30plus_only_new_andstates4 = castes30plus_only_new_andstates3 %>% 
  mutate(State = coalesce(State,Modal_State_by_caste1))
#remove Delhi* and replace as Delhi in the matrimonial dataset in both andstates4 and andstates3 for later
castes30plus_only_new_andstates4$State=gsub("\\*","",castes30plus_only_new_andstates4$State)
castes30plus_only_new_andstates3$State=gsub("\\*","",castes30plus_only_new_andstates3$State)

#now make caste1_1 by state freq table
caste1_1_by_state_freqtable=data.frame(table(castes30plus_only_new_andstates4$caste1_1, castes30plus_only_new_andstates4$State))
caste1_1_by_state_freqtable_new=subset(caste1_1_by_state_freqtable, Freq!=0)
caste1_1_by_state_freqtable_new=caste1_1_by_state_freqtable_new[order(caste1_1_by_state_freqtable_new$Var1), ]
#nename columns
colnames(caste1_1_by_state_freqtable_new)=c("caste1_1", "State", "freq")

#make same spreadsheet but for state and freq by caste1 (not caste1_1)
caste1_by_state_freqtable=data.frame(table(castes30plus_only_new_andstates4$caste1, castes30plus_only_new_andstates4$State))
caste1_by_state_freqtable_new=subset(caste1_by_state_freqtable, Freq!=0)
caste1_by_state_freqtable_new=caste1_by_state_freqtable_new[order(caste1_by_state_freqtable_new$Var1), ]
#nename columns
colnames(caste1_by_state_freqtable_new)=c("caste1", "State", "freq")




##can now start constructing df with caste, matrimonial pop size, genetic popsize (+MI to add) for castes with 30+ datapoints
#make new freq table for castes30plus_only_new dataset using just caste1_1
library(plyr)
castes30plus_only_new_freqtable = castes30plus_only_new_andstates4 %>%
  dplyr:: count(caste1_1)

#import table of genetic samples with SNP
genetic_sample_table_SNP=read.csv("Diss_genetic_samples_withSNP.csv")
#remove bracketed name and export results
write.table(sub(" \\(.*", "", genetic_sample_table_SNP$ï..Population.name), "genetic_sample_table_no_brackets", sep=";")
#now import csv containing no brackets for cleaner merge
genetic_sample_table_SNP_clean=read.csv("Diss_genetic_samples_withSNP_nobrackets.csv")
#rename column titles
colnames(genetic_sample_table_SNP_clean) <- c("caste1_1", "Genetic pop size")
#merge genetic and matrimonial castes 30 plus datasets together
matrimonial30plus_genetic_withSNP_dt=merge(castes30plus_only_new_freqtable, genetic_sample_table_SNP_clean, by="caste1_1", all.y=TRUE)
#rename column titles
colnames(matrimonial30plus_genetic_withSNP_dt) <- c("caste1_1", "Matrimonial pop size", "Genetic pop size")



#preliminary analysis on matrimonial dataset - preferences for caste endogamy

#look at proportion of those who want to marry someone of same caste as themselves
#first cut out those in the dataset that don't include caste1_1 and caste2_1
castes30plus_new_caste1_1=castes30plus_only_new[!is.na(castes30plus_only_new$caste1_1),]
castes30plus_new_caste1_1_caste2_1=castes30plus_new_caste1_1[!is.na(castes30plus_new_caste1_1$caste2_1),]
#now calculate proportions for new dataset which contains all those with answers to both caste1 and caste2
WantingtomarryIN=nrow(subset(castes30plus_new_caste1_1_caste2_1, caste1_1==caste2_1))/nrow(castes30plus_new_caste1_1_caste2_1)
WantingtomarryOUT=nrow(subset(castes30plus_new_caste1_1_caste2_1, caste1_1!=caste2_1))/nrow(castes30plus_new_caste1_1_caste2_1)
#83% want to marry within, 17% want to marry out 
#VISUALISATION
MarriageINorOUTcaste=c(WantingtomarryIN, WantingtomarryOUT)
barplot(MarriageINorOUTcaste, main="Proportion of individuals wanting to marry within or outside of their own caste", names.arg = c("Want to marry within caste", "Want to marry out of caste"), ylab="Proportion of Individuals", col=c("limegreen", "orange"), ylim=c(0,1))
#find binomial probability 
testing alternative to binomial 
obs=length(which(castes30plus_new_caste1_1_caste2_1$caste1_1 == castes30plus_new_caste1_1_caste2_1$caste2_1)) #how many match
#original proportion:
obs.prop <- obs/length(na.omit(castes30plus_new_caste1_1_caste2_1$caste2_1)) #matching values over sample size of caste2, which is the maximum number of matches
difcount = 0 #start with 0
simcount = 10000 #number of simulations
for (i in 1:simcount) {
  castes30plus_new_caste1_1_caste2_1.tmp <- castes30plus_new_caste1_1_caste2_1 #copy the original data
  castes30plus_new_caste1_1_caste2_1.tmp <- transform(castes30plus_new_caste1_1_caste2_1.tmp, caste1=sample(caste1_1)) #randomly sort caste1
  obs.null <- length(which(castes30plus_new_caste1_1_caste2_1.tmp$caste1_1 == castes30plus_new_caste1_1_caste2_1.tmp$caste2_1)) #how many match between the unsorted and sorted columns
  #how often is obs > null_distribution - proportion #-proportion? 
  if (obs > obs.null - obs.prop) { #how often is the original observation greater than the shuffled distribution minus the original proportion
    difcount = difcount + 1 #if it is greater add it to our count
  }
}  
#simulated proportion
sim.prop <- difcount/simcount
#redo the binom test but this time with the sim.prop
binom.test(x=obs, n=length(na.omit(castes30plus_new_caste1_1_caste2_1$caste2_1)), p=obs.prop) #p=1 because x/n = obs.prop
binom.test(x=obs, n=length(na.omit(castes30plus_new_caste1_1_caste2_1$caste2_1)), p=sim.prop) #with the sim prop, much lower p value, but very dependent on what happens with the transform function above




#make map of cities in to visualize sample distribution of those in castes30plus_only_new, coords obtained at https://geocode.localfocus.nl/ and not yet manually corrected them
#now find number of cities in  dataset
castes30plus_only_new_citylist=sort(unique(castes30plus_only_new$birth4))
#make freq table as well for point sizing in map later 
library(plyr)
castes30plus_only_new_freqtable = castes30plus_only_new %>%
  dplyr:: count(birth4)
#run coords in the geodata website for coords and copy and paste into excel spreadsheet, then add freq column to it
#load in complete geodata 
fullgeodata_castes30plus_bornIndia=read.csv("castes30plus_only_new_bornIndia_raw.csv")
#now plot map
library(mapproj)
castes30_map=map(database="world", ylim=c(20, 21), xlim=c(70, 70), col="grey80", fill=TRUE, projection="gilbert", orientation= c(90, 0, 90))
long=c(78.54,91.28,78.01,72.57,74.73,73.69,92.72,74.64,82.54,77.01,76.34,78.08,74.33,81.84,79.65,94.8,76.61,76.78,83.2,77.77,71.22,74.87,78.35,72.96,77.6,85.09,81.69,87.26,84.67,84.78,77.73,79.55,75.34,83.25,79.13,75.65,79.77,88.25,81.59,91.39,80.18,83.49,86.92,84.03,82.4,88.78,80.34,72.84,77.61,86.92,86.79,74.44,81.33,76.42,88.52,87.86,79.42,83.62,71.41,75.55,91.04,74.9,82.29,74.94,86.13,74.52,76.93,77.9,83.58,86.49,86.98,79.65,77.33,73.01,72.15,83.17,81.35,74.63,78.79,76.13,77.44,85.81,69.66,77.55,85.52,75.71,78.14,73.31,82.15,93.82,86.46,90.55,84.14,77.84,75.64,76.24,83.98,75.81,92.77,85.81,76.94,76.12,80.07,93.24,83.21,94.1,76.79,79.3,96.46,84.87,80.2,84.7,79.59,85,78.94,78.05,75.77,90.59,76.39,81.1,78.95,93.67,74.61,76.99,79.75,85.87,74.26,72.84,79.43,80.79,85.9,88.26,78.46,76.33,75.91,85.52,78.03,81.33,86.74,83.78,76.05,81.55,86.43,75.3,78.1,75.16,94.58,85.6,77.7,89.97,74.77,94.91,93.73,77.98,80.99,93.43,91.82,70.98,87.3,73.71,81.29,81.11,88.14,77.71,78.77,79.03,82.15,77.31,74.76,75.45,79.63,76.4,80.8,78.39,74.62,75.88,80.14,72.65,73.87,88.61,83.67,84.98,77.44,83.86,86.31,90.62,87.21,73.63,93.97,81.99,80.19,84.44,83.37,77.51,76.55,84.54,77.31,80.42,75.33,77.06,91.77,78.17,82.47,93.02,92.57,85.23,79.52,80.16,74.29,77.1,80.13,78.07,76.1,78.05,75.4,85.36,77.15,75.73,77.71,75.9,88.19,75.14,78.4,93.94,75.86,93.62,79.92,82.03,75.79,70.91,75.61,75.57,75.9,89.14,70.05,86.18,86.23,83.92,82.69,85.03,74.54,76.65,76.16,78.57,84.02,75.4,76.31,73.02,94.21,70.45,78.83,92,76.42,82.23,76.09,73.13,91.97,91.29,79.71,81.49,79.92,75.37,80.32,77.34,75.38,79.84,77.03,92.36,79.12,77,78.24,74.13,74.94,87.57,80.4,81.45,72.64,81.15,86.42,85.66,86.48,83.07,80.16,76.35,75.62,72.69,80.8,95.57,87.85,76.33,94.1,90.26,78.13,92.68,74.24,88.42,76.6,76.16,82.71,82.71,82.33,75.84,76.53,88.48,77.11,78.01,76.81,80.78,86.09,78.4,72.83,76.57,84.68,73.44,80.94,75.84,92.74,81.13,86.88,86.07,75.74,78.13,83.12,82.1,78.14,75.54,79.87,79.03,76.08,81.89,77.13,76.94,80.37,75.07,76.89,92.03,74.85,88.52,75.4,92.34,77.69,83.62,77.72,72.39,77.01,82.6,75.16,76.71,94.52,95.06,78.78,78,84.92,72.88,86.48,77.71,85.39,76.64,82.3,92.7,79.84,73.72,77.41,79.07,77.29,79.46,91.39,79.27,78.17,88.36,77.31,74.25,81.15,76.11,79.04,73.78,73.03,73.15,85.54,77.43,85.01,75.11,79.98,78.6,78.11,77.35,91.88,82.54,80.05,76.71,79.46,76.28,77.18,76.96,87.66,76.65,72.43,72.77,73.33,76.96,76.97,73.82,80.19,76.76,95.33,72.13,76.78,76.39,85.11,78.88,79.8,80.21,69.62,93.96,92.73,74.78,79.8,78.82,73.84,85.84,87.47,86.37,81.25,77.35,83.38,88.12,83.4,81.65,77.6,75.39,70.79,80.74,73.5,73.86,77.28,78.83,77.63,85.33,75.05,73.3,83.47,78.26,75.91,78.49,93.72,91.74,77.63,81.7,73.02,72.86,76.94,73.91,72.54,76.34,77.19,77.31,85.31,77.61,76.7,72.19,78.36,73.81,72.43,91.89,92.89,94.56,85.9,79.79,80.23,76.95,88.36,73.82,91.59,80.57,79.18,88.54,81.29,76.62,83.06,76.6,84.86,79.04,79.44,76.53,78.75,77.56,86.6,87.65,78.15,85.78,83.97,74.43,75.85,74,80.83,76.36,78.49,77.21,94.11,79.55,92.85,81.36,79.91,76.28,85.86,85.3,76.7,91.89,77.17,75.58,78.44,81.88,94.21,81.88,75.15,92.8,88.42,75.15,82.63,72.75,75.03,85.49,83.29,78.49,84.49,77.1,75.91,77.08,83.83,78.78,83.82,82.07,84.03,86.6,72.82,71.63,93.49,87.93,79.12,73.4,74.93,91.76,92.79,96.2,72.99,77.48,79.91,76.95,79.63,78.13,94,76.12,78.83,95.35,77.74,79.07,75.79,78.69,94.82,77.12,90.23,76.71,73.71,74.74,75.79,94.43,80.84,71.04,80.46,78.44,73.19,72.94,83,79.14,77.81,80.64,79.49,77.96,83.32,83.41,79.59,78.6,77.14,94.26,77.29,82.2,78.13,93.84,94.52)
lat=c(19.67,23.84,27.19,23.03,19.11,20.76,23.74,26.47,26.43,20.7,9.5,27.9,22.23,25.45,29.59,28.17,27.56,30.38,23.12,20.95,21.6,31.64,28.84,22.56,14.69,20.83,23.1,26.31,25.55,25.14,24.57,26.63,19.88,26.25,28.04,16.18,29.84,24.1,27.58,26.67,21.82,20.7,21.5,25.9,27.32,25.22,25.48,19.06,12.97,24.88,22.97,23.55,26.95,24.86,22.74,23.24,28.35,21.35,25.76,30.37,26.34,22.03,26.81,30.21,25.42,15.87,15.14,21.9,25.03,21.06,25.25,21.16,27.48,21.71,21.76,19.91,21.2,25.33,26.57,28.8,23.24,20.26,23.25,17.9,25.2,16.83,29.38,28.01,22.08,24.47,23.64,26.48,20.72,28.41,25.45,21.32,25.57,11.28,9.19,22.54,11.92,32.56,29.31,23.56,24.94,24.22,30.73,19.95,27.47,24.2,13.03,25.79,24.92,19.36,22.06,13.17,13.32,26.65,14.23,25.2,13.48,24.33,28.06,11.01,11.75,20.47,22.84,20.43,23.83,18.88,26.14,27.04,25.67,26.89,14.35,20.81,30.32,24.73,24.34,26.5,22.97,20.71,23.8,22.59,12.02,15.38,27.48,20.66,26.68,26.02,20.92,27.48,25.9,10.36,23,25.84,26.13,20.71,24.29,23.84,21.2,16.71,25.02,11.34,27.51,26.78,26.77,28.42,30.72,29.51,27.36,30.6,25.92,27.15,30.93,15.09,19.68,23.23,29.9,27.33,24.15,24.7,28.67,25.59,24.18,26.16,24.82,22.78,26.51,27.16,21.47,26.47,26.75,28.46,17.24,23.04,24.64,16.32,31.9,28.46,26.17,26.2,25.33,25.16,24.68,25.69,29.22,25.96,29.61,22.34,27.39,29.95,13.01,27.6,14.79,24,19.72,29.15,22.75,31.53,22.58,15.36,17.46,24.81,22.71,27.09,23.17,19.09,26.9,26.91,31.31,20.99,19.85,26.71,22.47,22.8,24.93,22.83,25.75,25.15,22.9,28.61,24.59,25.45,21.86,28.13,29.32,26.26,26.76,21.52,14.48,24.33,29.81,16.96,11.69,19.25,26.12,26,12.83,20.27,27.05,11.88,26.47,8.32,31.38,10.92,26.5,24.87,18.43,29.68,10.96,14.82,12.62,25.53,23.83,25.47,10.57,22.11,20.5,21.51,25.51,26.77,17.24,21.83,21.82,22.75,27.9,27.02,26.27,9.96,25.68,26.4,13.13,24.22,16.69,22.61,8.9,15.35,18.81,22.36,23.55,25.17,9.59,23.37,31.96,15.5,30.07,27.95,25.18,24.69,19.06,18.4,23.43,18.75,26.86,30.89,22.89,16.18,25.98,26.35,12.42,9.93,26.26,21.11,15.88,11.72,25.29,27.23,11.05,18.36,32.03,31.7,22.6,24.07,12.53,26.44,12.9,27.51,29.99,26.38,27.5,25.99,28.98,23.62,27.96,25.05,30.82,30.71,26.33,26.74,28.84,26.5,26.66,19.1,25.38,29.47,26.12,12.32,19.62,26.31,10.77,27.2,8.18,21.15,30.56,29.39,26.67,17.06,11.22,27.16,19.14,21.38,19.5,28.06,22.97,19.99,19.08,20.75,24.89,17.22,20.22,24.57,14.44,30.48,18.67,28.57,25.88,20.59,15.51,11.4,25.98,17.82,32.26,9.86,24.58,10.77,24.18,19.7,25.77,30.68,29.38,15.48,24.72,19.29,28.05,23.85,9.27,30.34,25.62,11.24,28.64,29.58,21.64,24.81,11.67,24.03,10.94,10.37,18.54,19.81,25.78,23.33,26.22,16.2,21.92,25.61,21.9,21.24,23.25,28.64,22.29,21.05,21.92,25.05,12.72,9.37,31.45,23.38,23.35,16.99,19.37,31.53,22.76,17.43,27.15,26.19,12.94,21.21,20.24,20.4,11.92,15.28,23.03,29.14,32.25,34.03,23.35,12.88,8.74,10.87,22.95,18.52,23.03,25.57,22.53,26.1,20.48,11.93,12.98,27.06,27.31,15.5,23.69,26.88,30.21,22.96,24.53,28.19,24.68,28.89,22.24,30.53,28.96,30.97,23.86,29.96,25.87,25.24,11.67,25.86,21.47,17.09,30.23,17.7,24.57,26.02,17.48,23.25,25.35,22.09,23.29,23.29,27.87,23.43,25.13,26.53,25.67,25.58,31.1,13.93,30.11,27.68,26.76,24.39,27.61,24.82,26.73,24.48,24.11,24.99,29.53,26.59,22.82,9.86,26.1,30.9,17.67,29.01,18.29,30.22,20.96,26.27,22.12,26.12,21.2,22.72,24.95,22.3,10.76,21.05,31.45,27.64,26.63,28.01,19.22,10,13.14,8.51,10.77,8.78,24.64,10.45,24.74,27.49,8.7,12.26,26.17,10.81,26.24,13.33,25.53,11.4,24.58,13.35,23.17,24.99,23.53,20.82,26.5,30.73,22.31,20.61,25.29,12.93,23.52,16.51,11.93,9.59,17.73,18.11,17.97,20.74,20.13,26.1,30.13,16.73,20.38,27.59,25.97)
freq=c(124,140,1067,2075,454,13,27,459,42,240,301,475,13,1070,192,9,224,506,55,264,67,761,58,206,222,55,16,19,119,8,23,27,309,148,99,100,27,193,45,5,59,37,139,178,33,23,68,17,3558,19,73,37,63,22,60,200,536,31,23,35,14,14,94,216,111,448,191,78,16,81,218,77,110,125,198,21,249,82,39,196,732,490,122,94,148,136,202,170,263,10,302,33,5,334,39,52,50,357,1,22,12,32,7,6,9,3,1085,166,3,8,3046,149,58,20,114,40,116,1,69,11,299,4,53,852,152,357,66,12,41,11,208,48,27,25,153,12,693,13722,42,129,55,25,382,28,121,109,5,42,36,19,161,98,12,125,4,5,8,4,21,71,92,156,10,302,123,140,146,710,51,50,102,22,90,120,177,83,11,83,85,17,19,214,790,110,45,7,21,61,18,84,68,71,451,16,167,10,42,503,186,342,311,551,3,14,10,77,78,185,52,31,94,298,141,73,48,100,18,311,59,227,233,305,2468,44,750,17,710,43,1031,12,681,378,68,48,217,686,23,6,133,22,17,64,30,357,34,102,88,348,43,161,229,16,106,252,31,385,3,3,111,10,48,374,1651,96,51,26,18,25,194,284,86,114,85,47,65,10,4,2,50,27,27,12,153,79,36,69,23,1,32,610,9,8,153,1,480,4052,376,34,50,70,12,361,371,64,17,224,113,80,15,52,1,147,4,60,1885,905,1,74,29,92,58,572,12,16,88,15,17,94,112,5,9,117,29,56,98,5,492,1,51,4,328,57,995,99,6,110,98,42,2,3,379,36,93,9832,110,384,256,492,17,51,63,269,169,1094,30,160,11,113,91,1,125,35,2,58,40,698,81,90,32,44,32,52,288,7,107,75,1,6,178,98,75,54,1338,4,3,378,56,28,82,44,239,176,35,62,5,116,216,405,1229,21,72,64,56,1,56,80,203,63,2222,92,59,55,155,107,118,23,66,333,14,37,444,32,19,18,14,54,111,486,106,201,16,2,8,547,8,79,298,56,2,1,28,119,289,162,89,22,79,257,361,1,181,292,2,3,1,4,166,21,246,229,2,434,5,474,199,344,117,90,22,336,224,16,52,46,170,353,49,12,418,100,94,322,95,365,112,43,259,27,4,33,1,62,107,28,14,4,13,94,199,190,52,2,23,32,69,96,150,6,33,26,108,56,92,35,77,69,389,289,75,253,10,105,15,14,446,87,12,32,282,6,21,2,64,6,618,78,48,644,69,143,3,484,28,54,329,118,26,172,3,150,11,15,244,210,192,3,9,86,76,43,855,135,894,383,71,594,135,89,566,95,244,120,34,4,234,6,99,6,1)
coord=mapproject(long, lat, proj="gilbert", orientation = c(90,0,90))
points(coord, pch=20, cex=freq/250, col=alpha("red", 0.4), lwd=5)



###see if missingness and doesnt matter are correlated by caste in the matrimonial dataset
##COMPLEXION 2
#first get complete table 
#looking at proportion of missingness for complexion2 between different castes 
#first add comp2_presence values by unique caste categories
castes30plusstates4_comp2_by_caste1_1_dt=aggregate(castes30plus_only_new_andstates4$complexion2_presence, by=list(caste1_1=castes30plus_only_new_andstates4$caste1_1), FUN=sum)
#now make caste freq table 
#looking at proportion of missingness for complexion2 between different castes 
#first add comp2_presence values by unique caste categories
castes30plusstates4_comp2_by_caste1_1_dt=aggregate(castes30plus_only_new_andstates4$complexion2_presence, by=list(caste1_1=castes30plus_only_new_andstates4$caste1_1), FUN=sum)
#now make caste freq table 
library(plyr)
castes30plus_only_new_andstates4_castefreqtable = castes30plus_only_new_andstates4 %>%
  dplyr:: count(caste1_1)
#merge tables to get table with caste, comp2_presence and caste freq
castes30plusstates4_comp2_missingness_dt=merge(castes30plusstates4_comp2_by_caste1_1_dt, castes30plus_only_new_andstates4_castefreqtable, by="caste1_1")
#now add column for proportion missingness of comp2 info per caste
castes30plusstates4_comp2_missingness_dt$comp2_missingness_proportion=(castes30plusstates4_comp2_missingness_dt$n-castes30plusstates4_comp2_missingness_dt$x)/castes30plusstates4_comp2_missingness_dt$n
#make table for doesn't matters by caste1_1
castes30plus_only_new_andstates4$doesnmatterpresence=0
castes30plus_only_new_andstates4$doesnmatterpresence[castes30plus_only_new_andstates4$complexion2=="Doesn't Matter"]=1
castes30plusstates4_doesntmatters_by_caste1_1_dt=aggregate(castes30plus_only_new_andstates4$doesnmatterpresence, by=list(caste1_1=castes30plus_only_new_andstates4$caste1_1), FUN=sum)
#add column to main dataset 
castes30plusstates4_comp2_missingness_dt2=merge(castes30plusstates4_comp2_missingness_dt, castes30plusstates4_doesntmatters_by_caste1_1_dt, by="caste1_1")
#make proportion doesnt matter column
castes30plusstates4_comp2_missingness_dt2$doesntmatterproportion=(castes30plusstates4_comp2_missingness_dt2$x.y/castes30plusstates4_comp2_missingness_dt2$n)
##now run correlation of missingness prop against doesnt matter prop
mod9=lm(doesntmatterproportion~comp2_missingness_proportion, data=castes30plusstates4_comp2_missingness_dt2)
plot(castes30plusstates4_comp2_missingness_dt2$comp2_missingness_proportion, castes30plusstates4_comp2_missingness_dt2$doesntmatterproportion, xlab="Proportion of caste not providing information on skin complexion desired in future spouse", ylab="Proportion of caste stating that skin complexion of spouse 'Doesn't Matter'", main="Scatterplot of proportion missingness against proportion 'Doesn't Matter' in caste in regards to skin complexion preference in future spouse", cex.lab=1, cex.main=1, pch=20)
abline(mod9, col="red")
#check for normality
hist(castes30plusstates4_comp2_missingness_dt2$comp2_missingness_proportion)
hist(castes30plusstates4_comp2_missingness_dt2$doesntmatterproportion)
shapiro.test(castes30plusstates4_comp2_missingness_dt2$comp2_missingness_proportion)
shapiro.test(castes30plusstates4_comp2_missingness_dt2$doesntmatterproportion)
#not normal, do kendall's tau instead 
cor.test(castes30plusstates4_comp2_missingness_dt2$comp2_missingness_proportion, castes30plusstates4_comp2_missingness_dt2$doesntmatterproportion, method="kendall")

##repeat for COMPLEXION 1 variable 
#need to add comp1_presence to main datatable 
castes30plus_only_new_andstates4$complexion1_presence=0
castes30plus_only_new_andstates4$complexion1_presence[which(castes30plus_only_new_andstates4$complexion1!="NA")]=1
#first get complete table 
#looking at proportion of missingness for complexion1 between different castes 
#first add comp1_presence values by unique caste categories
castes30plusstates4_comp1_by_caste1_1_dt=aggregate(castes30plus_only_new_andstates4$complexion1_presence, by=list(caste1_1=castes30plus_only_new_andstates4$caste1_1), FUN=sum)
#now make caste freq table 
#looking at proportion of missingness for complexion1 between different castes 
#first add comp1_presence values by unique caste categories
castes30plusstates4_comp1_by_caste1_1_dt=aggregate(castes30plus_only_new_andstates4$complexion1_presence, by=list(caste1_1=castes30plus_only_new_andstates4$caste1_1), FUN=sum)
#now make caste freq table 
library(plyr)
castes30plus_only_new_andstates4_castefreqtable = castes30plus_only_new_andstates4 %>%
  dplyr:: count(caste1_1)
#merge tables to get table with caste, comp1_presence and caste freq
castes30plusstates4_comp1_missingness_dt=merge(castes30plusstates4_comp1_by_caste1_1_dt, castes30plus_only_new_andstates4_castefreqtable, by="caste1_1")
#now add column for proportion missingness of comp1 info per caste
castes30plusstates4_comp1_missingness_dt$comp1_missingness_proportion=(castes30plusstates4_comp1_missingness_dt$n-castes30plusstates4_comp1_missingness_dt$x)/castes30plusstates4_comp1_missingness_dt$n
#make table for doesn't matters by caste1_1
castes30plus_only_new_andstates4$doesnmatterpresence_comp1=0
castes30plus_only_new_andstates4$doesnmatterpresence_comp1[castes30plus_only_new_andstates4$complexion1=="Doesn't Matter"]=1
castes30plusstates4_doesntmatters_by_caste1_1_dt=aggregate(castes30plus_only_new_andstates4$doesnmatterpresence_comp1, by=list(caste1_1=castes30plus_only_new_andstates4$caste1_1), FUN=sum)
#add column to main dataset 
castes30plusstates4_comp1_missingness_dt2=merge(castes30plusstates4_comp1_missingness_dt, castes30plusstates4_doesntmatters_by_caste1_1_dt, by="caste1_1")
#make proportion doesnt matter column
castes30plusstates4_comp1_missingness_dt2$doesntmatterproportion=(castes30plusstates4_comp1_missingness_dt2$x.y/castes30plusstates4_comp1_missingness_dt2$n)
##now run correlation of missingness prop against doesnt matter prop
mod8=lm(doesntmatterproportion~comp1_missingness_proportion, data=castes30plusstates4_comp1_missingness_dt2)
tiff("comp1missingness_vs_comp1doesnmatter.tiff", res=96, height=650, width=700)
plot(castes30plusstates4_comp1_missingness_dt2$comp1_missingness_proportion, castes30plusstates4_comp1_missingness_dt2$doesntmatterproportion, xlab="Proportion of caste not providing information on own skin complexion", ylab="Proportion of caste stating that their own skin complexion 'Doesn't Matter'", main="Scatterplot of proportion missingness against proportion 'Doesn't Matter' in caste in regards to one's own skin complexion", cex.lab=1, cex.main=1.1, pch=20)
abline(mod8, col="red")
dev.off()
#check for normality
hist(castes30plusstates4_comp1_missingness_dt2$comp1_missingness_proportion)
hist(castes30plusstates4_comp1_missingness_dt2$doesntmatterproportion)
shapiro.test(castes30plusstates4_comp1_missingness_dt2$comp1_missingness_proportion)
shapiro.test(castes30plusstates4_comp1_missingness_dt2$doesntmatterproportion)
#not normal, do kendall's tau instead 
cor.test(castes30plusstates4_comp1_missingness_dt2$comp1_missingness_proportion, castes30plusstates4_comp1_missingness_dt2$doesntmatterproportion, method="kendall")
#--> p value < 2.2204e-16, relatively strong positive correlation, evidence to suggest that as missingness of information increases, number of doesnt matter answers increases 




##now run ordinal regression to see what variables predict desired skin complexion in dataset - make new 30pluscastes dataset with extra variables previously dropped, using same method as above
library(foreign)
library(ggplot2)
library(MASS)
library(Hmisc)
library(reshape2)
#now make castes30plus dataset but with allbrides and allgrooms rather than cutdownbrides cutdowngrooms, so all variables included
allbrides$source="Brides"
allgrooms$source="Grooms"
allbridesandgrooms=rbind(allbrides, allgrooms)
allbridesandgrooms$birth3 <- as.character(allbridesandgrooms$birth3)
allbridesandgrooms$birth3[allbridesandgrooms$birth3==""] <- NA
allbridesandgrooms$birth3 <- as.factor(allbridesandgrooms$birth3)
sum(is.na(allbridesandgrooms$birth3))/nrow(allbridesandgrooms)
allbridesandgrooms_India=allbridesandgrooms[with(allbridesandgrooms, birth3=="India"|is.na(birth3)), ]
(nrow(allbridesandgrooms)-nrow(allbridesandgrooms_India))/nrow(allbridesandgrooms)
library(data.table) 
allbridesandgrooms_caste1split=setDT(allbridesandgrooms_India)[, paste0("caste1", 1:99) := tstrsplit(caste1, " - |,")]
allbridesandgrooms_caste1and2split=setDT(allbridesandgrooms_caste1split)[, paste0("caste2", 100:199) := tstrsplit(caste2, " - |,")]
allbridesandgrooms_caste1and2split_new=allbridesandgrooms_caste1and2split[is.na(allbridesandgrooms_caste1and2split$caste2102),]
allbridesandgrooms_caste1and2split_new2=allbridesandgrooms_caste1and2split_new[is.na(allbridesandgrooms_caste1and2split_new$caste13),]
(nrow(allbridesandgrooms_caste1and2split)-nrow(allbridesandgrooms_caste1and2split_new2))/nrow(allbridesandgrooms_caste1and2split)
library(dplyr)
allbridesandgrooms_cleaned_freqtable2 = allbridesandgrooms_caste1and2split_new2 %>% dplyr:: count(caste1)
allbridesandgrooms_cleaned_withfreq2=merge(allbridesandgrooms_caste1and2split_new2, allbridesandgrooms_cleaned_freqtable2, by="caste1")
castes30plus_only2=subset(allbridesandgrooms_cleaned_withfreq2, n>=30)
castes30plus_only2_new=castes30plus_only2[, c(1:69, 167,168)]
colnames(castes30plus_only2_new)[68:71] <- c("caste1_1", "caste1_2", "caste2_1", "caste2_2")
###now merge all cities and states spreadsheet with castes30plus_only2_new datatable to give state for each individual 
#set birth4 to NA for blanks in new matrimonial dataset
castes30plus_only2_new$birth4 <- as.character(castes30plus_only2_new$birth4)
castes30plus_only2_new$birth4[castes30plus_only2_new$birth4==""] <- NA
castes30plus_only2_new$birth4 <- as.factor(castes30plus_only2_new$birth4)
#now merge to have castes30plus group with states
castes30plus_only2_new_andstates=merge(castes30plus_only2_new, alltownsandstates_India_new, by="birth4", all.x=TRUE)
##to assign state to those without birth4 info, create list of modal State for each caste
castes30plus_only2_new_andstates$caste1 <- as.character(castes30plus_only2_new_andstates$caste1)
castes30plus_only2_new_andstates$caste1[castes30plus_only2_new_andstates$caste1==""] <- NA
castes30plus_only2_new_andstates$caste1 <- as.factor(castes30plus_only2_new_andstates$caste1)
# Mode function
calculate_mode <- function(x) {
  uniqx <- na.omit(unique(x))
  uniqx[which.max(tabulate(match(x, uniqx)))]
}
#run mode function for state
library(dplyr)
library(modeest)
castes30plus_only2_new_andstates_caste1_by_State_mode=castes30plus_only2_new_andstates %>%  
  dplyr:: group_by(caste1) %>%
  dplyr:: summarise(State = calculate_mode(State))
colnames(castes30plus_only2_new_andstates_caste1_by_State_mode)=c("caste1", "State")
#export
write.table(castes30plus_only2_new_andstates_caste1_by_State_mode, "castes30plus_only2_new_andstates_caste1_by_State_mode", sep=";")                        
#now merge state mode datatable, with main dataframe
castes30plus_only2_new_andstates2=merge(castes30plus_only2_new_andstates, castes30plus_only2_new_andstates_caste1_by_State_mode, by="caste1", all.x = TRUE)
#rename columns 
colnames(castes30plus_only2_new_andstates2)[75]=c("State")
colnames(castes30plus_only2_new_andstates2)[78]=c("Modal_State_by_caste1")
#now assign State as Modal_State_by_caste1 when state=NA
#first ascribe NA to blanks in State
castes30plus_only2_new_andstates2$State <- as.character(castes30plus_only2_new_andstates2$State)
castes30plus_only2_new_andstates2$State[castes30plus_only2_new_andstates2$State==""] <- NA
castes30plus_only2_new_andstates2$State <- as.factor(castes30plus_only2_new_andstates2$State)
#####
#now do assigning of states when state is NA
library(dplyr)
castes30plus_only2_new_andstates3 = castes30plus_only2_new_andstates2 %>% 
  mutate(State = coalesce(State,Modal_State_by_caste1))
#remove Delhi* and replace as Delhi in the matrimonial dataset in both andstates3 and andstates2 for later
castes30plus_only2_new_andstates3$State=gsub("\\*","",castes30plus_only2_new_andstates3$State)
castes30plus_only2_new_andstates2$State=gsub("\\*","",castes30plus_only2_new_andstates2$State)
##now got all columns for cleaned dataset, can now run ordinal regression 
#keep variables: caste1, 1_1, 1_2, caste2, caste2_1, caste2_2, complexion1, complexion2, age2, birth1, birth4, body1, body2, brothers, created_for, degree, drinking1, drinking2, eating1, eating2, employed1, emploted2, gotra, children, height1, height2, income1, income2,marital_status1, marital_status2, religion1, religion2, sisters, smoking1, smoking2, subcaste, status1, status2, weight1, weight2, State
castes30plus_only2_new_andstates4=castes30plus_only2_new_andstates3[, c(1,2,6,7,10:13,16:19,21:24,26,27,32,33,34,35,37,38,52,53,54,61,65,66,67,68,69,70,71,74)]
castes30plus_only2_new_andstates4<- castes30plus_only2_new_andstates4 %>% mutate_if(is.character,as.factor)
#remove doesn't matter answers for comp1 and comp2 in new dataset 
castes30plus_only2_new_andstates4_new=subset(castes30plus_only2_new_andstates4, complexion1!="Doesn't Matter")
castes30plus_only2_new_andstates4_new2=subset(castes30plus_only2_new_andstates4_new, complexion2!="Doesn't Matter")
#remove NAs for complexion 2 and complexion 1
castes30plus_only2_new_andstates4_new2 <- castes30plus_only2_new_andstates4_new2 %>% mutate_all(na_if,"")
castes30plus_only2_new_andstates4_new3=castes30plus_only2_new_andstates4_new2[!is.na(castes30plus_only2_new_andstates4_new2$complexion1)]
castes30plus_only2_new_andstates4_new4=castes30plus_only2_new_andstates4_new3[!is.na(castes30plus_only2_new_andstates4_new3$complexion2)]
#add birth year column to new2 dataframe
castes30plus_only2_new_andstates4_new4$BirthYear <- substr(castes30plus_only2_new_andstates4_new4$birth1, 8, 12)
castes30plus_only2_new_andstates4_new4$BirthYear = as.numeric(as.character(castes30plus_only2_new_andstates4_new4$BirthYear))
#need to tell R order of levels in complexion variables
castes30plus_only2_new_andstates4_new4$complexion2 <- factor(castes30plus_only2_new_andstates4_new4$complexion2, levels=c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark"), ordered=TRUE)
castes30plus_only2_new_andstates4_new4$complexion1 <- factor(castes30plus_only2_new_andstates4_new4$complexion1, levels=c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark"), ordered=TRUE)
ord.comp1=factor(castes30plus_only2_new_andstates4_new4$complexion1, level=c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark"), ordered=TRUE)
#make new without any created_for variable blanks
castes30plus_only2_new_andstates4_new5=castes30plus_only2_new_andstates4_new4[!is.na(castes30plus_only2_new_andstates4_new4$created_for)]
#redefine levels 
castes30plus_only2_new_andstates4_new5$complexion2 <- factor(castes30plus_only2_new_andstates4_new5$complexion2, levels=c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark"), ordered=TRUE)
castes30plus_only2_new_andstates4_new5$complexion1 <- factor(castes30plus_only2_new_andstates4_new5$complexion1, levels=c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark"), ordered=TRUE)
castes30plus_only2_new_andstates4_new5$source <- factor(castes30plus_only2_new_andstates4_new5$source, levels=c("Brides", "Grooms"), ordered=FALSE)
castes30plus_only2_new_andstates4_new5$created_for <- factor(castes30plus_only2_new_andstates4_new5$created_for, levels=c("Self", "Brother", "Daughter", "Friend", "Relative", "Sister", "Son"), ordered=FALSE)
castes30plus_only2_new_andstates4_new5$body1 <- factor(castes30plus_only2_new_andstates4_new5$body1, levels=c("Average","Athletic", "Doesn't Matter", "Heavy", "Slim"), ordered=FALSE)
#now run model
Fullermodel=polr(complexion2~source+created_for+BirthYear+body1, data=castes30plus_only2_new_andstates4_new5, Hess=TRUE)
(ctable <- coef(summary(Fullermodel)))
p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
Fullermodel_ctable=(ctable <- cbind(ctable, "p value" = p))
(ci <- confint(Fullermodel))
confint.default(Fullermodel)
exp(coef(Fullermodel))
Fullermodel_oddstable=exp(cbind(OR = coef(Fullermodel), ci))
AnovaFullerMod=Anova(Fullermodel)





##run glm and visualisation of how self-stated complexion affects wishing to marry within varna for exploratory analyses
#need to create new dataset and remove comp1 doesnt matters
castes30plus_only2_new_andstates4_new5=subset(castes30plus_only2_new_andstates4, complexion1!="Doesn't Matter")
#add NAs for blanks 
castes30plus_only2_new_andstates4_new5 <- castes30plus_only2_new_andstates4_new5 %>% mutate_all(na_if,"")
#remove any Nas for birth year 
castes30plus_only2_new_andstates4_new5=castes30plus_only2_new_andstates4_new5[!is.na(castes30plus_only2_new_andstates4_new5$BirthYear)]
#remove any NAs for complexion1
castes30plus_only2_new_andstates4_new5=castes30plus_only2_new_andstates4_new5[!is.na(castes30plus_only2_new_andstates4_new5$complexion1)]
#need to add castesame variable
castes30plus_only2_new_andstates4_new5$castesame=NA
castes30plus_only2_new_andstates4_new5$castesame[as.character(castes30plus_only2_new_andstates4_new5$caste1_1)==as.character(castes30plus_only2_new_andstates4_new5$caste2_1)]=1
castes30plus_only2_new_andstates4_new5$castesame[as.character(castes30plus_only2_new_andstates4_new5$caste1_1)!=as.character(castes30plus_only2_new_andstates4_new5$caste2_1)]=0
#now remove any NAs for caste same
castes30plus_only2_new_andstates4_new5=castes30plus_only2_new_andstates4_new5[!is.na(castes30plus_only2_new_andstates4_new5$castesame)]
#create order of levels for comp1
ord.comp1=factor(castes30plus_only2_new_andstates4_new5$complexion1, level=c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark"), ordered=TRUE)
assortativemodelling_new=glm(castesame~ord.comp1, data=castes30plus_only2_new_andstates4_new5, family=binomial)
summary(assortativemodelling_new)

##repeat but for caste1_2 and 2_1 ie. to see how self-stated skin affects wanting to marry within jati
#run glm and visualisation of how wanting endogamy interacts with self-stated complexion
#need to create new dataset and remove comp1 doesnt matters
castes30plus_only2_new_andstates4_new5=subset(castes30plus_only2_new_andstates4, complexion1!="Doesn't Matter")
#add NAs for blanks 
castes30plus_only2_new_andstates4_new5 <- castes30plus_only2_new_andstates4_new5 %>% mutate_all(na_if,"")
#remove any Nas for birth year 
castes30plus_only2_new_andstates4_new5=castes30plus_only2_new_andstates4_new5[!is.na(castes30plus_only2_new_andstates4_new5$BirthYear)]
#remove any NAs for complexion1
castes30plus_only2_new_andstates4_new5=castes30plus_only2_new_andstates4_new5[!is.na(castes30plus_only2_new_andstates4_new5$complexion1)]
#need to add castesame variable
castes30plus_only2_new_andstates4_new5$castesame=NA
castes30plus_only2_new_andstates4_new5$castesame[as.character(castes30plus_only2_new_andstates4_new5$caste1_2)==as.character(castes30plus_only2_new_andstates4_new5$caste2_2)]=1
castes30plus_only2_new_andstates4_new5$castesame[as.character(castes30plus_only2_new_andstates4_new5$caste1_2)!=as.character(castes30plus_only2_new_andstates4_new5$caste2_2)]=0
#now remove any NAs for caste same
castes30plus_only2_new_andstates4_new5=castes30plus_only2_new_andstates4_new5[!is.na(castes30plus_only2_new_andstates4_new5$castesame)]
#create order of levels for comp1
ord.comp1=factor(castes30plus_only2_new_andstates4_new5$complexion1, level=c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark"), ordered=TRUE)
castes30plus_only2_new_andstates4_new5$BirthYear = as.numeric(as.character(castes30plus_only2_new_andstates4_new5$BirthYear))
assortativemodelling_new=glm(castesame~ord.comp1, data=castes30plus_only2_new_andstates4_new5, family=binomial)
summary(assortativemodelling_new)









#merge genetic and matrimonial populations
#create and export matrimonial and genetic dt with column included comp2 answers 
matrimonial30plus_genetic_withSNP_withcomp2answers_dt=merge(castes30plus_comp2_by_caste1_1_dt, matrimonial30plus_genetic_withSNP_dt, by="caste1_1", all.y=TRUE)
#relabel columns for ease of understanding
colnames(matrimonial30plus_genetic_withSNP_withcomp2answers_dt)=c("caste1_1", "complexion2_#answers", "Matrimonial pop size", "Genetic pop size")
#now get 7 common populations subseted 
GujuratiBrahmin=subset(castes30plus_only_new_andstates4, caste1_1=="Brahmin" & State=="Gujarat")
Gujjar=castes30plus_only_new_andstates4[grep("Gujjar", castes30plus_only_new_andstates4$caste1), ]
Iyer=castes30plus_only_new_andstates4[grep("Iyer", castes30plus_only_new_andstates4$caste1), ]
Kamboj=castes30plus_only_new_andstates4[grep("Kamboj", castes30plus_only_new_andstates4$caste1), ]
Khatri=castes30plus_only_new_andstates4[grep("Khatri", castes30plus_only_new_andstates4$caste1), ]
Maratha=castes30plus_only_new_andstates4[grep("Maratha", castes30plus_only_new_andstates4$caste1), ]
WestBengalBrahmin=subset(castes30plus_only_new_andstates4, caste1_1=="Brahmin" & State=="West Bengal")
#then add updated matrimonial dataset numbers to matrimonial-genetic spreadsheet with SNP now more inclusive subsets have been created 
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$`Matrimonial pop size`[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujarati Brahmin"]=nrow(GujuratiBrahmin)
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$`Matrimonial pop size`[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="West Bengal Brahmin"]=nrow(WestBengalBrahmin)
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$`Matrimonial pop size`[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujjar"]=nrow(Gujjar)
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$`Matrimonial pop size`[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Iyer"]=nrow(Iyer)
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$`Matrimonial pop size`[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Kamboj"]=nrow(Kamboj)
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$`Matrimonial pop size`[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Khatri"]=nrow(Khatri)
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$`Matrimonial pop size`[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Maratha"]=nrow(Maratha)


#now add #comp2 responses for each population in the withSNP spreadhseet 
#first set complexion2 blanks to NA
castes30plus_only_new_andstates4$complexion2 <- as.character(castes30plus_only_new_andstates4$complexion2)
castes30plus_only_new_andstates4$complexion2[castes30plus_only_new_andstates4$complexion2==""] <- NA
castes30plus_only_new_andstates4$complexion2 <- as.factor(castes30plus_only_new_andstates4$complexion2)
#now need to make comp2 presence for castes30plus_only
castes30plus_only_new_andstates4$complexion2_presence=0
castes30plus_only_new_andstates4$complexion2_presence[which(castes30plus_only_new_andstates4$complexion2!="NA")]=1
#now run subsets again
GujuratiBrahmin=subset(castes30plus_only_new_andstates4, caste1_1=="Brahmin" & State=="Gujarat")
Gujjar=castes30plus_only_new_andstates4[grep("Gujjar", castes30plus_only_new_andstates4$caste1), ]
Iyer=castes30plus_only_new_andstates4[grep("Iyer", castes30plus_only_new_andstates4$caste1), ]
Kamboj=castes30plus_only_new_andstates4[grep("Kamboj", castes30plus_only_new_andstates4$caste1), ]
Khatri=castes30plus_only_new_andstates4[grep("Khatri", castes30plus_only_new_andstates4$caste1), ]
Maratha=castes30plus_only_new_andstates4[grep("Maratha", castes30plus_only_new_andstates4$caste1), ]
WestBengalBrahmin=subset(castes30plus_only_new_andstates4, caste1_1=="Brahmin" & State=="West Bengal")
#now make counts of comp2 answers in each pop
GujuratiBrahmins_bycomp2answers=sum(GujuratiBrahmin$complexion2_presence)
Gujjars_bycomp2answers=sum(Gujjar$complexion2_presence)
Iyers_bycomp2answers=sum(Iyer$complexion2_presence)
Kambojs_bycomp2answers=sum(Kamboj$complexion2_presence)
Khatris_bycomp2answers=sum(Khatri$complexion2_presence)
Marathas_bycomp2answers=sum(Maratha$complexion2_presence)
WestBengalBrahmins_bycomp2answers=sum(WestBengalBrahmin$complexion2_presence)
#now add #comp2 answers to table 
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$`complexion2_#answers`[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujarati Brahmin"]=GujuratiBrahmins_bycomp2answers
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$`complexion2_#answers`[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujjar"]=sum(Gujjars_bycomp2answers)
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$`complexion2_#answers`[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Iyer"]=sum(Iyers_bycomp2answers)
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$`complexion2_#answers`[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Kamboj"]=sum(Kambojs_bycomp2answers)
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$`complexion2_#answers`[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Khatri"]=sum(Khatris_bycomp2answers)
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$`complexion2_#answers`[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Maratha"]=sum(Marathas_bycomp2answers)
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$`complexion2_#answers`[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="West Bengal Brahmin"]=WestBengalBrahmins_bycomp2answers


#now create subset of castes30plus_only_new for brides and grooms separately for next analyses
castes30plus_only_new_brides=subset(castes30plus_only_new_andstates4, source=="cutdownbrides")
castes30plus_only_new_grooms=subset(castes30plus_only_new_andstates4, source=="cutdowngrooms")


#analysis of COMPLEXION1 DISTRIBUTION
#now can calculate self stated skin colour proportions 
#first set all complexion1 blanks to NA
castes30plus_only_new_brides$complexion1 <- as.character(castes30plus_only_new_brides$complexion1)
castes30plus_only_new_brides$complexion1[castes30plus_only_new_brides$complexion1==""] <- NA
castes30plus_only_new_brides$complexion1 <- as.factor(castes30plus_only_new_brides$complexion1)
#also set all complexion1 blanks to NA for grooms
castes30plus_only_new_grooms$complexion1 <- as.character(castes30plus_only_new_grooms$complexion1)
castes30plus_only_new_grooms$complexion1[castes30plus_only_new_grooms$complexion1==""] <- NA
castes30plus_only_new_grooms$complexion1 <- as.factor(castes30plus_only_new_grooms$complexion1)

#now collate all individuals who have self stated skin colour complexion1
castes30plus_only_new_brides_comp1=castes30plus_only_new_brides[!is.na(castes30plus_only_new_brides$complexion1), ]
castes30plus_only_new_grooms_comp1=castes30plus_only_new_grooms[!is.na(castes30plus_only_new_grooms$complexion1), ]
#now find proportions of brides who state they are in each of 6 skin categories
Prop_VeryFairBrides=nrow(subset(castes30plus_only_new_brides_comp1, complexion1=="Very Fair"))/nrow(castes30plus_only_new_brides_comp1)
Prop_FairBrides=nrow(subset(castes30plus_only_new_brides_comp1, complexion1=="Fair"))/nrow(castes30plus_only_new_brides_comp1)
Prop_WheatishBrides=nrow(subset(castes30plus_only_new_brides_comp1, complexion1=="Wheatish"))/nrow(castes30plus_only_new_brides_comp1)
Prop_WhetishMediumBrides=nrow(subset(castes30plus_only_new_brides_comp1, complexion1=="Whetish Medium"))/nrow(castes30plus_only_new_brides_comp1)
Prop_DarkBrides=nrow(subset(castes30plus_only_new_brides_comp1, complexion1=="Dark"))/nrow(castes30plus_only_new_brides_comp1)
Prop_DoesntMatterBrides=nrow(subset(castes30plus_only_new_brides_comp1, complexion1=="Doesn't Matter"))/nrow(castes30plus_only_new_brides_comp1)
#do same with grooms
Prop_VeryFairGrooms=nrow(subset(castes30plus_only_new_grooms_comp1, complexion1=="Very Fair"))/nrow(castes30plus_only_new_grooms_comp1)
Prop_FairGrooms=nrow(subset(castes30plus_only_new_grooms_comp1, complexion1=="Fair"))/nrow(castes30plus_only_new_grooms_comp1)
Prop_WheatishGrooms=nrow(subset(castes30plus_only_new_grooms_comp1, complexion1=="Wheatish"))/nrow(castes30plus_only_new_grooms_comp1)
Prop_WhetishMediumGrooms=nrow(subset(castes30plus_only_new_grooms_comp1, complexion1=="Whetish Medium"))/nrow(castes30plus_only_new_grooms_comp1)
Prop_DarkGrooms=nrow(subset(castes30plus_only_new_grooms_comp1, complexion1=="Dark"))/nrow(castes30plus_only_new_grooms_comp1)
Prop_DoesntMatterGrooms=nrow(subset(castes30plus_only_new_grooms_comp1, complexion1=="Doesn't Matter"))/nrow(castes30plus_only_new_grooms_comp1)
#VISUALISATION
selfstatedskin=c(Prop_VeryFairGrooms, Prop_FairGrooms, Prop_WheatishGrooms, Prop_WhetishMediumGrooms, Prop_DarkGrooms, Prop_DoesntMatterGrooms, Prop_VeryFairBrides, Prop_FairBrides, Prop_WheatishBrides, Prop_WhetishMediumBrides, Prop_DarkBrides, Prop_DoesntMatterBrides)
Gender=c("Men","Men","Men","Men","Men","Men","Women", "Women","Women","Women","Women","Women")
skincategory=c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark", "Doesn't Matter", "Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark", "Doesn't Matter")
selfstatedskin_table= data.frame(selfstatedskin, Gender,skincategory)
selfstatedskin_table$skincategory = factor(selfstatedskin_table$skincategory, levels=c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark", "Doesn't Matter"))
library(ggplot2)
ggplot(selfstatedskin_table, 
       aes(x = skincategory , y=selfstatedskin, fill=Gender)) +
  geom_bar(stat='identity', position="dodge") +
  ggtitle("Self-stated Skin Complexion of Men and Women") +
  labs(y="Proportion of Men and Women", x="Self-stated Skin Complexion Categeory") +
  ylim(0,0.6)




#COMPLEXION2 DISTRIBUTION
#comparing men and women's preferences for spouse complexion in all ads including comp2
#first set all complexion2 blanks to NA
castes30plus_only_new_brides$complexion2 <- as.character(castes30plus_only_new_brides$complexion2)
castes30plus_only_new_brides$complexion2[castes30plus_only_new_brides$complexion2==""] <- NA
castes30plus_only_new_brides$complexion2 <- as.factor(castes30plus_only_new_brides$complexion2)
#also set all complexion2 blanks to NA for grooms
castes30plus_only_new_grooms$complexion2 <- as.character(castes30plus_only_new_grooms$complexion2)
castes30plus_only_new_grooms$complexion2[castes30plus_only_new_grooms$complexion2==""] <- NA
castes30plus_only_new_grooms$complexion2 <- as.factor(castes30plus_only_new_grooms$complexion2)
#now collate all individuals who have entry for complexion2
castes30plus_only_new_brides_comp2=castes30plus_only_new_brides[!is.na(castes30plus_only_new_brides$complexion2), ]
castes30plus_only_new_grooms_comp2=castes30plus_only_new_grooms[!is.na(castes30plus_only_new_grooms$complexion2), ]
#make props for each complexion category
Prop_BridesWantingVeryFair=nrow(subset(castes30plus_only_new_brides_comp2, complexion2=="Very Fair"))/nrow(castes30plus_only_new_brides_comp2)
Prop_BridesWantingFair=nrow(subset(castes30plus_only_new_brides_comp2, complexion2=="Fair"))/nrow(castes30plus_only_new_brides_comp2)
Prop_BridesWantingWheatish=nrow(subset(castes30plus_only_new_brides_comp2, complexion2=="Wheatish"))/nrow(castes30plus_only_new_brides_comp2)
Prop_BridesWantingWhetishMedium=nrow(subset(castes30plus_only_new_brides_comp2, complexion2=="Whetish Medium"))/nrow(castes30plus_only_new_brides_comp2)
Prop_BridesWantingDark=nrow(subset(castes30plus_only_new_brides_comp2, complexion2=="Dark"))/nrow(castes30plus_only_new_brides_comp2)
Prop_BridesDontMind=nrow(subset(castes30plus_only_new_brides_comp2, complexion2=="Doesn't Matter"))/nrow(castes30plus_only_new_brides_comp2)

Prop_GroomsWantingVeryFair=nrow(subset(castes30plus_only_new_grooms_comp2, complexion2=="Very Fair"))/nrow(castes30plus_only_new_grooms_comp2)
Prop_GroomsWantingFair=nrow(subset(castes30plus_only_new_grooms_comp2, complexion2=="Fair"))/nrow(castes30plus_only_new_grooms_comp2)
Prop_GroomsWantingWheatish=nrow(subset(castes30plus_only_new_grooms_comp2, complexion2=="Wheatish"))/nrow(castes30plus_only_new_grooms_comp2)
Prop_GroomsWantingWhetishMedium=nrow(subset(castes30plus_only_new_grooms_comp2, complexion2=="Whetish Medium"))/nrow(castes30plus_only_new_grooms_comp2)
Prop_GroomsWantingDark=nrow(subset(castes30plus_only_new_grooms_comp2, complexion2=="Dark"))/nrow(castes30plus_only_new_grooms_comp2)
Prop_GroomsDontMind=nrow(subset(castes30plus_only_new_grooms_comp2, complexion2=="Doesn't Matter"))/nrow(castes30plus_only_new_grooms_comp2)
#VISUALISATION
wantedskin=c(Prop_GroomsWantingVeryFair, Prop_GroomsWantingFair, Prop_GroomsWantingWheatish, Prop_GroomsWantingWhetishMedium, Prop_GroomsWantingDark, Prop_GroomsDontMind, Prop_BridesWantingVeryFair, Prop_BridesWantingFair, Prop_BridesWantingWheatish, Prop_BridesWantingWhetishMedium, Prop_BridesWantingDark, Prop_BridesDontMind)
Gender=c("Men","Men","Men","Men","Men","Men","Women", "Women","Women","Women","Women","Women")
skincategory=c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark", "Doesn't Matter", "Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark", "Doesn't Matter")
wantedskin_table= data.frame(wantedskin, Gender,skincategory)
wantedskin_table$skincategory = factor(wantedskin_table$skincategory, levels=c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark", "Doesn't Matter"))
library(ggplot2)
ggplot(wantedskin_table, 
       aes(x = skincategory , y=wantedskin, fill=Gender)) +
  geom_bar(stat='identity', position="dodge") +
  ggtitle("Men and Women's Preferences for Skin Complexion in Future Spouse") +
  labs(y="Proportion of Men and Women", x="Desired Skin Complexion Categeory") +
  ylim(0,0.6)

#now perform stats test on these two distributions
#MALES VS FEMALES self-stated skin
#need to do Mann Whitney test for distribution differences between self-stated skin complexion, because data is ordinal 
castes30plus_only_new_andstates4_comp1test=castes30plus_only_new_andstates4
#have to remove "Doesn't Matter" first because not part of order
castes30plus_only_new_andstates4_comp1test=castes30plus_only_new_andstates4_comp1test[!castes30plus_only_new_andstates4_comp1test$complexion1=="Doesn't Matter"]
castes30plus_only_new_andstates4_comp1test$complexion1=as.character(castes30plus_only_new_andstates4_comp1test$complexion1)
castes30plus_only_new_andstates4_comp1test$complexion1=factor(castes30plus_only_new_andstates4_comp1test$complexion1, levels=c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark"), ordered=TRUE)
males=subset(castes30plus_only_new_andstates4_comp1test, source=="cutdowngrooms")
females=subset(castes30plus_only_new_andstates4_comp1test, source=="cutdownbrides")
wilcox.test(x=as.numeric(males$complexion1), y=as.numeric(females$complexion1))
#p-value<2.2e-16
#now check differences in "Doesn't Matter"s using chi squared goodness of fit test
castes30plus_only_new_andstates4_comp1test2=castes30plus_only_new_andstates4[castes30plus_only_new_andstates4$complexion1=="Doesn't Matter"]
males=subset(castes30plus_only_new_andstates4_comp1test2, source=="cutdowngrooms")
females=subset(castes30plus_only_new_andstates4_comp1test2, source=="cutdownbrides")
chisqtable=rbind(nrow(males), nrow(females))
chisqtable=as.data.table(chisqtable)
colnames(chisqtable)="DMs"
rownames(chisqtable)=c("males", "females")
propfemales=nrow(subset(castes30plus_only_new_andstates4, source=="cutdownbrides"))/nrow(castes30plus_only_new_andstates4)
propmales=nrow(subset(castes30plus_only_new_andstates4, source=="cutdowngrooms"))/nrow(castes30plus_only_new_andstates4)
chisq.test(chisqtable$DMs, p=c(propmales, propfemales))
#p-value<2.2e-16
#now check differences in missingness using chi squared goodness of fit test 
castes30plus_only_new_andstates4_comp1test3=castes30plus_only_new_andstates4[is.na(castes30plus_only_new_andstates4$complexion1)]
males=subset(castes30plus_only_new_andstates4_comp1test3, source=="cutdowngrooms")
females=subset(castes30plus_only_new_andstates4_comp1test3, source=="cutdownbrides")
chisqtable=rbind(nrow(males), nrow(females))
colnames(chisqtable)="missing"
rownames(chisqtable)=c("males", "females")
chisqtable=as.data.table(chisqtable)
chisq.test(chisqtable$missing, p=c(propmales, propfemales))
#p-value<2.2e-16
#check proportion of men and women with missing info for comp1
nrow(subset(castes30plus_only_new_andstates4_comp1test3, source=="cutdownbrides"))/nrow(subset(castes30plus_only_new_andstates4, source=="cutdownbrides"))
nrow(subset(castes30plus_only_new_andstates4_comp1test3, source=="cutdowngrooms"))/nrow(subset(castes30plus_only_new_andstates4, source=="cutdowngrooms"))
#--> 23.9% of brides,31.3% of grooms



#MALES VS FEMALES DESIRED skin
#need to do Mann Whitney test for distribution differences between self-stated skin complexion, because data is ordinal 
castes30plus_only_new_andstates4_comp2test=castes30plus_only_new_andstates4
#have to remove "Doesn't Matter" first because not part of order
castes30plus_only_new_andstates4_comp2test=castes30plus_only_new_andstates4_comp2test[!castes30plus_only_new_andstates4_comp2test$complexion2=="Doesn't Matter"]
castes30plus_only_new_andstates4_comp2test$complexion2=as.character(castes30plus_only_new_andstates4_comp2test$complexion2)
castes30plus_only_new_andstates4_comp2test$complexion2=factor(castes30plus_only_new_andstates4_comp2test$complexion2, levels=c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark"), ordered=TRUE)
males=subset(castes30plus_only_new_andstates4_comp2test, source=="cutdowngrooms")
females=subset(castes30plus_only_new_andstates4_comp2test, source=="cutdownbrides")
wilcox.test(x=as.numeric(males$complexion2), y=as.numeric(females$complexion2))
#now check differences in "Doesn't Matter"s using chi squared goodness of fit test
castes30plus_only_new_andstates4_comp2test2=castes30plus_only_new_andstates4[castes30plus_only_new_andstates4$complexion2=="Doesn't Matter"]
males=subset(castes30plus_only_new_andstates4_comp2test2, source=="cutdowngrooms")
females=subset(castes30plus_only_new_andstates4_comp2test2, source=="cutdownbrides")
chisqtable=rbind(nrow(males), nrow(females))
chisqtable=as.data.table(chisqtable)
colnames(chisqtable)="DMs"
rownames(chisqtable)=c("males", "females")
propfemales=nrow(subset(castes30plus_only_new_andstates4, source=="cutdownbrides"))/nrow(castes30plus_only_new_andstates4)
propmales=nrow(subset(castes30plus_only_new_andstates4, source=="cutdowngrooms"))/nrow(castes30plus_only_new_andstates4)
chisq.test(chisqtable$DMs, p=c(propmales, propfemales))
#p-value<2.2e-16
#now check differences in missingness using chi squared goodness of fit test 
castes30plus_only_new_andstates4_comp2test3=castes30plus_only_new_andstates4[is.na(castes30plus_only_new_andstates4$complexion2)]
males=subset(castes30plus_only_new_andstates4_comp2test3, source=="cutdowngrooms")
females=subset(castes30plus_only_new_andstates4_comp2test3, source=="cutdownbrides")
chisqtable=rbind(nrow(males), nrow(females))
colnames(chisqtable)="missing"
rownames(chisqtable)=c("males", "females")
chisqtable=as.data.table(chisqtable)
chisq.test(chisqtable$missing, p=c(propmales, propfemales))
#check proportion of men and women with missing info for comp1
nrow(subset(castes30plus_only_new_andstates4_comp2test3, source=="cutdownbrides"))/nrow(subset(castes30plus_only_new_andstates4, source=="cutdownbrides"))
nrow(subset(castes30plus_only_new_andstates4_comp2test3, source=="cutdowngrooms"))/nrow(subset(castes30plus_only_new_andstates4, source=="cutdowngrooms"))
#--> 23.9% of brides,31.3% of grooms


#test differences of distributions of men and women wanting spouse of lighter darker or same complexion, to look at hypotheses 1/2
#now find total females wanting or don't mind spouse darker
BridesWantingDarker_number=nrow(subset(castes30plus_only_new_brides_comp1_comp2, code_ratio12<1))
#now find total proportion of males wanting or don't mind spouse darker
GroomsWantingDarker_number=nrow(subset(castes30plus_only_new_grooms_comp1_comp2, code_ratio12<1))
#now find total proportion of females wanting spouse lighter
BridesWantingLighter_number=nrow(subset(castes30plus_only_new_brides_comp1_comp2, code_ratio12>1))
#now find total proportion of males wanting spouse lighter
GroomsWantingLighter_number=nrow(subset(castes30plus_only_new_grooms_comp1_comp2, code_ratio12>1))
#now find total proportion of females wanting spouse to have same skin complexion
BridesWantingSame_number=nrow(subset(castes30plus_only_new_brides_comp1_comp2, code_ratio12==1))
#now find total proportion of males wanting spouse to have same skin complexion
GroomsWantingSame_number=nrow(subset(castes30plus_only_new_grooms_comp1_comp2, code_ratio12==1))
#make chi squared table 
lighterdarkersame_chi_table=matrix(c(BridesWantingLighter_number, GroomsWantingLighter_number, BridesWantingDarker_number, GroomsWantingDarker_number, BridesWantingSame_number, GroomsWantingSame_number), ncol=2, byrow = TRUE)
rownames(lighterdarkersame_chi_table)=c("Lighter", "Darker", "Same")
colnames(lighterdarkersame_chi_table)=c("Brides", "Grooms")
chisq.test(lighterdarkersame_chi_table)
#-->p<2.2e-16
###visualize 
#need to make new proportions excluding doesnt matters
nbrides_excl_DMs=nrow(subset(castes30plus_only_new_brides_comp1_comp2, complexion1!="Doesn't Matter" & complexion2!="Doesn't Matter")) 
ngrooms_excl_DMs=nrow(subset(castes30plus_only_new_grooms_comp1_comp2, complexion1!="Doesn't Matter" & complexion2!="Doesn't Matter")) 
BridesWantingSame_newprop=BridesWantingSame_number/nbrides_excl_DMs
BridesWantingDarker_newprop=BridesWantingDarker_number/nbrides_excl_DMs
BridesWantingLighter_newprop=BridesWantingLighter_number/nbrides_excl_DMs
GroomsWantingSame_newprop=GroomsWantingSame_number/ngrooms_excl_DMs
GroomsWantingDarker_newprop=GroomsWantingDarker_number/ngrooms_excl_DMs
GroomsWantingLighter_newprop=GroomsWantingLighter_number/ngrooms_excl_DMs
wantedskin_new=c(GroomsWantingLighter_newprop, GroomsWantingSame_newprop, GroomsWantingDarker_newprop, BridesWantingLighter_newprop, BridesWantingSame_newprop, BridesWantingDarker_newprop)
Gender=c("Men", "Men", "Men","Women", "Women", "Women")
skincategory_new=c("Want Lighter Than Themselves", "Want Same As Themselves", "Want Darker Than Themselves", "Want Lighter Than Themselves", "Want Same As Themselves", "Want Darker Than Themselves")
wantedskin_table_new=data.frame(wantedskin_new, Gender, skincategory_new)
#visualise
library(ggplot2)
ggplot(wantedskin_table_new, 
       aes(x = skincategory_new , y=wantedskin_new, fill=Gender)) +
  geom_bar(stat='identity', position="dodge") +
  ggtitle("Proportion of Men and Women wanting Future Spouse to be of Darker, Lighter or the Same Skin Complexion as Themsleves") +
  labs(y="Proportion of Men and Women", x="Men and Women's Desired Skin Complexion in Future Spouse")






#ASSORTATIVE MATING - hypothesis 2
#identifying assortative mating i.e. proportion of those who want same skin colour as themselves in dataset including comp1 and comp2 - to test Hypothesis 2
#first create new dataset where there are entries for both comp1 and comp2
castes30plus_only_new_brides_comp1_comp2=castes30plus_only_new_brides_comp2[!is.na(castes30plus_only_new_brides_comp2$complexion1), ]
castes30plus_only_new_grooms_comp1_comp2=castes30plus_only_new_grooms_comp2[!is.na(castes30plus_only_new_grooms_comp2$complexion1), ]

Prop_VeryFairBride_WantingVeryFair=nrow(subset(castes30plus_only_new_brides_comp1_comp2, complexion1=="Very Fair" & complexion2=="Very Fair"))/nrow(subset(castes30plus_only_new_brides_comp1_comp2, complexion1=="Very Fair"))
Prop_FairBride_WantingFair=nrow(subset(castes30plus_only_new_brides_comp1_comp2, complexion1=="Fair" & complexion2=="Fair"))/nrow(subset(castes30plus_only_new_brides_comp1_comp2, complexion1=="Fair"))
Prop_WheatishBride_WantingWheatish=nrow(subset(castes30plus_only_new_brides_comp1_comp2, complexion1=="Wheatish" & complexion2=="Wheatish"))/nrow(subset(castes30plus_only_new_brides_comp1_comp2, complexion1=="Wheatish"))
Prop_WhetishMediumBride_WantingWhetishMedium=nrow(subset(castes30plus_only_new_brides_comp1_comp2, complexion1=="Whetish Medium" & complexion2=="Whetish Medium"))/nrow(subset(castes30plus_only_new_brides_comp1_comp2, complexion1=="Whetish Medium"))
Prop_DarkBride_WantingDark=nrow(subset(castes30plus_only_new_brides_comp1_comp2, complexion1=="Dark" & complexion2=="Dark"))/nrow(subset(castes30plus_only_new_brides_comp1_comp2, complexion1=="Dark"))
Prop_DoesntMatterBride_WantingDoesntMatter=nrow(subset(castes30plus_only_new_brides_comp1_comp2, complexion1=="Doesn't Matter" & complexion2=="Doesn't Matter"))/nrow(subset(castes30plus_only_new_brides_comp1_comp2, complexion1=="Doesn't Matter"))

Prop_VeryFairGroom_WantingVeryFair=nrow(subset(castes30plus_only_new_grooms_comp1_comp2, complexion1=="Very Fair" & complexion2=="Very Fair"))/nrow(subset(castes30plus_only_new_grooms_comp1_comp2, complexion1=="Very Fair"))
Prop_FairGroom_WantingFair=nrow(subset(castes30plus_only_new_grooms_comp1_comp2, complexion1=="Fair" & complexion2=="Fair"))/nrow(subset(castes30plus_only_new_grooms_comp1_comp2, complexion1=="Fair"))
Prop_WheatishGroom_WantingWheatish=nrow(subset(castes30plus_only_new_grooms_comp1_comp2, complexion1=="Wheatish" & complexion2=="Wheatish"))/nrow(subset(castes30plus_only_new_grooms_comp1_comp2, complexion1=="Wheatish"))
Prop_WhetishMediumGroom_WantingWhetishMedium=nrow(subset(castes30plus_only_new_grooms_comp1_comp2, complexion1=="Whetish Medium" & complexion2=="Whetish Medium"))/nrow(subset(castes30plus_only_new_grooms_comp1_comp2, complexion1=="Whetish Medium"))
Prop_DarkGroom_WantingDark=nrow(subset(castes30plus_only_new_grooms_comp1_comp2, complexion1=="Dark" & complexion2=="Dark"))/nrow(subset(castes30plus_only_new_grooms_comp1_comp2, complexion1=="Dark"))
Prop_DoesntMatterGroom_WantingDoesntMatter=nrow(subset(castes30plus_only_new_grooms_comp1_comp2, complexion1=="Doesn't Matter" & complexion2=="Doesn't Matter"))/nrow(subset(castes30plus_only_new_grooms_comp1_comp2, complexion1=="Doesn't Matter"))



#look at proportion of brides and grooms, respectively, who want spouse to have same or darker or lighter complexion than themselves
#create index of preference of skin complexion first in datasets including entries for comp1 and comp2
#brides:
castes30plus_only_new_brides_comp1_comp2$complexion1code=NA
castes30plus_only_new_brides_comp1_comp2$complexion2code=NA
castes30plus_only_new_brides_comp1_comp2$complexion1code[which(castes30plus_only_new_brides_comp1_comp2$complexion1=="Very Fair")]=1
castes30plus_only_new_brides_comp1_comp2$complexion2code[which(castes30plus_only_new_brides_comp1_comp2$complexion2=="Very Fair")]=1
castes30plus_only_new_brides_comp1_comp2$complexion1code[which(castes30plus_only_new_brides_comp1_comp2$complexion1=="Fair")]=2
castes30plus_only_new_brides_comp1_comp2$complexion2code[which(castes30plus_only_new_brides_comp1_comp2$complexion2=="Fair")]=2
castes30plus_only_new_brides_comp1_comp2$complexion1code[which(castes30plus_only_new_brides_comp1_comp2$complexion1=="Wheatish")]=3
castes30plus_only_new_brides_comp1_comp2$complexion2code[which(castes30plus_only_new_brides_comp1_comp2$complexion2=="Wheatish")]=3
castes30plus_only_new_brides_comp1_comp2$complexion1code[which(castes30plus_only_new_brides_comp1_comp2$complexion1=="Whetish Medium")]=4
castes30plus_only_new_brides_comp1_comp2$complexion2code[which(castes30plus_only_new_brides_comp1_comp2$complexion2=="Whetish Medium")]=4
castes30plus_only_new_brides_comp1_comp2$complexion1code[which(castes30plus_only_new_brides_comp1_comp2$complexion1=="Dark")]=5
castes30plus_only_new_brides_comp1_comp2$complexion2code[which(castes30plus_only_new_brides_comp1_comp2$complexion2=="Dark")]=5
castes30plus_only_new_brides_comp1_comp2$complexion1code[which(castes30plus_only_new_brides_comp1_comp2$complexion1=="Doesn't Matter")]=NA
castes30plus_only_new_brides_comp1_comp2$complexion2code[which(castes30plus_only_new_brides_comp1_comp2$complexion2=="Doesn't Matter")]=NA
#create indices for grooms too
castes30plus_only_new_grooms_comp1_comp2$complexion1code=NA
castes30plus_only_new_grooms_comp1_comp2$complexion2code=NA
castes30plus_only_new_grooms_comp1_comp2$complexion1code[which(castes30plus_only_new_grooms_comp1_comp2$complexion1=="Very Fair")]=1
castes30plus_only_new_grooms_comp1_comp2$complexion2code[which(castes30plus_only_new_grooms_comp1_comp2$complexion2=="Very Fair")]=1
castes30plus_only_new_grooms_comp1_comp2$complexion1code[which(castes30plus_only_new_grooms_comp1_comp2$complexion1=="Fair")]=2
castes30plus_only_new_grooms_comp1_comp2$complexion2code[which(castes30plus_only_new_grooms_comp1_comp2$complexion2=="Fair")]=2
castes30plus_only_new_grooms_comp1_comp2$complexion1code[which(castes30plus_only_new_grooms_comp1_comp2$complexion1=="Wheatish")]=3
castes30plus_only_new_grooms_comp1_comp2$complexion2code[which(castes30plus_only_new_grooms_comp1_comp2$complexion2=="Wheatish")]=3
castes30plus_only_new_grooms_comp1_comp2$complexion1code[which(castes30plus_only_new_grooms_comp1_comp2$complexion1=="Whetish Medium")]=4
castes30plus_only_new_grooms_comp1_comp2$complexion2code[which(castes30plus_only_new_grooms_comp1_comp2$complexion2=="Whetish Medium")]=4
castes30plus_only_new_grooms_comp1_comp2$complexion1code[which(castes30plus_only_new_grooms_comp1_comp2$complexion1=="Dark")]=5
castes30plus_only_new_grooms_comp1_comp2$complexion2code[which(castes30plus_only_new_grooms_comp1_comp2$complexion2=="Dark")]=5
castes30plus_only_new_grooms_comp1_comp2$complexion1code[which(castes30plus_only_new_grooms_comp1_comp2$complexion1=="Doesn't Matter")]=NA
castes30plus_only_new_grooms_comp1_comp2$complexion2code[which(castes30plus_only_new_grooms_comp1_comp2$complexion2=="Doesn't Matter")]=NA
#now create complexion code ratio in each dataset for next few analyses
castes30plus_only_new_brides_comp1_comp2$code_ratio12=castes30plus_only_new_brides_comp1_comp2$complexion1code/castes30plus_only_new_brides_comp1_comp2$complexion2code
castes30plus_only_new_grooms_comp1_comp2$code_ratio12=castes30plus_only_new_grooms_comp1_comp2$complexion1code/castes30plus_only_new_grooms_comp1_comp2$complexion2code
#now find total proportion of females wanting or don't mind spouse darker
BridesWantingDarker=nrow(subset(castes30plus_only_new_brides_comp1_comp2, code_ratio12<1))/nrow(castes30plus_only_new_brides_comp1_comp2)
#now find total proportion of males wanting or don't mind spouse darker
GroomsWantingDarker=nrow(subset(castes30plus_only_new_grooms_comp1_comp2, code_ratio12<1))/nrow(castes30plus_only_new_grooms_comp1_comp2)
#now find total proportion of females wanting spouse lighter
BridesWantingLighter=nrow(subset(castes30plus_only_new_brides_comp1_comp2, code_ratio12>1))/nrow(castes30plus_only_new_brides_comp1_comp2)
#now find total proportion of males wanting spouse lighter
GroomsWantingLighter=nrow(subset(castes30plus_only_new_grooms_comp1_comp2, code_ratio12>1))/nrow(castes30plus_only_new_grooms_comp1_comp2)
#now find total proportion of females wanting spouse to have same skin complexion
BridesWantingSame=nrow(subset(castes30plus_only_new_brides_comp1_comp2, code_ratio12==1))/nrow(castes30plus_only_new_brides_comp1_comp2)
#now find total proportion of males wanting spouse to have same skin complexion
GroomsWantingSame=nrow(subset(castes30plus_only_new_grooms_comp1_comp2, code_ratio12==1))/nrow(castes30plus_only_new_grooms_comp1_comp2)

##now run TEST and VISUALISATION of assortative mating using chi sq. test of independence to test hypothesis 2
#do chi sqaured test of independence to see if skin complexion you are affects what you want
####BRIDES
#make crosstab and run chi squared test
xtab3=table(factor(castes30plus_only_new_brides_comp1_comp2$complexion2, levels=c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark", "Doesn't Matter")), droplevels(factor(castes30plus_only_new_brides_comp1_comp2$complexion1, levels = c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark"))))
xtab3chisqt=chisq.test(xtab3)
#--> significant difference in what someone wants depending on own skin colour
#visualise
corrplot(xtab3chisqt$residuals, is.cor = FALSE, title="Corrplot of women's self-stated and desired skin pigmentation", mar=c(0,0,2,0), col=colorRampPalette(c("orange","white","green"))(200))
####same but for GROOMS
#make crosstab and run chi squared test
xtab5=table(factor(castes30plus_only_new_grooms_comp1_comp2$complexion2, levels=c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark", "Doesn't Matter")), droplevels(factor(castes30plus_only_new_grooms_comp1_comp2$complexion1, levels = c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark"))))
xtab5chisqt=chisq.test(xtab5)
#--> significant difference in what someone wants depending on own skin colour
#visualise
tiff("Grooms_corrplot_assortativeskin_alloptions.tiff", height=1000, width=800, res=96)
corrplot(xtab5chisqt$residuals, is.cor = FALSE, title="Corrplot of men's self-stated and desired skin pigmentation", col=colorRampPalette(c("orange","white","green"))(200), mar=c(0,0,2,0))
dev.off()





###model what variables predict positive assortative mating
#now add compsame column to main dataframe with added variables 
castes30plus_only2_new_andstates4$compsame[castes30plus_only2_new_andstates4$complexion1==castes30plus_only2_new_andstates4$complexion2]=1
castes30plus_only2_new_andstates4$compsame[castes30plus_only2_new_andstates4$complexion1!=castes30plus_only2_new_andstates4$complexion2]=0
castes30plus_only2_new_andstates4$BirthYear <- substr(castes30plus_only2_new_andstates4$birth1, 8, 12)
#set blanks to NA
castes30plus_only2_new_andstates4 <- castes30plus_only2_new_andstates4 %>% dplyr::  mutate_all(na_if,"")
#row run regression 
castes30plus_only2_new_andstates4_new=castes30plus_only2_new_andstates4[!is.na(castes30plus_only2_new_andstates4$compsame)]
castes30plus_only2_new_andstates4_new=castes30plus_only2_new_andstates4_new[!is.na(castes30plus_only2_new_andstates4_new$BirthYear)]
castes30plus_only2_new_andstates4_new=subset(castes30plus_only2_new_andstates4_new, complexion1!="Doesn't Matter")
ord.comp1=factor(castes30plus_only2_new_andstates4_new$complexion1, level=c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark"), ordered=TRUE)
castes30plus_only2_new_andstates4_new$BirthYear = as.numeric(as.character(castes30plus_only2_new_andstates4_new$BirthYear))
assortativemodelling=glm(compsame~ord.comp1+source+BirthYear+created_for, data=castes30plus_only2_new_andstates4_new, family=binomial)
summary(assortativemodelling)
#run stepAIC
assortativemodelBest=stepAIC(assortativemodelling)
summary(assortativemodelBest)
library(car)
vif(assortativemodelBest)
#--> stepwise keeps all variables 
####visualize in 3 plots 
#start with complexion1
VeryFairdataframe=data.frame(ord.comp1="Very Fair", source="Brides", BirthYear=1938:1995, created_for="Self")
Fairdataframe=data.frame(ord.comp1="Fair", source="Brides", BirthYear=1938:1995, created_for="Self")
Wheatishdataframe=data.frame(ord.comp1="Wheatish", source="Brides", BirthYear=1938:1995, created_for="Self")
WhetishMediumdataframe=data.frame(ord.comp1="Whetish Medium", source="Brides", BirthYear=1938:1995, created_for="Self")
Darkdataframe=data.frame(ord.comp1="Dark", source="Brides", BirthYear=1938:1995, created_for="Self")
predictVeryFair=predict(assortativemodelling, newdata=VeryFairdataframe, type="response")
predictFair=predict(assortativemodelling, newdata=Fairdataframe, type="response")
predictWheatish=predict(assortativemodelling, newdata=Wheatishdataframe, type="response")
predictWhetishMedium=predict(assortativemodelling, newdata=WhetishMediumdataframe, type="response")
predictDark=predict(assortativemodelling, newdata=Darkdataframe, type="response")
tiff("glm_comp1predictingprobassortative.tiff", res=96, height=1000, width=1000)
plot(1938:1995, predictVeryFair, xlab="Birth Year", ylab="Probability of wishing to marry a spouse of the same skin complexion as oneself", col="navy", type = "l", lwd=3, cex.lab=1.5, ylim=c(0,0.5))
lines(1938:1995, predictFair, col="indianred", lwd=3)
lines(1938:1995, predictWheatish, col="orange", lwd=3)
lines(1938:1995, predictWhetishMedium, col="tan3", lwd=3)
lines(1938:1995, predictDark, col="maroon1", lwd=3)
legend(x="topright", legend=c( "Fair","Very Fair", "Wheatish", "Whetish Medium", "Dark"), col=c( "indianred", "navy","orange", "tan3", "maroon1"), title = "Self-stated skin complexion", cex=1.8, pch=15)
dev.off()
#do same visualization but based on sex
mdataframe=data.frame(ord.comp1="Fair", source="Grooms", BirthYear=1938:1995, created_for="Self")
fdataframe=data.frame(ord.comp1="Fair", source="Brides", BirthYear=1938:1995, created_for="Self")
predictmales=predict(assortativemodelling, newdata=mdataframe, type="response")
predictfemales=predict(assortativemodelling, newdata=fdataframe, type="response")
tiff("glm_sexpredictingprobassortative.tiff", res=96, height=1000, width=1000)
plot(1938:1995, predictmales, xlab="Birth Year", ylab="Probability of wishing to marry a spouse of the same skin complexion as oneself", col="lawngreen", type = "l", lwd=3, cex.lab=1.5, ylim=c(0,0.5))
lines(1938:1995, predictfemales, col="mediumpurple1", lwd=3)
legend(x="topright", legend=c("Male", "Female"), col=c("lawngreen", "mediumpurple1"), title = "Gender", cex=1.8, pch=15)
dev.off()
#do same visualization but based on who created ad 
forDaughter_data=data.frame(ord.comp1="Fair", source="Brides", BirthYear=1938:1995, created_for="Daughter")
forFriend_data=data.frame(ord.comp1="Fair", source="Brides", BirthYear=1938:1995, created_for="Friend")
forRelative_data=data.frame(ord.comp1="Fair", source="Brides", BirthYear=1938:1995, created_for="Relative")
forSelf_data=data.frame(ord.comp1="Fair", source="Brides", BirthYear=1938:1995, created_for="Self")
forSister_data=data.frame(ord.comp1="Fair", source="Brides", BirthYear=1938:1995, created_for="Sister")
forSon_data=data.frame(ord.comp1="Fair", source="Brides", BirthYear=1938:1995, created_for="Son")
predict_forDaughter=predict(assortativemodelling, newdata=forDaughter_data, type="response")
predict_forFriend=predict(assortativemodelling, newdata=forFriend_data, type="response")
predict_forRelative=predict(assortativemodelling, newdata=forRelative_data, type="response")
predict_forSelf=predict(assortativemodelling, newdata=forSelf_data, type="response")
predict_forSister=predict(assortativemodelling, newdata=forSister_data, type="response")
predict_forSon=predict(assortativemodelling, newdata=forSon_data, type="response")
tiff("glm_admakerpredictingprob_assortative.tiff", res=96, height=1000, width=1000)
plot(1938:1995, predict_forDaughter, xlab="Birth Year", ylab="Probability of wishing to marry a spouse of the same skin complexion as oneself", col="hotpink", type = "l", lwd=3, cex.lab=1.5, ylim=c(0,0.5))
lines(1938:1995, predict_forFriend, col="palegreen1", lwd=3)
lines(1938:1995, predict_forRelative, col="orangered", lwd=3)
lines(1938:1995, predict_forSelf, col="peru", lwd=3)
lines(1938:1995, predict_forSister, col="royalblue", lwd=3)
lines(1938:1995, predict_forSon, col="yellow", lwd=3)
legend(x="topright", legend=c("Son", "Daughter", "Friend", "Self", "Sister", "Relative"), col=c("yellow", "hotpink", "palegreen1", "peru", "royalblue", "orangered"), title = "Matrimonial advertisment made for:", cex=1.8, pch=15)
dev.off()








#see if there's statistical difference between what men want and what women self-report to test Hypothesis 4
xtab7=table(factor(castes30plus_only_new_grooms_comp1_comp2$complexion2, levels=c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark")))
xtab8=table(factor(castes30plus_only_new_brides_comp1_comp2$complexion1, levels=c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark")))
xtab9=prop.table(xtab7)
chisq.test(x=xtab8, p=xtab9)
#repeat for what women want and what men self-report
xtab10=table(factor(castes30plus_only_new_grooms_comp1_comp2$complexion1, levels=c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark")))
xtab11=table(factor(castes30plus_only_new_brides_comp1_comp2$complexion2, levels=c("Very Fair", "Fair", "Wheatish", "Whetish Medium", "Dark")))
xtab12=prop.table(xtab11)
chisq.test(x=xtab10, p=xtab12)




#find correlation between pickiness for caste same and complexion same as oneself  in future spouse using chi squared test of independence
#first make aggregated table of castes against caste same and different counts and complexion same and different counts 
#need to create subset of csates30plus_new with all ads having put something for caste1_1, 2_1, complexion1 and 2
# make the subsets
castes30plus_new_caste1_1_caste2_1_comp1=castes30plus_new_caste1_1_caste2_1[!is.na(castes30plus_new_caste1_1_caste2_1$complexion1), ]
castes30plus_new_caste1_1_caste2_1_comp1_comp2=castes30plus_new_caste1_1_caste2_1_comp1[!is.na(castes30plus_new_caste1_1_caste2_1_comp1$complexion2), ]
#now exclude any columns with "Doesn't Matter" in comp1 and comp2
castes30plus_new_caste1_1_caste2_1_comp1_comp2=castes30plus_new_caste1_1_caste2_1_comp1_comp2[!castes30plus_new_caste1_1_caste2_1_comp1_comp2$complexion1=="Doesn't Matter"]
castes30plus_new_caste1_1_caste2_1_comp1_comp2=castes30plus_new_caste1_1_caste2_1_comp1_comp2[!castes30plus_new_caste1_1_caste2_1_comp1_comp2$complexion2=="Doesn't Matter"]
#get rid of NAs 
castes30plus_new_caste1_1_caste2_1_comp1_comp2 <- castes30plus_new_caste1_1_caste2_1_comp1_comp2 %>% mutate_all(na_if,"")
castes30plus_new_caste1_1_caste2_1_comp1_comp2=castes30plus_new_caste1_1_caste2_1_comp1_comp2[!is.na(castes30plus_new_caste1_1_caste2_1_comp1_comp2$complexion1)]
castes30plus_new_caste1_1_caste2_1_comp1_comp2=castes30plus_new_caste1_1_caste2_1_comp1_comp2[!is.na(castes30plus_new_caste1_1_caste2_1_comp1_comp2$complexion2)]
#add same and different columns
castes30plus_new_caste1_1_caste2_1_comp1_comp2$caste1_1_2_1_same=0
castes30plus_new_caste1_1_caste2_1_comp1_comp2$caste1_1_2_1_same[which(castes30plus_new_caste1_1_caste2_1_comp1_comp2$caste1_1==castes30plus_new_caste1_1_caste2_1_comp1_comp2$caste2_1)]=1
castes30plus_new_caste1_1_caste2_1_comp1_comp2$caste1_1_2_1_different=0
castes30plus_new_caste1_1_caste2_1_comp1_comp2$caste1_1_2_1_different[which(castes30plus_new_caste1_1_caste2_1_comp1_comp2$caste1_1!=castes30plus_new_caste1_1_caste2_1_comp1_comp2$caste2_1)]=1
castes30plus_new_caste1_1_caste2_1_comp1_comp2$comp1_comp2_same=0
castes30plus_new_caste1_1_caste2_1_comp1_comp2$comp1_comp2_same[which(castes30plus_new_caste1_1_caste2_1_comp1_comp2$complexion1==castes30plus_new_caste1_1_caste2_1_comp1_comp2$complexion2)]=1
castes30plus_new_caste1_1_caste2_1_comp1_comp2$comp1_comp2_different=0
castes30plus_new_caste1_1_caste2_1_comp1_comp2$comp1_comp2_different[which(castes30plus_new_caste1_1_caste2_1_comp1_comp2$complexion1!=castes30plus_new_caste1_1_caste2_1_comp1_comp2$complexion2)]=1

#now do chi squared test of independence on total dataset
CasteSameSum=nrow(castes30plus_new_caste1_1_caste2_1_comp1_comp2[castes30plus_new_caste1_1_caste2_1_comp1_comp2$caste1_1==castes30plus_new_caste1_1_caste2_1_comp1_comp2$caste2_1])
CasteDifferentSum=nrow(castes30plus_new_caste1_1_caste2_1_comp1_comp2[castes30plus_new_caste1_1_caste2_1_comp1_comp2$caste1_1!=castes30plus_new_caste1_1_caste2_1_comp1_comp2$caste2_1])
CompSameSum=nrow(castes30plus_new_caste1_1_caste2_1_comp1_comp2[castes30plus_new_caste1_1_caste2_1_comp1_comp2$complexion1==castes30plus_new_caste1_1_caste2_1_comp1_comp2$complexion2])
CompDifferentSum=nrow(castes30plus_new_caste1_1_caste2_1_comp1_comp2[castes30plus_new_caste1_1_caste2_1_comp1_comp2$complexion1!=castes30plus_new_caste1_1_caste2_1_comp1_comp2$complexion2])
chisq_table_castes_comp_castes30plus <- matrix(c(CasteSameSum, CompSameSum, CasteDifferentSum, CompDifferentSum), ncol=2, byrow=TRUE)
colnames(chisq_table_castes_comp_castes30plus) <- c("caste1_1","complexion")
rownames(chisq_table_castes_comp_castes30plus) <- c("same","different")
chisq_table_castes_comp_castes30plus <- as.table(chisq_table_castes_comp_castes30plus)
chisqt=chisq.test(chisq_table_castes_comp_castes30plus)
chisqt
#visualisation
library(corrplot)
corrplot_caste_complexion = corrplot(chisqt$residuals, is.cor = FALSE, col=colorRampPalette(c("blue","white","red"))(200), mar=c(0,0,1,0), addCoef.col ="white")








####### MI dataset analyses
##see if MI data has any common population with matrimonial 
MI_data=read.csv("MI_data.csv", na.strings = " ", header=TRUE)
colnames(MI_data)=c("caste1", "State", "MI", "SNPrs1426654", "Publication")
#first make caste1 freq table
library(plyr)
castes30plus_only_new_andstates4_caste1freqtable = castes30plus_only_new_andstates4 %>% 
  dplyr:: count(caste1)
#now merge to see if it has castes in common with matrimonial data 
MI_data_merge=merge(MI_data, castes30plus_only_new_andstates4_caste1freqtable, by="caste1", all.x=TRUE)
##--> Badaga, Brahmin, Kapu, Naidu, Reddy match plus also have Brahmin Uttar Pradesh and can find Brahmin Deshastha and Yadav(a), then from Mishra there is Kshatriya, Paswan, Thakur, plus Bumihar, Muslim, Srivastava, Rai (Yadav, Bihar), Kurmi 
#set comp1 to NA when blanks
castes30plus_only_new_andstates4$complexion1 <- as.character(castes30plus_only_new_andstates4$complexion1)
castes30plus_only_new_andstates4$complexion1[castes30plus_only_new_andstates4$complexion1==""] <- NA
castes30plus_only_new_andstates4$complexion1 <- as.factor(castes30plus_only_new_andstates4$complexion1)
#manually add matrimonial n to MI_data_merge table for 2 other populations, replace Brahmins in MI data with Tamil Nadu Brahmins in matimonial dataset only, add Kunbis from Maharashtra from matrimonial to MI table and update caste numbers taking into account state
Badaga_TN=subset(castes30plus_only_new_andstates4, caste1_1=="Badaga" & State=="Tamil Nadu")
Brahmin_TN=subset(castes30plus_only_new_andstates4, caste1_1=="Brahmin" & State=="Tamil Nadu")
Brahmin_UP=subset(castes30plus_only_new_andstates4, caste1_1=="Brahmin" & State=="Uttar Pradesh")
Deshastha_Brahmin=subset(castes30plus_only_new_andstates4, caste1=="Brahmin-Deshastha"|caste1_1=="Brahmin" & subcaste=="Deshastha")
Kapu_AP=subset(castes30plus_only_new_andstates4, caste1_1=="Kapu" & State=="Andhra Pradesh"|caste1_1=="Kapu Naidu" & State=="Andhra Pradesh"|caste1_1=="Munnuru Kapu" & State=="Andhra Pradesh")
Kunbi_Maratha=subset(castes30plus_only_new_andstates4, caste1=="Maratha" & subcaste=="Kunbi Maratha")
Naidu_AP=subset(castes30plus_only_new_andstates4, caste1=="Naidu" & State=="Andhra Pradesh")
Reddy_AP=subset(castes30plus_only_new_andstates4, caste1=="Reddy" & State=="Andhra Pradesh")
Yadava_TN=subset(castes30plus_only_new_andstates4, caste1=="Yadav" & State=="Tamil Nadu")
Bhumihar_grep=castes30plus_only_new_andstates4[grep("Bhumihar", castes30plus_only_new_andstates4$caste1), ]
Bhumihar_UP_B=subset(Bhumihar_grep, State=="Uttar Pradesh"|State=="Bihar")
Kshatriya_grep=castes30plus_only_new_andstates4[grep("Kshatriya", castes30plus_only_new_andstates4$caste1), ]
Kshatriya_UP_B=subset(Kshatriya_grep, State=="Bihar"|State=="Uttar Pradesh")
Kurmi_grep=castes30plus_only_new_andstates4[grep("Kurmi", castes30plus_only_new_andstates4$caste1), ]
Kurmi_UP=subset(Kurmi_grep, State=="Uttar Pradesh")
Muslim_B=subset(castes30plus_only_new_andstates4, religion1=="Muslim"&State=="Bihar")
Paswan_grep=castes30plus_only_new_andstates4[grep("Paswan", castes30plus_only_new_andstates4$caste1), ]
Paswan_B=subset(Paswan_grep, State=="Bihar")
Yadav_B=subset(castes30plus_only_new_andstates4, caste1=="Yadav" & State=="Bihar")
Srivastava_UP_B=subset(castes30plus_only_new_andstates4, (subcaste=="Srivastava" & State=="Bihar")|(subcaste=="Srivastava" & State=="Uttar Pradesh"))
Thakur_B=subset(castes30plus_only_new_andstates4, (caste1=="Thakur" & State=="Bihar")|(subcaste=="Thakur"&State=="Bihar"))

MI_data_merge$n[MI_data_merge$caste1=="Badaga"]=nrow(Badaga_TN)
MI_data_merge$n[MI_data_merge$caste1=="Brahmin"]=nrow(Brahmin_TN)
MI_data_merge$n[MI_data_merge$caste1=="BrahminUP"]=nrow(Brahmin_UP)
MI_data_merge$n[MI_data_merge$caste1=="Deshastha Brahmin"]=nrow(Deshastha_Brahmin)
MI_data_merge$n[MI_data_merge$caste1=="Kapu"]=nrow(Kapu_AP)
MI_data_merge$n[MI_data_merge$caste1=="Kunbi Maratha"]=nrow(Kunbi_Maratha)
MI_data_merge$n[MI_data_merge$caste1=="Naidu"]=nrow(Naidu_AP)
MI_data_merge$n[MI_data_merge$caste1=="Reddy"]=nrow(Reddy_AP)
MI_data_merge$n[MI_data_merge$caste1=="Yadava"]=nrow(Yadava_TN)
MI_data_merge$n[MI_data_merge$caste1=="Bhumihar"]=nrow(Bhumihar_UP_B)
MI_data_merge$n[MI_data_merge$caste1=="Kshatriya"]=nrow(Kshatriya_UP_B)
MI_data_merge$n[MI_data_merge$caste1=="Kurmis"]=nrow(Kurmi_UP)
MI_data_merge$n[MI_data_merge$caste1=="Muslim"]=nrow(Muslim_B)
MI_data_merge$n[MI_data_merge$caste1=="Paswan"]=nrow(Paswan_B)
MI_data_merge$n[MI_data_merge$caste1=="Yadav"]=nrow(Yadav_B)
MI_data_merge$n[MI_data_merge$caste1=="Srivastava"]=nrow(Srivastava_UP_B)
MI_data_merge$n[MI_data_merge$caste1=="Thakur"]=nrow(Thakur_B)
#--> 17 populations have MI and matrimonial data 
#export list of 17 populations with MI average and matrimonial_n
MI_17pops=subset(MI_data_merge, n!="NA")
MI_17pops=MI_17pops[, c(1,2,3,4,5,6,11)]
library(stringr)
library(dplyr)
MI_17pops <- MI_17pops %>% mutate(Publication = str_replace_all(Publication, "\\s*\\([^\\)]+\\)",""))
MI_17pops$Publication[MI_17pops$Publication=="Iliescu"]="Iliescu et al 2018"
MI_17pops$Publication[MI_17pops$Publication=="Jonnagalada"]="Jonnagalada et al 2015"
MI_17pops$Publication[MI_17pops$Publication=="Mishra"]="Mishra et al 2017"
colnames(MI_17pops)=c("Caste", "State", "Average recorded MI", "rs1426654-A allele frequency", "Publication", "Matrimonial dataset sample size", "Average self-stated skin complexion index in matrimonial dataset")


##see if comp1 or comp2 missingness is correlated with darker castes 
#complexion2
#find proportion missingness in each population 
Badaga_comp2_missingness_prop=nrow(Badaga_TN[is.na(Badaga_TN$complexion2)])/nrow(Badaga_TN)
Brahmin_TN_comp2_missingness_prop=nrow(Brahmin_TN[is.na(Brahmin_TN$complexion2)])/nrow(Brahmin_TN)
Brahmin_UP_comp2_missingness_prop=nrow(Brahmin_UP[is.na(Brahmin_UP$complexion2)])/nrow(Brahmin_UP)
Deshastha_Brahmin_comp2_missingness_prop=nrow(Deshastha_Brahmin[is.na(Deshastha_Brahmin$complexion2)])/nrow(Deshastha_Brahmin)
Kapu_AP_comp2_missingness_prop=nrow(Kapu_AP[is.na(Kapu_AP$complexion2)])/nrow(Kapu_AP)
Kunbi_Maratha_comp2_missingness_prop=nrow(Kunbi_Maratha[is.na(Kunbi_Maratha$complexion2)])/nrow(Kunbi_Maratha)
Naidu_AP_comp2_missingness_prop=nrow(Naidu_AP[is.na(Naidu_AP$complexion2)])/nrow(Naidu_AP)
Reddy_AP_comp2_missingness_prop=nrow(Reddy_AP[is.na(Reddy_AP$complexion2)])/nrow(Reddy_AP)
Yadava_TN_comp2_missingness_prop=nrow(Yadava_TN[is.na(Yadava_TN$complexion2)])/nrow(Yadava_TN)
Bhumihar_UP_B_comp2_missingness_prop=nrow(Bhumihar_UP_B[is.na(Bhumihar_UP_B$complexion2)])/nrow(Bhumihar_UP_B)
Kshatriya_UP_B_comp2_missingness_prop=nrow(Kshatriya_UP_B[is.na(Kshatriya_UP_B$complexion2)])/nrow(Kshatriya_UP_B)
Kurmi_UP_comp2_missingness_prop=nrow(Kurmi_UP[is.na(Kurmi_UP$complexion2)])/nrow(Kurmi_UP)
Muslim_B_comp2_missingness_prop=nrow(Muslim_B[is.na(Muslim_B$complexion2)])/nrow(Muslim_B)
Paswan_B_comp2_missingness_prop=nrow(Paswan_B[is.na(Paswan_B$complexion2)])/nrow(Paswan_B)
Yadav_B_comp2_missingness_prop=nrow(Yadav_B[is.na(Yadav_B$complexion2)])/nrow(Yadav_B)
Srivastava_UP_B_comp2_missingness_prop=nrow(Srivastava_UP_B[is.na(Srivastava_UP_B$complexion2)])/nrow(Srivastava_UP_B)
Thakur_B_comp2_missingness_prop=nrow(Thakur_B[is.na(Thakur_B$complexion2)])/nrow(Thakur_B)

#repeat for complexion1
#find proportion missingness in each population 
Badaga_comp1_missingness_prop=nrow(Badaga_TN[is.na(Badaga_TN$complexion1)])/nrow(Badaga_TN)
Brahmin_TN_comp1_missingness_prop=nrow(Brahmin_TN[is.na(Brahmin_TN$complexion1)])/nrow(Brahmin_TN)
Brahmin_UP_comp1_missingness_prop=nrow(Brahmin_UP[is.na(Brahmin_UP$complexion1)])/nrow(Brahmin_UP)
Deshastha_Brahmin_comp1_missingness_prop=nrow(Deshastha_Brahmin[is.na(Deshastha_Brahmin$complexion1)])/nrow(Deshastha_Brahmin)
Kapu_AP_comp1_missingness_prop=nrow(Kapu_AP[is.na(Kapu_AP$complexion1)])/nrow(Kapu_AP)
Kunbi_Maratha_comp1_missingness_prop=nrow(Kunbi_Maratha[is.na(Kunbi_Maratha$complexion1)])/nrow(Kunbi_Maratha)
Naidu_AP_comp1_missingness_prop=nrow(Naidu_AP[is.na(Naidu_AP$complexion1)])/nrow(Naidu_AP)
Reddy_AP_comp1_missingness_prop=nrow(Reddy_AP[is.na(Reddy_AP$complexion1)])/nrow(Reddy_AP)
Yadava_TN_comp1_missingness_prop=nrow(Yadava_TN[is.na(Yadava_TN$complexion1)])/nrow(Yadava_TN)
Bhumihar_UP_B_comp1_missingness_prop=nrow(Bhumihar_UP_B[is.na(Bhumihar_UP_B$complexion1)])/nrow(Bhumihar_UP_B)
Kshatriya_UP_B_comp1_missingness_prop=nrow(Kshatriya_UP_B[is.na(Kshatriya_UP_B$complexion1)])/nrow(Kshatriya_UP_B)
Kurmi_UP_comp1_missingness_prop=nrow(Kurmi_UP[is.na(Kurmi_UP$complexion1)])/nrow(Kurmi_UP)
Muslim_B_comp1_missingness_prop=nrow(Muslim_B[is.na(Muslim_B$complexion1)])/nrow(Muslim_B)
Paswan_B_comp1_missingness_prop=nrow(Paswan_B[is.na(Paswan_B$complexion1)])/nrow(Paswan_B)
Yadav_B_comp1_missingness_prop=nrow(Yadav_B[is.na(Yadav_B$complexion1)])/nrow(Yadav_B)
Srivastava_UP_B_comp1_missingness_prop=nrow(Srivastava_UP_B[is.na(Srivastava_UP_B$complexion1)])/nrow(Srivastava_UP_B)
Thakur_B_comp1_missingness_prop=nrow(Thakur_B[is.na(Thakur_B$complexion1)])/nrow(Thakur_B)

#add comp1 and comp2 missingness to MI_data_merge data 
MI_data_merge$comp1_missingness_prop=NA
MI_data_merge$comp2_missingness_prop=NA
#add proportions to table 
MI_data_merge$comp1_missingness_prop[MI_data_merge$caste1=="Badaga"]=Badaga_comp1_missingness_prop
MI_data_merge$comp2_missingness_prop[MI_data_merge$caste1=="Badaga"]=Badaga_comp2_missingness_prop
MI_data_merge$comp1_missingness_prop[MI_data_merge$caste1=="Brahmin"]=Brahmin_TN_comp1_missingness_prop
MI_data_merge$comp2_missingness_prop[MI_data_merge$caste1=="Brahmin"]=Brahmin_TN_comp2_missingness_prop
MI_data_merge$comp1_missingness_prop[MI_data_merge$caste1=="BrahminUP"]=Brahmin_UP_comp1_missingness_prop
MI_data_merge$comp2_missingness_prop[MI_data_merge$caste1=="BrahminUP"]=Brahmin_UP_comp2_missingness_prop
MI_data_merge$comp1_missingness_prop[MI_data_merge$caste1=="Deshastha Brahmin"]=Deshastha_Brahmin_comp1_missingness_prop 
MI_data_merge$comp2_missingness_prop[MI_data_merge$caste1=="Deshastha Brahmin"]=Deshastha_Brahmin_comp2_missingness_prop 
MI_data_merge$comp1_missingness_prop[MI_data_merge$caste1=="Kapu"]=Kapu_AP_comp1_missingness_prop 
MI_data_merge$comp2_missingness_prop[MI_data_merge$caste1=="Kapu"]=Kapu_AP_comp2_missingness_prop 
MI_data_merge$comp1_missingness_prop[MI_data_merge$caste1=="Kunbi Maratha"]=Kunbi_Maratha_comp1_missingness_prop 
MI_data_merge$comp2_missingness_prop[MI_data_merge$caste1=="Kunbi Maratha"]=Kunbi_Maratha_comp2_missingness_prop 
MI_data_merge$comp1_missingness_prop[MI_data_merge$caste1=="Reddy"]=Reddy_AP_comp1_missingness_prop 
MI_data_merge$comp2_missingness_prop[MI_data_merge$caste1=="Reddy"]=Reddy_AP_comp2_missingness_prop 
MI_data_merge$comp1_missingness_prop[MI_data_merge$caste1=="Naidu"]=Naidu_AP_comp1_missingness_prop 
MI_data_merge$comp2_missingness_prop[MI_data_merge$caste1=="Naidu"]=Naidu_AP_comp2_missingness_prop 
MI_data_merge$comp1_missingness_prop[MI_data_merge$caste1=="Yadava"]=Yadava_TN_comp1_missingness_prop 
MI_data_merge$comp2_missingness_prop[MI_data_merge$caste1=="Yadava"]=Yadava_TN_comp2_missingness_prop 
MI_data_merge$comp1_missingness_prop[MI_data_merge$caste1=="Bhumihar"]=Bhumihar_UP_B_comp1_missingness_prop
MI_data_merge$comp2_missingness_prop[MI_data_merge$caste1=="Bhumihar"]=Bhumihar_UP_B_comp2_missingness_prop 
MI_data_merge$comp1_missingness_prop[MI_data_merge$caste1=="Kshatriya"]=Kshatriya_UP_B_comp1_missingness_prop
MI_data_merge$comp2_missingness_prop[MI_data_merge$caste1=="Kshatriya"]=Kshatriya_UP_B_comp2_missingness_prop
MI_data_merge$comp1_missingness_prop[MI_data_merge$caste1=="Kurmis"]=Kurmi_UP_comp1_missingness_prop 
MI_data_merge$comp2_missingness_prop[MI_data_merge$caste1=="Kurmis"]=Kurmi_UP_comp2_missingness_prop
MI_data_merge$comp1_missingness_prop[MI_data_merge$caste1=="Muslim"]=Muslim_B_comp1_missingness_prop
MI_data_merge$comp2_missingness_prop[MI_data_merge$caste1=="Muslim "]=Muslim_B_comp2_missingness_prop 
MI_data_merge$comp1_missingness_prop[MI_data_merge$caste1=="Paswan"]=Paswan_B_comp1_missingness_prop 
MI_data_merge$comp2_missingness_prop[MI_data_merge$caste1=="Paswan"]=Paswan_B_comp2_missingness_prop 
MI_data_merge$comp1_missingness_prop[MI_data_merge$caste1=="Yadav"]=Yadav_B_comp1_missingness_prop 
MI_data_merge$comp2_missingness_prop[MI_data_merge$caste1=="Yadav"]=Yadav_B_comp2_missingness_prop 
MI_data_merge$comp1_missingness_prop[MI_data_merge$caste1=="Srivastava"]=Srivastava_UP_B_comp1_missingness_prop
MI_data_merge$comp2_missingness_prop[MI_data_merge$caste1=="Srivastava"]=Srivastava_UP_B_comp2_missingness_prop 
MI_data_merge$comp1_missingness_prop[MI_data_merge$caste1=="Thakur"]=Thakur_B_comp1_missingness_prop
MI_data_merge$comp2_missingness_prop[MI_data_merge$caste1=="Thakur"]=Thakur_B_comp2_missingness_prop 


#now run correlation for comp1 and comp2 missingness and MI 
#check if variables follow normal distribution
par(mfrow=c(1,1))
hist(MI_data_merge$MI)
hist(MI_data_merge$comp1_missingness_prop)
hist(MI_data_merge$comp2_missingness_prop)
#--> don't seem to follow normal distribution, but now let's plot for better visualisation of relationship
plot(MI_data_merge$MI, MI_data_merge$comp1_missingness_prop, xlab="Melanin Index", ylab="Proportion of caste not self-reporting skin complexion", main = "Melanin Index against proportion of caste with self-reported skin complexion information missingness", cex.lab=0.8, cex.main=1, pch=20)
plot(MI_data_merge$MI, MI_data_merge$comp2_missingness_prop, xlab="Melanin Index", ylab="Proportion of caste not stating desired skin complexion of future spouse", main = "Melanin Index against proportion of caste not stating desired skin complexion in future spouse", cex.lab=0.8, cex.main=1, pch=20)
#now run correlations 
cor.test(MI_data_merge$MI, MI_data_merge$comp1_missingness_prop, method="kendall")
cor.test(MI_data_merge$MI, MI_data_merge$comp2_missingness_prop, method="kendall")



#repeat correlations but don't include people saying "Doesn't Matter"
#complexion2
######find proportion missingness in each population also including "Doesn't Matter" as missing
Badaga_comp2_missingness_prop_new=(nrow(Badaga_TN[is.na(Badaga_TN$complexion2)])+nrow(subset(Badaga_TN, complexion2=="Doesn't Matter")))/nrow(Badaga_TN)
Brahmin_TN_comp2_missingness_prop_new=(nrow(Brahmin_TN[is.na(Brahmin_TN$complexion2)])+nrow(subset(Brahmin_TN, complexion2=="Doesn't Matter")))/nrow(Brahmin_TN)
Brahmin_UP_comp2_missingness_prop_new=(nrow(Brahmin_UP[is.na(Brahmin_UP$complexion2)])+nrow(subset(Brahmin_UP, complexion2=="Doesn't Matter")))/nrow(Brahmin_UP)
Deshastha_Brahmin_comp2_missingness_prop_new=(nrow(Deshastha_Brahmin[is.na(Deshastha_Brahmin$complexion2)])+nrow(subset(Deshastha_Brahmin, complexion2=="Doesn't Matter")))/nrow(Deshastha_Brahmin)
Kapu_AP_comp2_missingness_prop_new=(nrow(Kapu_AP[is.na(Kapu_AP$complexion2)])+nrow(subset(Kapu_AP, complexion2=="Doesn't Matter")))/nrow(Kapu_AP)
Kunbi_Maratha_comp2_missingness_prop_new=(nrow(Kunbi_Maratha[is.na(Kunbi_Maratha$complexion2)])+nrow(subset(Kunbi_Maratha, complexion2=="Doesn't Matter")))/nrow(Kunbi_Maratha)
Naidu_AP_comp2_missingness_prop_new=(nrow(Naidu_AP[is.na(Naidu_AP$complexion2)])+nrow(subset(Naidu_AP, complexion2=="Doesn't Matter")))/nrow(Naidu_AP)
Reddy_AP_comp2_missingness_prop_new=(nrow(Reddy_AP[is.na(Reddy_AP$complexion2)])+nrow(subset(Reddy_AP, complexion2=="Doesn't Matter")))/nrow(Reddy_AP)
Yadava_TN_comp2_missingness_prop_new=(nrow(Yadava_TN[is.na(Yadava_TN$complexion2)])+nrow(subset(Yadava_TN, complexion2=="Doesn't Matter")))/nrow(Yadava_TN)
Bhumihar_UP_B_comp2_missingness_prop_new=(nrow(Bhumihar_UP_B[is.na(Bhumihar_UP_B$complexion2)])+nrow(subset(Bhumihar_UP_B, complexion2=="Doesn't Matter")))/nrow(Bhumihar_UP_B)
Kshatriya_UP_B_comp2_missingness_prop_new=(nrow(Kshatriya_UP_B[is.na(Kshatriya_UP_B$complexion2)])+nrow(subset(Kshatriya_UP_B, complexion2=="Doesn't Matter")))/nrow(Kshatriya_UP_B)
Kurmi_UP_comp2_missingness_prop_new=(nrow(Kurmi_UP[is.na(Kurmi_UP$complexion2)])+nrow(subset(Kurmi_UP, complexion2=="Doesn't Matter")))/nrow(Kurmi_UP)
Muslim_B_comp2_missingness_prop_new=(nrow(Muslim_B[is.na(Muslim_B$complexion2)])+nrow(subset(Muslim_B, complexion2=="Doesn't Matter")))/nrow(Muslim_B)
Paswan_B_comp2_missingness_prop_new=(nrow(Paswan_B[is.na(Paswan_B$complexion2)])+nrow(subset(Paswan_B, complexion2=="Doesn't Matter")))/nrow(Paswan_B)
Srivastava_UP_B_comp2_missingness_prop_new=(nrow(Srivastava_UP_B[is.na(Srivastava_UP_B$complexion2)])+nrow(subset(Srivastava_UP_B, complexion2=="Doesn't Matter")))/nrow(Srivastava_UP_B)
Thakur_B_comp2_missingness_prop_new=(nrow(Thakur_B[is.na(Thakur_B$complexion2)])+nrow(subset(Thakur_B, complexion2=="Doesn't Matter")))/nrow(Thakur_B)
Yadav_B_comp2_missingness_prop_new=(nrow(Yadav_B[is.na(Yadav_B$complexion2)])+nrow(subset(Yadav_B, complexion2=="Doesn't Matter")))/nrow(Yadav_B)

#repeat for complexion1
Badaga_comp1_missingness_prop_new=(nrow(Badaga_TN[is.na(Badaga_TN$complexion1)])+nrow(subset(Badaga_TN, complexion1=="Doesn't Matter")))/nrow(Badaga_TN)
Brahmin_TN_comp1_missingness_prop_new=(nrow(Brahmin_TN[is.na(Brahmin_TN$complexion1)])+nrow(subset(Brahmin_TN, complexion1=="Doesn't Matter")))/nrow(Brahmin_TN)
Brahmin_UP_comp1_missingness_prop_new=(nrow(Brahmin_UP[is.na(Brahmin_UP$complexion1)])+nrow(subset(Brahmin_UP, complexion1=="Doesn't Matter")))/nrow(Brahmin_UP)
Deshastha_Brahmin_comp1_missingness_prop_new=(nrow(Deshastha_Brahmin[is.na(Deshastha_Brahmin$complexion1)])+nrow(subset(Deshastha_Brahmin, complexion1=="Doesn't Matter")))/nrow(Deshastha_Brahmin)
Kapu_AP_comp1_missingness_prop_new=(nrow(Kapu_AP[is.na(Kapu_AP$complexion1)])+nrow(subset(Kapu_AP, complexion1=="Doesn't Matter")))/nrow(Kapu_AP)
Kunbi_Maratha_comp1_missingness_prop_new=(nrow(Kunbi_Maratha[is.na(Kunbi_Maratha$complexion1)])+nrow(subset(Kunbi_Maratha, complexion1=="Doesn't Matter")))/nrow(Kunbi_Maratha)
Naidu_AP_comp1_missingness_prop_new=(nrow(Naidu_AP[is.na(Naidu_AP$complexion1)])+nrow(subset(Naidu_AP, complexion1=="Doesn't Matter")))/nrow(Naidu_AP)
Reddy_AP_comp1_missingness_prop_new=(nrow(Reddy_AP[is.na(Reddy_AP$complexion1)])+nrow(subset(Reddy_AP, complexion1=="Doesn't Matter")))/nrow(Reddy_AP)
Yadava_TN_comp1_missingness_prop_new=(nrow(Yadava_TN[is.na(Yadava_TN$complexion1)])+nrow(subset(Yadava_TN, complexion1=="Doesn't Matter")))/nrow(Yadava_TN)
Bhumihar_UP_B_comp1_missingness_prop_new=(nrow(Bhumihar_UP_B[is.na(Bhumihar_UP_B$complexion1)])+nrow(subset(Bhumihar_UP_B, complexion1=="Doesn't Matter")))/nrow(Bhumihar_UP_B)
Kshatriya_UP_B_comp1_missingness_prop_new=(nrow(Kshatriya_UP_B[is.na(Kshatriya_UP_B$complexion1)])+nrow(subset(Kshatriya_UP_B, complexion1=="Doesn't Matter")))/nrow(Kshatriya_UP_B)
Kurmi_UP_comp1_missingness_prop_new=(nrow(Kurmi_UP[is.na(Kurmi_UP$complexion1)])+nrow(subset(Kurmi_UP, complexion1=="Doesn't Matter")))/nrow(Kurmi_UP)
Muslim_B_comp1_missingness_prop_new=(nrow(Muslim_B[is.na(Muslim_B$complexion1)])+nrow(subset(Muslim_B, complexion1=="Doesn't Matter")))/nrow(Muslim_B)
Paswan_B_comp1_missingness_prop_new=(nrow(Paswan_B[is.na(Paswan_B$complexion1)])+nrow(subset(Paswan_B, complexion1=="Doesn't Matter")))/nrow(Paswan_B)
Srivastava_UP_B_comp1_missingness_prop_new=(nrow(Srivastava_UP_B[is.na(Srivastava_UP_B$complexion1)])+nrow(subset(Srivastava_UP_B, complexion1=="Doesn't Matter")))/nrow(Srivastava_UP_B)
Thakur_B_comp1_missingness_prop_new=(nrow(Thakur_B[is.na(Thakur_B$complexion1)])+nrow(subset(Thakur_B, complexion1=="Doesn't Matter")))/nrow(Thakur_B)
Yadav_B_comp1_missingness_prop_new=(nrow(Yadav_B[is.na(Yadav_B$complexion1)])+nrow(subset(Yadav_B, complexion1=="Doesn't Matter")))/nrow(Yadav_B)


#add comp1 and comp2 missingness_new to MI_data_merge data 
MI_data_merge$comp1_missingness_prop_new=NA
MI_data_merge$comp2_missingness_prop_new=NA
#add proportions to table 
MI_data_merge$comp1_missingness_prop_new[MI_data_merge$caste1=="Badaga"]=Badaga_comp1_missingness_prop_new
MI_data_merge$comp2_missingness_prop_new[MI_data_merge$caste1=="Badaga"]=Badaga_comp2_missingness_prop_new
MI_data_merge$comp1_missingness_prop_new[MI_data_merge$caste1=="Brahmin"]=Brahmin_TN_comp1_missingness_prop_new
MI_data_merge$comp2_missingness_prop_new[MI_data_merge$caste1=="Brahmin"]=Brahmin_TN_comp2_missingness_prop_new
MI_data_merge$comp1_missingness_prop_new[MI_data_merge$caste1=="BrahminUP"]=Brahmin_UP_comp1_missingness_prop_new
MI_data_merge$comp2_missingness_prop_new[MI_data_merge$caste1=="BrahminUP"]=Brahmin_UP_comp2_missingness_prop_new
MI_data_merge$comp1_missingness_prop_new[MI_data_merge$caste1=="Deshastha Brahmin"]=Deshastha_Brahmin_comp1_missingness_prop_new 
MI_data_merge$comp2_missingness_prop_new[MI_data_merge$caste1=="Deshastha Brahmin"]=Deshastha_Brahmin_comp2_missingness_prop_new 
MI_data_merge$comp1_missingness_prop_new[MI_data_merge$caste1=="Kapu"]=Kapu_AP_comp1_missingness_prop_new 
MI_data_merge$comp2_missingness_prop_new[MI_data_merge$caste1=="Kapu"]=Kapu_AP_comp2_missingness_prop_new 
MI_data_merge$comp1_missingness_prop_new[MI_data_merge$caste1=="Kunbi Maratha"]=Kunbi_Maratha_comp1_missingness_prop_new 
MI_data_merge$comp2_missingness_prop_new[MI_data_merge$caste1=="Kunbi Maratha"]=Kunbi_Maratha_comp2_missingness_prop_new 
MI_data_merge$comp1_missingness_prop_new[MI_data_merge$caste1=="Reddy"]=Reddy_AP_comp1_missingness_prop_new 
MI_data_merge$comp2_missingness_prop_new[MI_data_merge$caste1=="Reddy"]=Reddy_AP_comp2_missingness_prop_new 
MI_data_merge$comp1_missingness_prop_new[MI_data_merge$caste1=="Naidu"]=Naidu_AP_comp1_missingness_prop_new 
MI_data_merge$comp2_missingness_prop_new[MI_data_merge$caste1=="Naidu"]=Naidu_AP_comp2_missingness_prop_new 
MI_data_merge$comp1_missingness_prop_new[MI_data_merge$caste1=="Yadava"]=Yadava_TN_comp1_missingness_prop_new 
MI_data_merge$comp2_missingness_prop_new[MI_data_merge$caste1=="Yadava"]=Yadava_TN_comp2_missingness_prop_new 
MI_data_merge$comp1_missingness_prop_new[MI_data_merge$caste1=="Bhumihar"]=Bhumihar_UP_B_comp1_missingness_prop_new 
MI_data_merge$comp2_missingness_prop_new[MI_data_merge$caste1=="Bhumihar"]=Bhumihar_UP_B_comp2_missingness_prop_new
MI_data_merge$comp1_missingness_prop_new[MI_data_merge$caste1=="Kshatriya"]=Kshatriya_UP_B_comp1_missingness_prop_new
MI_data_merge$comp2_missingness_prop_new[MI_data_merge$caste1=="Kshatriya"]=Kshatriya_UP_B_comp2_missingness_prop_new
MI_data_merge$comp1_missingness_prop_new[MI_data_merge$caste1=="Kurmis"]=Kurmi_UP_comp1_missingness_prop_new
MI_data_merge$comp2_missingness_prop_new[MI_data_merge$caste1=="Kurmis"]=Kurmi_UP_comp2_missingness_prop_new
MI_data_merge$comp1_missingness_prop_new[MI_data_merge$caste1=="Muslim"]=Muslim_B_comp1_missingness_prop_new
MI_data_merge$comp2_missingness_prop_new[MI_data_merge$caste1=="Muslim "]=Muslim_B_comp2_missingness_prop_new
MI_data_merge$comp1_missingness_prop_new[MI_data_merge$caste1=="Paswan"]=Paswan_B_comp1_missingness_prop_new
MI_data_merge$comp2_missingness_prop_new[MI_data_merge$caste1=="Paswan"]=Paswan_B_comp2_missingness_prop_new
MI_data_merge$comp1_missingness_prop_new[MI_data_merge$caste1=="Yadav"]=Yadav_B_comp1_missingness_prop_new
MI_data_merge$comp2_missingness_prop_new[MI_data_merge$caste1=="Yadav"]=Yadav_B_comp2_missingness_prop_new
MI_data_merge$comp1_missingness_prop_new[MI_data_merge$caste1=="Srivastava"]=Srivastava_UP_B_comp1_missingness_prop_new
MI_data_merge$comp2_missingness_prop_new[MI_data_merge$caste1=="Srivastava"]=Srivastava_UP_B_comp2_missingness_prop_new
MI_data_merge$comp1_missingness_prop_new[MI_data_merge$caste1=="Thakur"]=Thakur_B_comp1_missingness_prop_new
MI_data_merge$comp2_missingness_prop_new[MI_data_merge$caste1=="Thakur"]=Thakur_B_comp2_missingness_prop_new


#see if self reporting skin colour in brides and grooms is accurate to test hypothesis 1
#first add skin complexion index to castes30plus_only_new_andstates4, then rerun subsets so each population includes index
castes30plus_only_new_andstates4$complexion1code=NA
castes30plus_only_new_andstates4$complexion2code=NA
castes30plus_only_new_andstates4$complexion1code[which(castes30plus_only_new_andstates4$complexion1=="Very Fair")]=1
castes30plus_only_new_andstates4$complexion2code[which(castes30plus_only_new_andstates4$complexion2=="Very Fair")]=1
castes30plus_only_new_andstates4$complexion1code[which(castes30plus_only_new_andstates4$complexion1=="Fair")]=2
castes30plus_only_new_andstates4$complexion2code[which(castes30plus_only_new_andstates4$complexion2=="Fair")]=2
castes30plus_only_new_andstates4$complexion1code[which(castes30plus_only_new_andstates4$complexion1=="Wheatish")]=3
castes30plus_only_new_andstates4$complexion2code[which(castes30plus_only_new_andstates4$complexion2=="Wheatish")]=3
castes30plus_only_new_andstates4$complexion1code[which(castes30plus_only_new_andstates4$complexion1=="Whetish Medium")]=4
castes30plus_only_new_andstates4$complexion2code[which(castes30plus_only_new_andstates4$complexion2=="Whetish Medium")]=4
castes30plus_only_new_andstates4$complexion1code[which(castes30plus_only_new_andstates4$complexion1=="Dark")]=5
castes30plus_only_new_andstates4$complexion2code[which(castes30plus_only_new_andstates4$complexion2=="Dark")]=5
castes30plus_only_new_andstates4$complexion1code[which(castes30plus_only_new_andstates4$complexion1=="Doesn't Matter")]=NA
castes30plus_only_new_andstates4$complexion2code[which(castes30plus_only_new_andstates4$complexion2=="Doesn't Matter")]=NA
#run subsets again
Badaga_TN=subset(castes30plus_only_new_andstates4, caste1_1=="Badaga" & State=="Tamil Nadu")
Brahmin_TN=subset(castes30plus_only_new_andstates4, caste1_1=="Brahmin" & State=="Tamil Nadu")
Brahmin_UP=subset(castes30plus_only_new_andstates4, caste1_1=="Brahmin" & State=="Uttar Pradesh")
Deshastha_Brahmin=subset(castes30plus_only_new_andstates4, caste1=="Brahmin-Deshastha"|caste1_1=="Brahmin" & subcaste=="Deshastha")
Kapu_AP=subset(castes30plus_only_new_andstates4, caste1_1=="Kapu" & State=="Andhra Pradesh"|caste1_1=="Kapu Naidu" & State=="Andhra Pradesh"|caste1_1=="Munnuru Kapu" & State=="Andhra Pradesh")
Kunbi_Maratha=subset(castes30plus_only_new_andstates4, caste1=="Maratha" & subcaste=="Kunbi Maratha")
Naidu_AP=subset(castes30plus_only_new_andstates4, caste1=="Naidu" & State=="Andhra Pradesh")
Reddy_AP=subset(castes30plus_only_new_andstates4, caste1=="Reddy" & State=="Andhra Pradesh")
Yadava_TN=subset(castes30plus_only_new_andstates4, caste1=="Yadav" & State=="Tamil Nadu")
Bhumihar_grep=castes30plus_only_new_andstates4[grep("Bhumihar", castes30plus_only_new_andstates4$caste1), ]
Bhumihar_UP_B=subset(Bhumihar_grep, State=="Uttar Pradesh"|State=="Bihar")
Kshatriya_grep=castes30plus_only_new_andstates4[grep("Kshatriya", castes30plus_only_new_andstates4$caste1), ]
Kshatriya_UP_B=subset(Kshatriya_grep, State=="Bihar"|State=="Uttar Pradesh")
Kurmi_grep=castes30plus_only_new_andstates4[grep("Kurmi", castes30plus_only_new_andstates4$caste1), ]
Kurmi_UP=subset(Kurmi_grep, State=="Uttar Pradesh")
Muslim_B=subset(castes30plus_only_new_andstates4, religion1=="Muslim"&State=="Bihar")
Paswan_grep=castes30plus_only_new_andstates4[grep("Paswan", castes30plus_only_new_andstates4$caste1), ]
Paswan_B=subset(Paswan_grep, State=="Bihar")
Yadav_B=subset(castes30plus_only_new_andstates4, caste1=="Yadav" & State=="Bihar")
Srivastava_UP_B=subset(castes30plus_only_new_andstates4, (subcaste=="Srivastava" & State=="Bihar")|(subcaste=="Srivastava" & State=="Uttar Pradesh"))
Thakur_B=subset(castes30plus_only_new_andstates4, (caste1=="Thakur" & State=="Bihar")|(subcaste=="Thakur"&State=="Bihar"))

#now find average self-reported skin colour: sum all those with answers for comp1 and divide
Badaga_comp1_average=sum(Badaga_TN$complexion1code, na.rm=TRUE)/nrow(Badaga_TN[!is.na(Badaga_TN$complexion1code)])
Brahmin_TN_comp1_average=sum(Brahmin_TN$complexion1code, na.rm=TRUE)/nrow(Brahmin_TN[!is.na(Brahmin_TN$complexion1code)])
Brahmin_UP_comp1_average=sum(Brahmin_UP$complexion1code, na.rm=TRUE)/nrow(Brahmin_UP[!is.na(Brahmin_UP$complexion1code)])
Deshastha_Brahmin_comp1_average=sum(Deshastha_Brahmin$complexion1code, na.rm=TRUE)/nrow(Deshastha_Brahmin[!is.na(Deshastha_Brahmin$complexion1code)])
Kapu_AP_comp1_average=sum(Kapu_AP$complexion1code, na.rm=TRUE)/nrow(Kapu_AP[!is.na(Kapu_AP$complexion1code)])
Kunbi_Maratha_comp1_average=sum(Kunbi_Maratha$complexion1code, na.rm=TRUE)/nrow(Kunbi_Maratha[!is.na(Kunbi_Maratha$complexion1code)])
Naidu_AP_comp1_average=sum(Naidu_AP$complexion1code, na.rm=TRUE)/nrow(Naidu_AP[!is.na(Naidu_AP$complexion1code)])
Reddy_AP_comp1_average=sum(Reddy_AP$complexion1code, na.rm=TRUE)/nrow(Reddy_AP[!is.na(Reddy_AP$complexion1code)])
Yadava_TN_comp1_average=sum(Yadava_TN$complexion1code, na.rm=TRUE)/nrow(Yadava_TN[!is.na(Yadava_TN$complexion1code)])
Bhumihar_UP_B_comp1_average=sum(Bhumihar_UP_B$complexion1code, na.rm=TRUE)/nrow(Bhumihar_UP_B[!is.na(Bhumihar_UP_B$complexion1code)])
Kshatriya_UP_B_comp1_average=sum(Kshatriya_UP_B$complexion1code, na.rm=TRUE)/nrow(Kshatriya_UP_B[!is.na(Kshatriya_UP_B$complexion1code)])
Kurmi_UP_comp1_average=sum(Kurmi_UP$complexion1code, na.rm=TRUE)/nrow(Kurmi_UP[!is.na(Kurmi_UP$complexion1code)])
Muslim_B_comp1_average=sum(Muslim_B$complexion1code, na.rm=TRUE)/nrow(Muslim_B[!is.na(Muslim_B$complexion1code)])
Paswan_B_comp1_average=sum(Paswan_B$complexion1code, na.rm=TRUE)/nrow(Paswan_B[!is.na(Paswan_B$complexion1code)])
Srivastava_UP_B_comp1_average=sum(Srivastava_UP_B$complexion1code, na.rm=TRUE)/nrow(Srivastava_UP_B[!is.na(Srivastava_UP_B$complexion1code)])
Thakur_B_comp1_average=sum(Thakur_B$complexion1code, na.rm=TRUE)/nrow(Thakur_B[!is.na(Thakur_B$complexion1code)])
Yadav_B_comp1_average=sum(Yadav_B$complexion1code, na.rm=TRUE)/nrow(Yadav_B[!is.na(Yadav_B$complexion1code)])

#now add average self-reported skin colour to MI_data_merge table
MI_data_merge$comp1_average[MI_data_merge$caste1=="Badaga"]=Badaga_comp1_average
MI_data_merge$comp1_average[MI_data_merge$caste1=="Brahmin"]=Brahmin_TN_comp1_average
MI_data_merge$comp1_average[MI_data_merge$caste1=="BrahminUP"]=Brahmin_UP_comp1_average
MI_data_merge$comp1_average[MI_data_merge$caste1=="Deshastha Brahmin"]=Deshastha_Brahmin_comp1_average 
MI_data_merge$comp1_average[MI_data_merge$caste1=="Kapu"]=Kapu_AP_comp1_average 
MI_data_merge$comp1_average[MI_data_merge$caste1=="Kunbi Maratha"]=Kunbi_Maratha_comp1_average 
MI_data_merge$comp1_average[MI_data_merge$caste1=="Reddy"]=Reddy_AP_comp1_average 
MI_data_merge$comp1_average[MI_data_merge$caste1=="Naidu"]=Naidu_AP_comp1_average 
MI_data_merge$comp1_average[MI_data_merge$caste1=="Yadava"]=Yadava_TN_comp1_average 
MI_data_merge$comp1_average[MI_data_merge$caste1=="Bhumihar"]=Bhumihar_UP_B_comp1_average
MI_data_merge$comp1_average[MI_data_merge$caste1=="Kshatriya"]=Kshatriya_UP_B_comp1_average
MI_data_merge$comp1_average[MI_data_merge$caste1=="Kurmis"]=Kurmi_UP_comp1_average 
MI_data_merge$comp1_average[MI_data_merge$caste1=="Muslim"]=Muslim_B_comp1_average 
MI_data_merge$comp1_average[MI_data_merge$caste1=="Paswan"]=Paswan_B_comp1_average 
MI_data_merge$comp1_average[MI_data_merge$caste1=="Srivastava"]=Srivastava_UP_B_comp1_average
MI_data_merge$comp1_average[MI_data_merge$caste1=="Thakur"]=Thakur_B_comp1_average 
MI_data_merge$comp1_average[MI_data_merge$caste1=="Yadav"]=Yadav_B_comp1_average 

#plot MI against self-reported 
plot(MI_data_merge$MI, MI_data_merge$comp1_average, xlab = "Actual Melanin Index", ylab="Average self-reported skin pigmentation", main="Melanin Index against self-reported skin pigmentation", pch=20)
#check distribution not normal before proceeding with correlation 
hist(MI_data_merge$MI)
hist(MI_data_merge$comp1_average)
#now run kendall correlation test
cor.test(MI_data_merge$MI, MI_data_merge$comp1_average, use="complete.obs", method="kendall")
#run linear modelling to see whether self-stated skin colour can predict actual MI
as.factor(MI_data_merge$comp1_average)
mod=lm(comp1_average~MI, data=MI_data_merge)
summary(mod)


#see if people self report lighter or darker than reality to test hypothesis 1- by making distribution with 20 percentile cut-offs if assuming each colour category is one step
h=hist(MI_data_merge$MI, breaks=seq(from=40, to=75, by=2))
h$density = h$counts/sum(h$counts)*100
plot(h,freq=FALSE, xlab="Melanin Index", main="Distribution of average recorded Melanin Index across 36 Indian populations", col="orange", cex.main=0.97, ylim=c(0,20))
quantile(MI_data_merge$MI, c(.20, .40, .60, .80), na.rm=TRUE)
twentyquantile=43.57000
fourtyquantile=49.81000
#--> very fair<43.8, 50.4>fair>=43.8, 57.5>wheatish>=50.4, 61.2>whetish medium>=57.5, dark>61.2
#all castes say they are between 2 and 3 (fair to wheatish), let's make object for transformed MI
Badaga_comp1_average_MI=twentyquantile+((Badaga_comp1_average-2)*(fourtyquantile-twentyquantile))
Bhumihar_UP_B_comp1_average_MI=twentyquantile+((Bhumihar_UP_B_comp1_average-2)*(fourtyquantile-twentyquantile))
Brahmin_TN_comp1_average_MI=twentyquantile+((Brahmin_TN_comp1_average-2)*(fourtyquantile-twentyquantile))
Brahmin_UP_comp1_average_MI=twentyquantile+((Brahmin_UP_comp1_average-2)*(fourtyquantile-twentyquantile))
Deshastha_Brahmin_comp1_average_MI=twentyquantile+((Deshastha_Brahmin_comp1_average-2)*(fourtyquantile-twentyquantile))
Kapu_AP_comp1_average_MI=twentyquantile+((Kapu_AP_comp1_average-2)*(fourtyquantile-twentyquantile))
Kshatriya_UP_B_comp1_average_MI=twentyquantile+((Kshatriya_UP_B_comp1_average-2)*(fourtyquantile-twentyquantile))
Kunbi_Maratha_comp1_average_MI=twentyquantile+((Kunbi_Maratha_comp1_average-2)*(fourtyquantile-twentyquantile))
Kurmi_UP_comp1_average_MI=twentyquantile+((Kurmi_UP_comp1_average-2)*(fourtyquantile-twentyquantile))
Muslim_B_comp1_average_MI=twentyquantile+((Muslim_B_comp1_average-2)*(fourtyquantile-twentyquantile))
Naidu_AP_comp1_average_MI=twentyquantile+((Naidu_AP_comp1_average-2)*(fourtyquantile-twentyquantile))
Paswan_B_comp1_average_MI=twentyquantile+((Paswan_B_comp1_average-2)*(fourtyquantile-twentyquantile))
Reddy_AP_comp1_average_MI=twentyquantile+((Reddy_AP_comp1_average-2)*(fourtyquantile-twentyquantile))
Srivastava_UP_B_comp1_average_MI=twentyquantile+((Srivastava_UP_B_comp1_average-2)*(fourtyquantile-twentyquantile))
Thakur_B_comp1_average_MI=twentyquantile+((Thakur_B_comp1_average-2)*(fourtyquantile-twentyquantile))
Yadav_B_comp1_average_MI=twentyquantile+((Yadav_B_comp1_average-2)*(fourtyquantile-twentyquantile))
Yadava_TN_comp1_average_MI=twentyquantile+((Yadava_TN_comp1_average-2)*(fourtyquantile-twentyquantile))
#now add values to MI table
MI_data_merge$comp1_average_MI=NA
MI_data_merge$comp1_average_MI[MI_data_merge$caste1=="Badaga"]=Badaga_comp1_average_MI
MI_data_merge$comp1_average_MI[MI_data_merge$caste1=="Brahmin"]=Brahmin_TN_comp1_average_MI
MI_data_merge$comp1_average_MI[MI_data_merge$caste1=="BrahminUP"]=Brahmin_UP_comp1_average_MI
MI_data_merge$comp1_average_MI[MI_data_merge$caste1=="Deshastha Brahmin"]=Deshastha_Brahmin_comp1_average_MI 
MI_data_merge$comp1_average_MI[MI_data_merge$caste1=="Kapu"]=Kapu_AP_comp1_average_MI 
MI_data_merge$comp1_average_MI[MI_data_merge$caste1=="Kunbi Maratha"]=Kunbi_Maratha_comp1_average_MI 
MI_data_merge$comp1_average_MI[MI_data_merge$caste1=="Reddy"]=Reddy_AP_comp1_average_MI 
MI_data_merge$comp1_average_MI[MI_data_merge$caste1=="Naidu"]=Naidu_AP_comp1_average_MI 
MI_data_merge$comp1_average_MI[MI_data_merge$caste1=="Bhumihar"]=Bhumihar_UP_B_comp1_average_MI
MI_data_merge$comp1_average_MI[MI_data_merge$caste1=="Kshatriya"]=Kshatriya_UP_B_comp1_average_MI
MI_data_merge$comp1_average_MI[MI_data_merge$caste1=="Kurmis"]=Kurmi_UP_comp1_average_MI 
MI_data_merge$comp1_average_MI[MI_data_merge$caste1=="Muslim"]=Muslim_B_comp1_average_MI 
MI_data_merge$comp1_average_MI[MI_data_merge$caste1=="Paswan"]=Paswan_B_comp1_average_MI 
MI_data_merge$comp1_average_MI[MI_data_merge$caste1=="Srivastava"]=Srivastava_UP_B_comp1_average_MI
MI_data_merge$comp1_average_MI[MI_data_merge$caste1=="Thakur"]=Thakur_B_comp1_average_MI 
MI_data_merge$comp1_average_MI[MI_data_merge$caste1=="Yadav"]=Yadav_B_comp1_average_MI 
MI_data_merge$comp1_average_MI[MI_data_merge$caste1=="Yadava"]=Yadava_TN_comp1_average_MI 
#run correlation
cor.test(MI_data_merge$MI, MI_data_merge$comp1_average_MI, method="kendall")
#--> not significant: 0.25 correlation, p value 0.1767
#do linear modeling 
MI_against_comp1MI=lm(comp1_average_MI~MI, data=MI_data_merge)
summary(MI_against_comp1MI)
plot(MI_data_merge$MI, MI_data_merge$comp1_average_MI, xlab="Measured average MI", ylab="Self-reported skin pigmentation transformed into average MI", pch=20)
abline(MI_against_comp1MI, col="red")
#--> significant at 0.1 level, gradient is 0.051, indicating as measured MI increases by 1, self-stated pigmentation increases by 0.051  
#do two sample t-test to compare distributions, check for normality first: 
shapiro.test(MI_data_merge$MI)
shapiro.test(MI_data_merge$comp1_average_MI)
var.test(MI_data_merge$MI, MI_data_merge$comp1_average_MI)
#do welch test instead because not equal variances
t.test(MI_data_merge$MI, MI_data_merge$comp1_average_MI, var.equal = FALSE)



#See if MI correlates with fairORvfair_prop (with "doesn't matter" set to NA) to test hypothesis 4
#make objects for each 
Badaga_comp2_fairORvfair_prop=nrow(subset(Badaga_TN, complexion2code==1|complexion2code==2))/nrow(Badaga_TN[!is.na(Badaga_TN$complexion2code)])
Bhumihar_UP_B_comp2_fairORvfair_prop=nrow(subset(Bhumihar_UP_B, complexion2code==1|complexion2code==2))/nrow(Bhumihar_UP_B[!is.na(Bhumihar_UP_B$complexion2code)])
Brahmin_TN_comp2_fairORvfair_prop=nrow(subset(Brahmin_TN, complexion2code==1|complexion2code==2))/nrow(Brahmin_TN[!is.na(Brahmin_TN$complexion2code)])
Brahmin_UP_comp2_fairORvfair_prop=nrow(subset(Brahmin_UP, complexion2code==1|complexion2code==2))/nrow(Brahmin_UP[!is.na(Brahmin_UP$complexion2code)])
Deshastha_Brahmin_comp2_fairORvfair_prop=nrow(subset(Deshastha_Brahmin, complexion2code==1|complexion2code==2))/nrow(Deshastha_Brahmin[!is.na(Deshastha_Brahmin$complexion2code)])
Kapu_AP_comp2_fairORvfair_prop=nrow(subset(Kapu_AP, complexion2code==1|complexion2code==2))/nrow(Kapu_AP[!is.na(Kapu_AP$complexion2code)])
Kshatriya_UP_B_comp2_fairORvfair_prop=nrow(subset(Kshatriya_UP_B, complexion2code==1|complexion2code==2))/nrow(Kshatriya_UP_B[!is.na(Kshatriya_UP_B$complexion2code)])
Kunbi_Maratha_comp2_fairORvfair_prop=nrow(subset(Kunbi_Maratha, complexion2code==1|complexion2code==2))/nrow(Kunbi_Maratha[!is.na(Kunbi_Maratha$complexion2code)])
Kurmi_UP_comp2_fairORvfair_prop=nrow(subset(Kurmi_UP, complexion2code==1|complexion2code==2))/nrow(Kurmi_UP[!is.na(Kurmi_UP$complexion2code)])
Muslim_B_comp2_fairORvfair_prop=nrow(subset(Muslim_B, complexion2code==1|complexion2code==2))/nrow(Muslim_B[!is.na(Muslim_B$complexion2code)])
Naidu_AP_comp2_fairORvfair_prop=nrow(subset(Naidu_AP, complexion2code==1|complexion2code==2))/nrow(Naidu_AP[!is.na(Naidu_AP$complexion2code)])
Paswan_B_comp2_fairORvfair_prop=nrow(subset(Paswan_B, complexion2code==1|complexion2code==2))/nrow(Paswan_B[!is.na(Paswan_B$complexion2code)])
Reddy_AP_comp2_fairORvfair_prop=nrow(subset(Reddy_AP, complexion2code==1|complexion2code==2))/nrow(Reddy_AP[!is.na(Reddy_AP$complexion2code)])
Srivastava_UP_B_comp2_fairORvfair_prop=nrow(subset(Srivastava_UP_B, complexion2code==1|complexion2code==2))/nrow(Srivastava_UP_B[!is.na(Srivastava_UP_B$complexion2code)])
Thakur_B_comp2_fairORvfair_prop=nrow(subset(Thakur_B, complexion2code==1|complexion2code==2))/nrow(Thakur_B[!is.na(Thakur_B$complexion2code)])
Yadav_B_comp2_fairORvfair_prop=nrow(subset(Yadav_B, complexion2code==1|complexion2code==2))/nrow(Yadav_B[!is.na(Yadav_B$complexion2code)])
Yadava_TN_comp2_fairORvfair_prop=nrow(subset(Yadava_TN, complexion2code==1|complexion2code==2))/nrow(Yadava_TN[!is.na(Yadava_TN$complexion2code)])
#now add objects to table 
MI_data_merge$comp2_fairORvfair_prop=NA
MI_data_merge$comp2_fairORvfair_prop[MI_data_merge$caste1=="Badaga"]=Badaga_comp2_fairORvfair_prop
MI_data_merge$comp2_fairORvfair_prop[MI_data_merge$caste1=="Brahmin"]=Brahmin_TN_comp2_fairORvfair_prop
MI_data_merge$comp2_fairORvfair_prop[MI_data_merge$caste1=="BrahminUP"]=Brahmin_UP_comp2_fairORvfair_prop
MI_data_merge$comp2_fairORvfair_prop[MI_data_merge$caste1=="Deshastha Brahmin"]=Deshastha_Brahmin_comp2_fairORvfair_prop 
MI_data_merge$comp2_fairORvfair_prop[MI_data_merge$caste1=="Kapu"]=Kapu_AP_comp2_fairORvfair_prop 
MI_data_merge$comp2_fairORvfair_prop[MI_data_merge$caste1=="Kunbi Maratha"]=Kunbi_Maratha_comp2_fairORvfair_prop 
MI_data_merge$comp2_fairORvfair_prop[MI_data_merge$caste1=="Reddy"]=Reddy_AP_comp2_fairORvfair_prop 
MI_data_merge$comp2_fairORvfair_prop[MI_data_merge$caste1=="Naidu"]=Naidu_AP_comp2_fairORvfair_prop 
MI_data_merge$comp2_fairORvfair_prop[MI_data_merge$caste1=="Yadava"]=Yadava_TN_comp2_fairORvfair_prop 
MI_data_merge$comp2_fairORvfair_prop[MI_data_merge$caste1=="Bhumihar"]=Bhumihar_UP_B_comp2_fairORvfair_prop
MI_data_merge$comp2_fairORvfair_prop[MI_data_merge$caste1=="Kshatriya"]=Kshatriya_UP_B_comp2_fairORvfair_prop
MI_data_merge$comp2_fairORvfair_prop[MI_data_merge$caste1=="Kurmis"]=Kurmi_UP_comp2_fairORvfair_prop 
MI_data_merge$comp2_fairORvfair_prop[MI_data_merge$caste1=="Muslim"]=Muslim_B_comp2_fairORvfair_prop 
MI_data_merge$comp2_fairORvfair_prop[MI_data_merge$caste1=="Paswan"]=Paswan_B_comp2_fairORvfair_prop 
MI_data_merge$comp2_fairORvfair_prop[MI_data_merge$caste1=="Srivastava"]=Srivastava_UP_B_comp2_fairORvfair_prop
MI_data_merge$comp2_fairORvfair_prop[MI_data_merge$caste1=="Thakur"]=Thakur_B_comp2_fairORvfair_prop 
MI_data_merge$comp2_fairORvfair_prop[MI_data_merge$caste1=="Yadav"]=Yadav_B_comp2_fairORvfair_prop 
#run correlation
cor.test(MI_data_merge$MI, MI_data_merge$comp2_fairORvfair_prop, method="kendall")


#check if MI correlates with comp2_dark_prop to test whether selection is against dark
#make objects for each 
Badaga_comp2_dark_prop=nrow(subset(Badaga_TN, complexion2code==5))/nrow(Badaga_TN[!is.na(Badaga_TN$complexion2code)])
Bhumihar_UP_B_comp2_dark_prop=nrow(subset(Bhumihar_UP_B, complexion2code==5))/nrow(Bhumihar_UP_B[!is.na(Bhumihar_UP_B$complexion2code)])
Brahmin_TN_comp2_dark_prop=nrow(subset(Brahmin_TN, complexion2code==5))/nrow(Brahmin_TN[!is.na(Brahmin_TN$complexion2code)])
Brahmin_UP_comp2_dark_prop=nrow(subset(Brahmin_UP, complexion2code==5))/nrow(Brahmin_UP[!is.na(Brahmin_UP$complexion2code)])
Deshastha_Brahmin_comp2_dark_prop=nrow(subset(Deshastha_Brahmin, complexion2code==5))/nrow(Deshastha_Brahmin[!is.na(Deshastha_Brahmin$complexion2code)])
Kapu_AP_comp2_dark_prop=nrow(subset(Kapu_AP, complexion2code==5))/nrow(Kapu_AP[!is.na(Kapu_AP$complexion2code)])
Kshatriya_UP_B_comp2_dark_prop=nrow(subset(Kshatriya_UP_B, complexion2code==5))/nrow(Kshatriya_UP_B[!is.na(Kshatriya_UP_B$complexion2code)])
Kunbi_Maratha_comp2_dark_prop=nrow(subset(Kunbi_Maratha, complexion2code==5))/nrow(Kunbi_Maratha[!is.na(Kunbi_Maratha$complexion2code)])
Kurmi_UP_comp2_dark_prop=nrow(subset(Kurmi_UP, complexion2code==5))/nrow(Kurmi_UP[!is.na(Kurmi_UP$complexion2code)])
Muslim_B_comp2_dark_prop=nrow(subset(Muslim_B, complexion2code==5))/nrow(Muslim_B[!is.na(Muslim_B$complexion2code)])
Naidu_AP_comp2_dark_prop=nrow(subset(Naidu_AP, complexion2code==5))/nrow(Naidu_AP[!is.na(Naidu_AP$complexion2code)])
Paswan_B_comp2_dark_prop=nrow(subset(Paswan_B, complexion2code==5))/nrow(Paswan_B[!is.na(Paswan_B$complexion2code)])
Reddy_AP_comp2_dark_prop=nrow(subset(Reddy_AP, complexion2code==5))/nrow(Reddy_AP[!is.na(Reddy_AP$complexion2code)])
Srivastava_UP_B_comp2_dark_prop=nrow(subset(Srivastava_UP_B, complexion2code==5))/nrow(Srivastava_UP_B[!is.na(Srivastava_UP_B$complexion2code)])
Thakur_B_comp2_dark_prop=nrow(subset(Thakur_B, complexion2code==5))/nrow(Thakur_B[!is.na(Thakur_B$complexion2code)])
Yadav_B_comp2_dark_prop=nrow(subset(Yadav_B, complexion2code==5))/nrow(Yadav_B[!is.na(Yadav_B$complexion2code)])
Yadava_TN_comp2_dark_prop=nrow(subset(Yadava_TN, complexion2code==5))/nrow(Yadava_TN[!is.na(Yadava_TN$complexion2code)])
#now add objects to table 
MI_data_merge$comp2_dark_prop=NA
MI_data_merge$comp2_dark_prop[MI_data_merge$caste1=="Badaga"]=Badaga_comp2_dark_prop
MI_data_merge$comp2_dark_prop[MI_data_merge$caste1=="Brahmin"]=Brahmin_TN_comp2_dark_prop
MI_data_merge$comp2_dark_prop[MI_data_merge$caste1=="BrahminUP"]=Brahmin_UP_comp2_dark_prop
MI_data_merge$comp2_dark_prop[MI_data_merge$caste1=="Deshastha Brahmin"]=Deshastha_Brahmin_comp2_dark_prop 
MI_data_merge$comp2_dark_prop[MI_data_merge$caste1=="Kapu"]=Kapu_AP_comp2_dark_prop 
MI_data_merge$comp2_dark_prop[MI_data_merge$caste1=="Kunbi Maratha"]=Kunbi_Maratha_comp2_dark_prop 
MI_data_merge$comp2_dark_prop[MI_data_merge$caste1=="Reddy"]=Reddy_AP_comp2_dark_prop 
MI_data_merge$comp2_dark_prop[MI_data_merge$caste1=="Naidu"]=Naidu_AP_comp2_dark_prop 
MI_data_merge$comp2_dark_prop[MI_data_merge$caste1=="Yadava"]=Yadava_TN_comp2_dark_prop 
MI_data_merge$comp2_dark_prop[MI_data_merge$caste1=="Bhumihar"]=Bhumihar_UP_B_comp2_dark_prop
MI_data_merge$comp2_dark_prop[MI_data_merge$caste1=="Kshatriya"]=Kshatriya_UP_B_comp2_dark_prop
MI_data_merge$comp2_dark_prop[MI_data_merge$caste1=="Kurmis"]=Kurmi_UP_comp2_dark_prop 
MI_data_merge$comp2_dark_prop[MI_data_merge$caste1=="Muslim"]=Muslim_B_comp2_dark_prop 
MI_data_merge$comp2_dark_prop[MI_data_merge$caste1=="Paswan"]=Paswan_B_comp2_dark_prop 
MI_data_merge$comp2_dark_prop[MI_data_merge$caste1=="Srivastava"]=Srivastava_UP_B_comp2_dark_prop
MI_data_merge$comp2_dark_prop[MI_data_merge$caste1=="Thakur"]=Thakur_B_comp2_dark_prop 
MI_data_merge$comp2_dark_prop[MI_data_merge$caste1=="Yadav"]=Yadav_B_comp2_dark_prop 
#run correlation
cor.test(MI_data_merge$MI, MI_data_merge$comp2_dark_prop, method="kendall")
#--> not significant: correlation -0.2236, p value is 0.2826 


#check all other univariate correlations: make matrix of correlations for MI_data_merge table
MI_data_merge_forcor=MI_data_merge[, c(3,4,7,8,9,10,11,12,13,14)]
colnames(MI_data_merge_forcor)=c("Average Recorded MI", "rs1426654-A allele frequency", "Proportion of caste with missing self-reported skin complexion information", "Proportion of caste with missing desired complexion in future spouse information", "Proportion of caste with missing self-reported skin complexion information, excluding 'Doesnt Matter'", "Proportion of caste with missing desired complexion in future spouse information, excluding 'Doesn't Matter'", "Average self-stated skin complexion Index", "Proportion of caste wanting future spouse to have fair or very fair skin complexion", "Proportion of caste wanting future spouse to have dark complexion", "Average self-stated skin complexion transformed into MI value")
library(rstatix)
corMatrix_MI <- MI_data_merge_forcor %>% cor_mat("Average Recorded MI":"Average self-stated skin complexion transformed into MI value", method = "kendall")
#for correlations' p values
corMatrix_MI_p <- corMatrix_MI %>% cor_get_pval()


#####do preferences have impact on recorded MI
###first add comp2_average to MI table 
Badaga_comp2_average=sum(Badaga_TN$complexion2code, na.rm=TRUE)/nrow(Badaga_TN[!is.na(Badaga_TN$complexion2code)])
Brahmin_TN_comp2_average=sum(Brahmin_TN$complexion2code, na.rm=TRUE)/nrow(Brahmin_TN[!is.na(Brahmin_TN$complexion2code)])
Brahmin_UP_comp2_average=sum(Brahmin_UP$complexion2code, na.rm=TRUE)/nrow(Brahmin_UP[!is.na(Brahmin_UP$complexion2code)])
Deshastha_Brahmin_comp2_average=sum(Deshastha_Brahmin$complexion2code, na.rm=TRUE)/nrow(Deshastha_Brahmin[!is.na(Deshastha_Brahmin$complexion2code)])
Kapu_AP_comp2_average=sum(Kapu_AP$complexion2code, na.rm=TRUE)/nrow(Kapu_AP[!is.na(Kapu_AP$complexion2code)])
Kunbi_Maratha_comp2_average=sum(Kunbi_Maratha$complexion2code, na.rm=TRUE)/nrow(Kunbi_Maratha[!is.na(Kunbi_Maratha$complexion2code)])
Naidu_AP_comp2_average=sum(Naidu_AP$complexion2code, na.rm=TRUE)/nrow(Naidu_AP[!is.na(Naidu_AP$complexion2code)])
Reddy_AP_comp2_average=sum(Reddy_AP$complexion2code, na.rm=TRUE)/nrow(Reddy_AP[!is.na(Reddy_AP$complexion2code)])
Yadava_TN_comp2_average=sum(Yadava_TN$complexion2code, na.rm=TRUE)/nrow(Yadava_TN[!is.na(Yadava_TN$complexion2code)])
Bhumihar_UP_B_comp2_average=sum(Bhumihar_UP_B$complexion2code, na.rm=TRUE)/nrow(Bhumihar_UP_B[!is.na(Bhumihar_UP_B$complexion2code)])
Kshatriya_UP_B_comp2_average=sum(Kshatriya_UP_B$complexion2code, na.rm=TRUE)/nrow(Kshatriya_UP_B[!is.na(Kshatriya_UP_B$complexion2code)])
Kurmi_UP_comp2_average=sum(Kurmi_UP$complexion2code, na.rm=TRUE)/nrow(Kurmi_UP[!is.na(Kurmi_UP$complexion2code)])
Muslim_B_comp2_average=sum(Muslim_B$complexion2code, na.rm=TRUE)/nrow(Muslim_B[!is.na(Muslim_B$complexion2code)])
Paswan_B_comp2_average=sum(Paswan_B$complexion2code, na.rm=TRUE)/nrow(Paswan_B[!is.na(Paswan_B$complexion2code)])
Srivastava_UP_B_comp2_average=sum(Srivastava_UP_B$complexion2code, na.rm=TRUE)/nrow(Srivastava_UP_B[!is.na(Srivastava_UP_B$complexion2code)])
Thakur_B_comp2_average=sum(Thakur_B$complexion2code, na.rm=TRUE)/nrow(Thakur_B[!is.na(Thakur_B$complexion2code)])
Yadav_B_comp2_average=sum(Yadav_B$complexion2code, na.rm=TRUE)/nrow(Yadav_B[!is.na(Yadav_B$complexion2code)])
#now add average self-reported skin colour to MI_data_merge table
MI_data_merge$comp2_average[MI_data_merge$caste1=="Badaga"]=Badaga_comp2_average
MI_data_merge$comp2_average[MI_data_merge$caste1=="Brahmin"]=Brahmin_TN_comp2_average
MI_data_merge$comp2_average[MI_data_merge$caste1=="BrahminUP"]=Brahmin_UP_comp2_average
MI_data_merge$comp2_average[MI_data_merge$caste1=="Deshastha Brahmin"]=Deshastha_Brahmin_comp2_average 
MI_data_merge$comp2_average[MI_data_merge$caste1=="Kapu"]=Kapu_AP_comp2_average 
MI_data_merge$comp2_average[MI_data_merge$caste1=="Kunbi Maratha"]=Kunbi_Maratha_comp2_average 
MI_data_merge$comp2_average[MI_data_merge$caste1=="Reddy"]=Reddy_AP_comp2_average 
MI_data_merge$comp2_average[MI_data_merge$caste1=="Naidu"]=Naidu_AP_comp2_average 
MI_data_merge$comp2_average[MI_data_merge$caste1=="Yadava"]=Yadava_TN_comp2_average 
MI_data_merge$comp2_average[MI_data_merge$caste1=="Bhumihar"]=Bhumihar_UP_B_comp2_average
MI_data_merge$comp2_average[MI_data_merge$caste1=="Kshatriya"]=Kshatriya_UP_B_comp2_average
MI_data_merge$comp2_average[MI_data_merge$caste1=="Kurmis"]=Kurmi_UP_comp2_average 
MI_data_merge$comp2_average[MI_data_merge$caste1=="Muslim"]=Muslim_B_comp2_average 
MI_data_merge$comp2_average[MI_data_merge$caste1=="Paswan"]=Paswan_B_comp2_average 
MI_data_merge$comp2_average[MI_data_merge$caste1=="Srivastava"]=Srivastava_UP_B_comp2_average
MI_data_merge$comp2_average[MI_data_merge$caste1=="Thakur"]=Thakur_B_comp2_average 
MI_data_merge$comp2_average[MI_data_merge$caste1=="Yadav"]=Yadav_B_comp2_average 
#now see if comp2 average and other preferences affect MI in linear model 
complexionmodel=lm(MI~comp2_average+comp2_fairORvfair_prop+comp2_dark_prop, data=MI_data_merge)
summary(complexionmodel)
complexionbest=stepAIC(complexionmodel)
summary(complexionbest)
#--> comp2_av remains and is significant 
#visualise 
tiff("scatter_comp2av_vs_MI.tiff", res=96, height=700, width=800)
plot(MI_data_merge$comp2_average, MI_data_merge$MI, xlab="Average desired skin complexion index in caste", ylab="Average recorded MI in caste", main="Scatterplot of average desired skin complexion against average recorded Melanin Index across castes", col="red", pch=20)
abline(complexionbest)
dev.off()

#now need to see if degree of ss correlates with MI 
#recode columns in states4 dataset so Vfair=0 and dark=4 in the complexion codes
castes30plus_only_new_andstates4$complexion1code[castes30plus_only_new_andstates4$complexion1=="Very Fair"]=0
castes30plus_only_new_andstates4$complexion1code[castes30plus_only_new_andstates4$complexion1=="Fair"]=1
castes30plus_only_new_andstates4$complexion1code[castes30plus_only_new_andstates4$complexion1=="Wheatish"]=2
castes30plus_only_new_andstates4$complexion1code[castes30plus_only_new_andstates4$complexion1=="Whetish Medium"]=3
castes30plus_only_new_andstates4$complexion1code[castes30plus_only_new_andstates4$complexion1=="Dark"]=4
castes30plus_only_new_andstates4$complexion2code[castes30plus_only_new_andstates4$complexion2=="Very Fair"]=0
castes30plus_only_new_andstates4$complexion2code[castes30plus_only_new_andstates4$complexion2=="Fair"]=1
castes30plus_only_new_andstates4$complexion2code[castes30plus_only_new_andstates4$complexion2=="Wheatish"]=2
castes30plus_only_new_andstates4$complexion2code[castes30plus_only_new_andstates4$complexion2=="Whetish Medium"]=3
castes30plus_only_new_andstates4$complexion2code[castes30plus_only_new_andstates4$complexion2=="Dark"]=4
castes30plus_only_new_andstates4$complexion2code[castes30plus_only_new_andstates4$complexion2=="Doesn't Matter"]=NA
castes30plus_only_new_andstates4$complexion1code[castes30plus_only_new_andstates4$complexion1=="Doesn't Matter"]=NA
#make new column to show ss strength 
castes30plus_only_new_andstates4$sss=castes30plus_only_new_andstates4$complexion1code-castes30plus_only_new_andstates4$complexion2code
#run subsets again to contain new column
Badaga_TN=subset(castes30plus_only_new_andstates4, caste1_1=="Badaga" & State=="Tamil Nadu")
Brahmin_TN=subset(castes30plus_only_new_andstates4, caste1_1=="Brahmin" & State=="Tamil Nadu")
Brahmin_UP=subset(castes30plus_only_new_andstates4, caste1_1=="Brahmin" & State=="Uttar Pradesh")
Deshastha_Brahmin=subset(castes30plus_only_new_andstates4, caste1=="Brahmin-Deshastha"|caste1_1=="Brahmin" & subcaste=="Deshastha")
Kapu_AP=subset(castes30plus_only_new_andstates4, caste1_1=="Kapu" & State=="Andhra Pradesh"|caste1_1=="Kapu Naidu" & State=="Andhra Pradesh"|caste1_1=="Munnuru Kapu" & State=="Andhra Pradesh")
Kunbi_Maratha=subset(castes30plus_only_new_andstates4, caste1=="Maratha" & subcaste=="Kunbi Maratha")
Naidu_AP=subset(castes30plus_only_new_andstates4, caste1=="Naidu" & State=="Andhra Pradesh")
Reddy_AP=subset(castes30plus_only_new_andstates4, caste1=="Reddy" & State=="Andhra Pradesh")
Yadava_TN=subset(castes30plus_only_new_andstates4, caste1=="Yadav" & State=="Tamil Nadu")
Bhumihar_grep=castes30plus_only_new_andstates4[grep("Bhumihar", castes30plus_only_new_andstates4$caste1), ]
Bhumihar_UP_B=subset(Bhumihar_grep, State=="Uttar Pradesh"|State=="Bihar")
Kshatriya_grep=castes30plus_only_new_andstates4[grep("Kshatriya", castes30plus_only_new_andstates4$caste1), ]
Kshatriya_UP_B=subset(Kshatriya_grep, State=="Bihar"|State=="Uttar Pradesh")
Kurmi_grep=castes30plus_only_new_andstates4[grep("Kurmi", castes30plus_only_new_andstates4$caste1), ]
Kurmi_UP=subset(Kurmi_grep, State=="Uttar Pradesh")
Muslim_B=subset(castes30plus_only_new_andstates4, religion1=="Muslim"&State=="Bihar")
Paswan_grep=castes30plus_only_new_andstates4[grep("Paswan", castes30plus_only_new_andstates4$caste1), ]
Paswan_B=subset(Paswan_grep, State=="Bihar")
Yadav_B=subset(castes30plus_only_new_andstates4, caste1=="Yadav" & State=="Bihar")
Srivastava_UP_B=subset(castes30plus_only_new_andstates4, (subcaste=="Srivastava" & State=="Bihar")|(subcaste=="Srivastava" & State=="Uttar Pradesh"))
Thakur_B=subset(castes30plus_only_new_andstates4, (caste1=="Thakur" & State=="Bihar")|(subcaste=="Thakur"&State=="Bihar"))
#now make sss_av object for each caste 
Badaga_sss_av=sum(as.numeric(Badaga_TN$sss), na.rm=TRUE)/nrow(Badaga_TN[!is.na(Badaga_TN$sss)])
Bhumihar_UP_B_sss_av=sum(as.numeric(Bhumihar_UP_B$sss), na.rm=TRUE)/nrow(Bhumihar_UP_B[!is.na(Bhumihar_UP_B$sss)])
Brahmin_TN_sss_av=sum(as.numeric(Brahmin_TN$sss), na.rm=TRUE)/nrow(Brahmin_TN[!is.na(Brahmin_TN$sss)])
Brahmin_UP_sss_av=nrow(subset(Brahmin_UP, complexion2code==1|complexion2code==2))/nrow(Brahmin_UP[!is.na(Brahmin_UP$complexion2code)])
Deshastha_Brahmin_sss_av=sum(as.numeric(Deshastha_Brahmin$sss), na.rm=TRUE)/nrow(Deshastha_Brahmin[!is.na(Deshastha_Brahmin$sss)])
Kapu_AP_sss_av=sum(as.numeric(Kapu_AP$sss), na.rm=TRUE)/nrow(Kapu_AP[!is.na(Kapu_AP$sss)])
Kshatriya_UP_B_sss_av=sum(as.numeric(Kshatriya_UP_B$sss), na.rm=TRUE)/nrow(Kshatriya_UP_B[!is.na(Kshatriya_UP_B$sss)])
Kunbi_Maratha_sss_av=sum(as.numeric(Kunbi_Maratha$sss), na.rm=TRUE)/nrow(Kunbi_Maratha[!is.na(Kunbi_Maratha$sss)])
Kurmi_UP_sss_av=sum(as.numeric(Kurmi_UP$sss), na.rm=TRUE)/nrow(Kurmi_UP[!is.na(Kurmi_UP$sss)])
Muslim_B_sss_av=sum(as.numeric(Muslim_B$sss), na.rm=TRUE)/nrow(Muslim_B[!is.na(Muslim_B$sss)])
Naidu_AP_sss_av=sum(as.numeric(Naidu_AP$sss), na.rm=TRUE)/nrow(Naidu_AP[!is.na(Naidu_AP$sss)])
Paswan_B_sss_av=sum(as.numeric(Paswan_B$sss), na.rm=TRUE)/nrow(Paswan_B[!is.na(Paswan_B$sss)])
Reddy_AP_sss_av=sum(as.numeric(Reddy_AP$sss), na.rm=TRUE)/nrow(Reddy_AP[!is.na(Reddy_AP$sss)])
Srivastava_UP_B_sss_av=sum(as.numeric(Srivastava_UP_B$sss), na.rm=TRUE)/nrow(Srivastava_UP_B[!is.na(Srivastava_UP_B$sss)])
Thakur_B_sss_av=sum(as.numeric(Thakur_B$sss), na.rm=TRUE)/nrow(Thakur_B[!is.na(Thakur_B$sss)])
Yadav_B_sss_av=sum(as.numeric(Yadav_B$sss), na.rm=TRUE)/nrow(Yadav_B[!is.na(Yadav_B$sss)])
Yadava_TN_sss_av=sum(as.numeric(Yadava_TN$sss), na.rm=TRUE)/nrow(Yadava_TN[!is.na(Yadava_TN$sss)])
#add objects to MI_data_merge table 
MI_data_merge$sss_av[MI_data_merge$caste1=="Badaga"]=Badaga_sss_av
MI_data_merge$sss_av[MI_data_merge$caste1=="Brahmin"]=Brahmin_TN_sss_av
MI_data_merge$sss_av[MI_data_merge$caste1=="BrahminUP"]=Brahmin_UP_sss_av
MI_data_merge$sss_av[MI_data_merge$caste1=="Deshastha Brahmin"]=Deshastha_Brahmin_sss_av 
MI_data_merge$sss_av[MI_data_merge$caste1=="Kapu"]=Kapu_AP_sss_av 
MI_data_merge$sss_av[MI_data_merge$caste1=="Kunbi Maratha"]=Kunbi_Maratha_sss_av 
MI_data_merge$sss_av[MI_data_merge$caste1=="Reddy"]=Reddy_AP_sss_av 
MI_data_merge$sss_av[MI_data_merge$caste1=="Naidu"]=Naidu_AP_sss_av 
MI_data_merge$sss_av[MI_data_merge$caste1=="Yadava"]=Yadava_TN_sss_av 
MI_data_merge$sss_av[MI_data_merge$caste1=="Bhumihar"]=Bhumihar_UP_B_sss_av
MI_data_merge$sss_av[MI_data_merge$caste1=="Kshatriya"]=Kshatriya_UP_B_sss_av
MI_data_merge$sss_av[MI_data_merge$caste1=="Kurmis"]=Kurmi_UP_sss_av 
MI_data_merge$sss_av[MI_data_merge$caste1=="Muslim"]=Muslim_B_sss_av 
MI_data_merge$sss_av[MI_data_merge$caste1=="Paswan"]=Paswan_B_sss_av 
MI_data_merge$sss_av[MI_data_merge$caste1=="Srivastava"]=Srivastava_UP_B_sss_av
MI_data_merge$sss_av[MI_data_merge$caste1=="Thakur"]=Thakur_B_sss_av 
MI_data_merge$sss_av[MI_data_merge$caste1=="Yadav"]=Yadav_B_sss_av 
#now see if average strength of ss can explain MI 
sssmodel=lm(MI~sss_av, data=MI_data_merge)
summary(sssmodel)
#--> average sss is not significant variable 


#now see if lighter than myself in men correlates with degree of self-reporting as lighter than men in women across castes to test hypothesis 4
#first recode complexion codes in dataset 
castes30plus_only_new_andstates4$complexion1code[castes30plus_only_new_andstates4$complexion1=="Very Fair"]=1
castes30plus_only_new_andstates4$complexion1code[castes30plus_only_new_andstates4$complexion1=="Fair"]=2
castes30plus_only_new_andstates4$complexion1code[castes30plus_only_new_andstates4$complexion1=="Wheatish"]=3
castes30plus_only_new_andstates4$complexion1code[castes30plus_only_new_andstates4$complexion1=="Whetish Medium"]=4
castes30plus_only_new_andstates4$complexion1code[castes30plus_only_new_andstates4$complexion1=="Dark"]=5
castes30plus_only_new_andstates4$complexion2code[castes30plus_only_new_andstates4$complexion2=="Very Fair"]=1
castes30plus_only_new_andstates4$complexion2code[castes30plus_only_new_andstates4$complexion2=="Fair"]=2
castes30plus_only_new_andstates4$complexion2code[castes30plus_only_new_andstates4$complexion2=="Wheatish"]=3
castes30plus_only_new_andstates4$complexion2code[castes30plus_only_new_andstates4$complexion2=="Whetish Medium"]=4
castes30plus_only_new_andstates4$complexion2code[castes30plus_only_new_andstates4$complexion2=="Dark"]=5
castes30plus_only_new_andstates4$complexion2code[castes30plus_only_new_andstates4$complexion2=="Doesn't Matter"]=NA
castes30plus_only_new_andstates4$complexion1code[castes30plus_only_new_andstates4$complexion1=="Doesn't Matter"]=NA
#add complexion ratio
castes30plus_only_new_andstates4$code_ratio12=castes30plus_only_new_andstates4$complexion1code/castes30plus_only_new_andstates4$complexion2code
#now add assortment_type variable
castes30plus_only_new_andstates4$assortment_type[castes30plus_only_new_andstates4$code_ratio12>1]="Lighter"
castes30plus_only_new_andstates4$assortment_type[castes30plus_only_new_andstates4$code_ratio12<1]="Darker"
castes30plus_only_new_andstates4$assortment_type[castes30plus_only_new_andstates4$code_ratio12==1]="Same"
#now make subsets of men and women 
states4men=subset(castes30plus_only_new_andstates4, source=="cutdowngrooms")
states4women=subset(castes30plus_only_new_andstates4, source=="cutdownbrides")
#make column for menwantinglighter
states4men$wantlighter[states4men$assortment_type=="Lighter"]=1
#aggregate number wantinglighter by caste1 the proportion of men who want spouse to be lighter in males subset 
states4men_caste1_by_wantinglighter=aggregate(states4men$wantlighter, na.rm=TRUE, by=list(caste1=states4men$caste1), FUN=sum)
#make freq table of states4 dataset by caste1
library(dplyr)
states4freqtable = castes30plus_only_new_andstates4 %>% dplyr:: count(caste1)
#now make aggregated dataset of comp1 for men by caste1 i.e. to get average comp1 by caste 
states4men_comp1_by_caste1=aggregate(states4men$complexion1code, na.rm=TRUE, by=list(caste1=states4men$caste1), FUN=mean)
#repeat for women
states4women_comp1_by_caste1=aggregate(states4women$complexion1code, na.rm=TRUE, by=list(caste1=states4women$caste1), FUN=mean)
#now merge all 4 vital tables together 
sss_fulltable=merge(states4men_caste1_by_wantinglighter, states4freqtable, by="caste1", all.x=TRUE)
colnames(sss_fulltable)=c("caste1", "numbermenwantinglighter", "freq")
sss_fulltable2=merge(sss_fulltable, states4men_comp1_by_caste1, by="caste1", all.x=TRUE)
colnames(sss_fulltable2)=c("caste1", "numbermenwantinglighter", "freq", "comp1average_men")
sss_fulltable3=merge(sss_fulltable2, states4women_comp1_by_caste1, by="caste1", all.x=TRUE)
colnames(sss_fulltable3)=c("caste1", "numbermenwantinglighter", "freq", "comp1average_men", "comp1average_women")
#make column of proportion of men wanting lighter women 
sss_fulltable3$prop_menwantinglighterwomen=sss_fulltable3$numbermenwantinglighter/sss_fulltable3$freq
#make column of difference between men and women 
sss_fulltable3$degree_womensaylighter=sss_fulltable3$comp1average_men-sss_fulltable3$comp1average_women
#now can run correlation 
#check normality 
shapiro.test(sss_fulltable3$prop_menwantinglighterwomen)
#not normal, do kendall's tau 
cor.test(sss_fulltable3$prop_menwantinglighterwomen, sss_fulltable3$degree_womensaylighter, method="kendall")
#p-value = 0.5987, cannot reject null hypothesis and so no evidence of sexual selection
#plot to visualise
plot(sss_fulltable3$prop_menwantinglighterwomen, sss_fulltable3$degree_womensaylighter)

#now do reverse - does preference for darker than myself in women correlated with self-stated skin complexion in men
#make column for womenwantingdarker 
states4women$wantdarker=NA
states4women$wantdarker[states4women$assortment_type=="Darker"]=1
#aggregate number wantinglighter by caste1 the proportion of men who want spouse to be lighter in males subset 
states4women_caste1_by_wantingdarker=aggregate(states4women$wantdarker, na.rm=TRUE, by=list(caste1=states4women$caste1), FUN=sum)
#make freq table of states4 dataset by caste1
library(dplyr)
states4freqtable = castes30plus_only_new_andstates4 %>% dplyr:: count(caste1)
#now make aggregated dataset of comp1 for men by caste1 i.e. to get average comp1 by caste 
states4women_comp1_by_caste1=aggregate(states4women$complexion1code, na.rm=TRUE, by=list(caste1=states4women$caste1), FUN=mean)
#repeat for men
states4men_comp1_by_caste1=aggregate(states4men$complexion1code, na.rm=TRUE, by=list(caste1=states4men$caste1), FUN=mean)
#now merge all 4 vital tables together 
sss_fulltable_secondversion=merge(states4women_caste1_by_wantingdarker, states4freqtable, by="caste1", all.x=TRUE)
colnames(sss_fulltable_secondversion)=c("caste1", "numberwomenwantingdarker", "freq")
sss_fulltable_secondversion2=merge(sss_fulltable_secondversion, states4women_comp1_by_caste1, by="caste1", all.x=TRUE)
colnames(sss_fulltable_secondversion2)=c("caste1", "numberwomenwantingdarker", "freq", "comp1average_women")
sss_fulltable_secondversion3=merge(sss_fulltable_secondversion2, states4men_comp1_by_caste1, by="caste1", all.x=TRUE)
colnames(sss_fulltable_secondversion3)=c("caste1", "numberwomenwantingdarker", "freq", "comp1average_women", "comp1average_men")
#make column of proportion of women wanting darker men 
sss_fulltable_secondversion3$prop_womenwantingdarkermen=sss_fulltable_secondversion3$numberwomenwantingdarker/sss_fulltable_secondversion3$freq
#make column of difference between men and women 
sss_fulltable_secondversion3$degree_mensaydarker=sss_fulltable_secondversion3$comp1average_men-sss_fulltable_secondversion3$comp1average_women
#now can run correlation 
#check normality 
shapiro.test(sss_fulltable_secondversion3$prop_womenwantingdarkermen)
#not normal, do kendall's tau 
cor.test(sss_fulltable_secondversion3$prop_womenwantingdarkermen, sss_fulltable_secondversion3$degree_mensaydarker, method="kendall")
#p-value = 0.3729, cannot reject null hypothesis and so no evidence of sexual selection
#plot to visualise
plot(sss_fulltable_secondversion3$prop_womenwantingdarkermen, sss_fulltable_secondversion3$degree_mensaydarker)







##genetic analyses: testing hypotheses 3 and 4
#PLINK CODES
BIRHOR
plink --bfile Basu_India367 --keep Basu_Birhor_samples.txt --make-bed --out Basu_Birhors


plink --bfile Basu_Birhors --hardy --from rs1426654 --to rs1426654 --out Basu_Birhors_HWE_output


plink --bfile Basu_Birhors --freq --from rs1426654 --to rs1426654 --out Basu_Birhors_freq_output


plink --bfile Basu_Birhors --freqx --from rs1426654 --to rs1426654 --out Basu_Birhors_freqx_output


plink --bfile Basu_Birhors  --max-maf 0.01 --make-bed --out Basu_Birhors_similarMAF
 

plink --bfile Basu_Birhors_similarMAF --hardy --out Basu_Birhors_similarMAF_hardy

plink --bfile Basu_Birhors_similarMAF --freqx  --out Basu_Birhors_similarMAF_freqx

plink --bfile Basu_Birhors_similarMAF --freq --out Basu_Birhors_similarMAF_freq



GOND
plink --bfile Basu_India367 --keep Basu_Gond_samples.txt --make-bed --out Basu_Gonds


plink --bfile Basu_Gonds --hardy --from rs1426654 --to rs1426654 --out Basu_Gonds_HWE_output


plink --bfile Basu_Gonds --freq --from rs1426654 --to rs1426654 --out Basu_Gonds_freq_output


plink --bfile Basu_Gonds --freqx --from rs1426654 --to rs1426654 --out Basu_Gonds_freqx_output


plink --bfile Basu_Gonds  --maf 0.25 --max-maf 0.26 --make-bed --out Basu_Gonds_similarMAF
 

plink --bfile Basu_Gonds_similarMAF --hardy --out Basu_Gonds_similarMAF_hardy

plink --bfile Basu_Gonds_similarMAF --freqx  --out Basu_Gonds_similarMAF_freqx

plink --bfile Basu_Gonds_similarMAF --freq --out Basu_Gonds_similarMAF_freq



GUJARATIS (Brahmins)
plink --bfile Basu_India367 --keep Basu_Gujurati_samples.txt --make-bed --out Basu_GujuratiBrahmins


plink --bfile Basu_GujuratiBrahmins --hardy --from rs1426654 --to rs1426654 --out Basu_GujuratiBrahmins_HWE_output


plink --bfile Basu_GujuratiBrahmins --freq --from rs1426654 --to rs1426654 --out Basu_GujuratiBrahmins_freq_output


plink --bfile Basu_GujuratiBrahmins --freqx --from rs1426654 --to rs1426654 --out Basu_GujuratiBrahmins_freqx_output


plink --bfile Basu_GujuratiBrahmins  --maf 0.15 --max-maf 0.16 --make-bed --out Basu_GujuratiBrahmins_similarMAF
 

plink --bfile Basu_GujuratiBrahmins_similarMAF --hardy --out Basu_GujuratiBrahmins_similarMAF_hardy

plink --bfile Basu_GujuratiBrahmins_similarMAF --freqx  --out Basu_GujuratiBrahmins_similarMAF_freqx

plink --bfile Basu_GujuratiBrahmins_similarMAF --freq --out Basu_GujuratiBrahmins_similarMAF_freq



GUJJAR
plink --bfile Pathak_2018 --keep Pathak_2018_Gujjars.txt --make-bed --out Pathak_Gujjars

plink --bfile Pathak_Gujjars --hardy --from rs1426654 --to rs1426654 --out Pathak_Gujjars_HWE_output


plink --bfile Pathak_Gujjars --freq --from rs1426654 --to rs1426654 --out Pathak_Gujjars_freq_output


plink --bfile Pathak_Gujjars --freqx --from rs1426654 --to rs1426654 --out Pathak_Gujjars_freqx_output

plink --bfile Pathak_Gujjars --maf 0.16 --max-maf 0.17 --make-bed --out Pathak_Gujjars_similarMAF
 

plink --bfile Pathak_Gujjars_similarMAF --hardy --out Pathak_Gujjars_similarMAF_hardy

plink --bfile Pathak_Gujjars_similarMAF --freqx  --out Pathak_Gujjars_similarMAF_freqx

plink --bfile Pathak_Gujjars_similarMAF --freq --out Pathak_Gujjars_similarMAF_freq




HO
plink --bfile Basu_India367 --keep Basu_Ho_samples.txt --make-bed --out Basu_Hos


plink --bfile Basu_Hos --hardy --from rs1426654 --to rs1426654 --out Basu_Hos_HWE_output


plink --bfile Basu_Hos --freq --from rs1426654 --to rs1426654 --out Basu_Hos_freq_output


plink --bfile Basu_Hos --freqx --from rs1426654 --to rs1426654 --out Basu_Hos_freqx_output


plink --bfile Basu_Hos --maf 0.02 --max-maf 0.03 --make-bed --out Basu_Hos_similarMAF
 

plink --bfile Basu_Hos_similarMAF --hardy --out Basu_Hos_similarMAF_hardy

plink --bfile Basu_Hos_similarMAF --freqx  --out Basu_Hos_similarMAF_freqx

plink --bfile Basu_Hos_similarMAF --freq --out Basu_Hos_similarMAF_freq




IRULA
plink --bfile Basu_India367 --keep Basu_Irula_samples.txt --make-bed --out Basu_Irulas


plink --bfile Basu_Irulas --hardy --from rs1426654 --to rs1426654 --out Basu_Irulas_HWE_output


plink --bfile Basu_Irulas --freq --from rs1426654 --to rs1426654 --out Basu_Irulas_freq_output


plink --bfile Basu_Irulas --freqx --from rs1426654 --to rs1426654 --out Basu_Irulas_freqx_output


plink --bfile Basu_Irulas --maf 0.325 --max-maf 0.325 --make-bed --out Basu_Irulas_similarMAF
 
plink --bfile Basu_Irulas_similarMAF --hardy --out Basu_Irulas_similarMAF_hardy

plink --bfile Basu_Irulas_similarMAF --freqx  --out Basu_Irulas_similarMAF_freqx

plink --bfile Basu_Irulas_similarMAF --freq --out Basu_Irulas_similarMAF_freq




IYER
plink --bfile Basu_India367 --keep Basu_Iyer_samples.txt --make-bed --out Basu_Iyers

plink --bfile Basu_Iyers --hardy --from rs1426654 --to rs1426654 --out Basu_Iyers_HWE_output


plink --bfile Basu_Iyers --freq --from rs1426654 --to rs1426654 --out Basu_Iyers_freq_output


plink --bfile Basu_Iyers --freqx --from rs1426654 --to rs1426654 --out Basu_Iyers_freqx_output


plink --bfile Basu_Iyers --maf 0.3 --max-maf 0.3 --make-bed --out Basu_Iyers_similarMAF
 

plink --bfile Basu_Iyers_similarMAF --hardy --out Basu_Iyers_similarMAF_hardy

plink --bfile Basu_Iyers_similarMAF --freqx  --out Basu_Iyers_similarMAF_freqx

plink --bfile Basu_Iyers_similarMAF --freq --out Basu_Iyers_similarMAF_freq




JAMATIA
plink --bfile Basu_India367 --keep Basu_Jamatia_samples.txt --make-bed --out Basu_Jamatias

plink --bfile Basu_Jamatias --hardy --from rs1426654 --to rs1426654 --out Basu_Jamatias_HWE_output


plink --bfile Basu_Jamatias --freq --from rs1426654 --to rs1426654 --out Basu_Jamatias_freq_output


plink --bfile Basu_Jamatias --freqx --from rs1426654 --to rs1426654 --out Basu_Jamatias_freqx_output


plink --bfile Basu_Jamatias --maf 0.08 --max-maf 0.09 --make-bed --out Basu_Jamatias_similarMAF
 

plink --bfile Basu_Jamatias_similarMAF --hardy --out Basu_Jamatias_similarMAF_hardy

plink --bfile Basu_Jamatias_similarMAF --freqx  --out Basu_Jamatias_similarMAF_freqx

plink --bfile Basu_Jamatias_similarMAF --freq --out Basu_Jamatias_similarMAF_freq



JARAWA
plink --bfile Basu_India367 --keep Basu_Jarawa_samples.txt --make-bed --out Basu_Jarawas

plink --bfile Basu_Jarawas --hardy --from rs1426654 --to rs1426654 --out Basu_Jarawas_HWE_output


plink --bfile Basu_Jarawas --freq --from rs1426654 --to rs1426654 --out Basu_Jarawas_freq_output


plink --bfile Basu_Jarawas --freqx --from rs1426654 --to rs1426654 --out Basu_Jarawas_freqx_output

plink --bfile Basu_Jarawas --max-maf 0.01 --make-bed --out Basu_Jarawas_similarMAF
 

plink --bfile Basu_Jarawas_similarMAF --hardy --out Basu_Jarawas_similarMAF_hardy

plink --bfile Basu_Jarawas_similarMAF --freqx --out Basu_Jarawas_similarMAF_freqx

plink --bfile Basu_Jarawas_similarMAF --freq --out Basu_Jarawas_similarMAF_freq





KADAR
plink --bfile Basu_India367 --keep Basu_Kadar_samples.txt --make-bed --out Basu_Kadars

plink --bfile Basu_Kadars --hardy --from rs1426654 --to rs1426654 --out Basu_Kadars_HWE_output


plink --bfile Basu_Kadars --freq --from rs1426654 --to rs1426654 --out Basu_Kadars_freq_output


plink --bfile Basu_Kadars --freqx --from rs1426654 --to rs1426654 --out Basu_Kadars_freqx_output

plink --bfile Basu_Kadars --maf 0.15 --max-maf 0.16 --make-bed --out Basu_Kadars_similarMAF
 

plink --bfile Basu_Kadars_similarMAF --hardy --out Basu_Kadars_similarMAF_hardy

plink --bfile Basu_Jarawas_similarMAF --freqx  --out Basu_Kadars_similarMAF_freqx

plink --bfile Basu_Kadars_similarMAF --freq  --out Basu_Kadars_similarMAF_freq



Kamboj
plink --bfile Pathak_2018 --keep Pathak_2018_Kambojs.txt --make-bed --out Pathak_Kambojs

plink --bfile Pathak_Kambojs --hardy --from rs1426654 --to rs1426654 --out Pathak_Kambojs_HWE_output


plink --bfile Pathak_Kambojs --freq --from rs1426654 --to rs1426654 --out Pathak_Kambojs_freq_output


plink --bfile Pathak_Kambojs --freqx --from rs1426654 --to rs1426654 --out Pathak_Kambojs_freqx_output

plink --bfile Pathak_Kambojs --maf 0.03 --max-maf 0.04 --make-bed --out Pathak_Kambojs_similarMAF
 

plink --bfile Pathak_Kambojs_similarMAF --hardy --out Pathak_Kambojs_similarMAF_hardy

plink --bfile Pathak_Kambojs_similarMAF --freqx --out Pathak_Kambojs_similarMAF_freqx

plink --bfile Pathak_Kambojs_similarMAF --freq --out Pathak_Kambojs_similarMAF_freq





Khatri
plink --bfile Basu_India367 --keep Basu_Kshatriya_samples.txt --make-bed --out Basu_Khatris

plink --bfile Basu_Khatris --hardy --from rs1426654 --to rs1426654 --out Basu_Khatris_HWE_output


plink --bfile Basu_Khatris --freq --from rs1426654 --to rs1426654 --out Basu_Khatris_freq_output


plink --bfile Basu_Khatris --freqx --from rs1426654 --to rs1426654 --out Basu_Khatris_freqx_output

plink --bfile Basu_Khatris --maf 0.05 --max-maf 0.06 --make-bed --out Basu_Khatris_similarMAF
 

plink --bfile Basu_Khatris_similarMAF --hardy --out Basu_Khatris_similarMAF_hardy

plink --bfile Basu_Khatris_similarMAF --freqx  --out Basu_Khatris_similarMAF_freqx

plink --bfile Basu_Khatris_similarMAF --freq --out Basu_Khatris_similarMAF_freq




Korwa
plink --bfile Basu_India367 --keep Basu_Korwa_samples.txt --make-bed --out Basu_Korwas

plink --bfile Basu_Korwas --hardy --from rs1426654 --to rs1426654 --out Basu_Korwas_HWE_output


plink --bfile Basu_Korwas --freq --from rs1426654 --to rs1426654 --out Basu_Korwas_freq_output


plink --bfile Basu_Korwas --freqx --from rs1426654 --to rs1426654 --out Basu_Korwas_freqx_output

plink --bfile Basu_Korwas --maf 0.02 --max-maf 0.03 --make-bed --out Basu_Korwas_similarMAF
 

plink --bfile Basu_Korwas_similarMAF --hardy --out Basu_Korwas_similarMAF_hardy

plink --bfile Basu_Korwas_similarMAF --freqx  --out Basu_Korwas_similarMAF_freqx

plink --bfile Basu_Korwas_similarMAF --freq --out Basu_Korwas_similarMAF_freq





Manipuri Brahmin
plink --bfile Basu_India367 --keep Basu_ManipuriBrahmin_samples.txt --make-bed --out Basu_ManipuriBrahmins

plink --bfile Basu_ManipuriBrahmins --hardy --from rs1426654 --to rs1426654 --out Basu_ManipuriBrahmins_HWE_output


plink --bfile Basu_ManipuriBrahmins --freq --from rs1426654 --to rs1426654 --out Basu_ManipuriBrahmins_freq_output


plink --bfile Basu_ManipuriBrahmins --freqx --from rs1426654 --to rs1426654 --out Basu_ManipuriBrahmins_freqx_output

plink --bfile Basu_ManipuriBrahmins --maf 0.45 --max-maf 0.46 --make-bed --out Basu_ManipuriBrahmins_similarMAF
 

plink --bfile Basu_ManipuriBrahmins_similarMAF --hardy --out Basu_ManipuriBrahmins_similarMAF_hardy

plink --bfile Basu_ManipuriBrahmins_similarMAF --freqx  --out Basu_ManipuriBrahmins_similarMAF_freqx

plink --bfile Basu_ManipuriBrahmins_similarMAF --freq --out Basu_ManipuriBrahmins_similarMAF_freq





Marathas
plink --bfile Basu_India367 --keep Basu_Maratha_samples.txt --make-bed --out Basu_Marathas

plink --bfile Basu_Marathas --hardy --from rs1426654 --to rs1426654 --out Basu_Marathas_HWE_output


plink --bfile Basu_Marathas --freq --from rs1426654 --to rs1426654 --out Basu_Marathas_freq_output


plink --bfile Basu_Marathas --freqx --from rs1426654 --to rs1426654 --out Basu_Marathas_freqx_output

plink --bfile Basu_Marathas --maf 0.21 --max-maf 0.22 --make-bed --out Basu_Marathas_similarMAF
 

plink --bfile Basu_Marathas_similarMAF --hardy --out Basu_Marathas_similarMAF_hardy

plink --bfile Basu_Marathas_similarMAF --freqx  --out Basu_Marathas_similarMAF_freqx

plink --bfile Basu_Marathas_similarMAF --freq --out Basu_Marathas_similarMAF_freq



Onge
plink --bfile Basu_India367 --keep Basu_Onge_samples.txt --make-bed --out Basu_Onges

plink --bfile Basu_Onges --hardy --from rs1426654 --to rs1426654 --out Basu_Onges_HWE_output


plink --bfile Basu_Onges --freq --from rs1426654 --to rs1426654 --out Basu_Onges_freq_output


plink --bfile Basu_Onges --freqx --from rs1426654 --to rs1426654 --out Basu_Onges_freqx_output

plink --bfile Basu_Onges --max-maf 0.01 --make-bed --out Basu_Onges_similarMAF
 

plink --bfile Basu_Onges_similarMAF --hardy --out Basu_Onges_similarMAF_hardy

plink --bfile Basu_Onges_similarMAF --freqx  --out Basu_Onges_similarMAF_freqx

plink --bfile Basu_Onges_similarMAF --freq --out Basu_Onges_similarMAF_freq





Pallan
plink --bfile Basu_India367 --keep Basu_Pallan_samples.txt --make-bed --out Basu_Pallans

plink --bfile Basu_Pallans --hardy --from rs1426654 --to rs1426654 --out Basu_Pallans_HWE_output


plink --bfile Basu_Pallans --freq --from rs1426654 --to rs1426654 --out Basu_Pallans_freq_output


plink --bfile Basu_Pallans --freqx --from rs1426654 --to rs1426654 --out Basu_Pallans_freqx_output

plink --bfile Basu_Pallans --maf 0.22 --max-maf 0.23 --make-bed --out Basu_Pallans_similarMAF
 

plink --bfile Basu_Pallans_similarMAF --hardy --out Basu_Pallans_similarMAF_hardy

plink --bfile Basu_Pallans_similarMAF --freqx  --out Basu_Pallans_similarMAF_freqx

plink --bfile Basu_Pallans_similarMAF --freq --out Basu_Pallans_similarMAF_freq




Paniya 
plink --bfile Basu_India367 --keep Basu_Paniya_samples.txt --make-bed --out Basu_Paniyas

plink --bfile Basu_Paniyas --hardy --from rs1426654 --to rs1426654 --out Basu_Paniyas_HWE_output


plink --bfile Basu_Paniyas --freq --from rs1426654 --to rs1426654 --out Basu_Paniyas_freq_output


plink --bfile Basu_Paniyas --freqx --from rs1426654 --to rs1426654 --out Basu_Paniyas_freqx_output

plink --bfile Basu_Paniyas --maf 0.22 --max-maf 0.23 --make-bed --out Basu_Paniyas_similarMAF
 

plink --bfile Basu_Paniyas_similarMAF --hardy --out Basu_Paniyas_similarMAF_hardy

plink --bfile Basu_Paniyas_similarMAF --freqx  --out Basu_Paniyas_similarMAF_freqx

plink --bfile Basu_Paniyas_similarMAF --freq --out Basu_Paniyas_similarMAF_freq


Ror
plink --bfile Pathak_2018 --keep Pathak_2018_Rors.txt --make-bed --out Pathak_Rors

plink --bfile Pathak_Rors --hardy --from rs1426654 --to rs1426654 --out Pathak_Rors_HWE_output


plink --bfile Pathak_Rors --freq --from rs1426654 --to rs1426654 --out Pathak_Rors_freq_output


plink --bfile Pathak_Rors --freqx --from rs1426654 --to rs1426654 --out Pathak_Rors_freqx_output

plink --bfile Pathak_Rors  --max-maf 0.01 --make-bed --out Pathak_Rors_similarMAF
 

plink --bfile Pathak_Rors_similarMAF --hardy --out Pathak_Rors_similarMAF_hardy

plink --bfile Pathak_Rors_similarMAF --freqx  --out Pathak_Rors_similarMAF_freqx

plink --bfile Pathak_Rors_similarMAF --freq  --out Pathak_Rors_similarMAF_freq



Santal
plink --bfile Basu_India367 --keep Basu_Santhal_samples.txt --make-bed --out Basu_Santals

plink --bfile Basu_Santals --hardy --from rs1426654 --to rs1426654 --out Basu_Santals_HWE_output


plink --bfile Basu_Santals --freq --from rs1426654 --to rs1426654 --out Basu_Santals_freq_output


plink --bfile Basu_Santals --freqx --from rs1426654 --to rs1426654 --out Basu_Santals_freqx_output

plink --bfile Basu_Santals --max-maf 0.01 --make-bed --out Basu_Santals_similarMAF
 

plink --bfile Basu_Santals_similarMAF --hardy --out Basu_Santals_similarMAF_hardy

plink --bfile Basu_Santals_similarMAF --freqx  --out Basu_Santals_similarMAF_freqx

plink --bfile Basu_Santals_similarMAF --freq --out Basu_Santals_similarMAF_freq







Tharu
plink --bfile Basu_India367 --keep Basu_Tharu_samples.txt --make-bed --out Basu_Tharus

plink --bfile Basu_Tharus --hardy --from rs1426654 --to rs1426654 --out Basu_Tharus_HWE_output


plink --bfile Basu_Tharus --freq --from rs1426654 --to rs1426654 --out Basu_Tharus_freq_output


plink --bfile Basu_Tharus --freqx --from rs1426654 --to rs1426654 --out Basu_Tharus_freqx_output

plink --bfile Basu_Tharus --maf 0.17 --max-maf 0.18 --make-bed --out Basu_Tharus_similarMAF
 

plink --bfile Basu_Tharus_similarMAF --hardy --out Basu_Tharus_similarMAF_hardy

plink --bfile Basu_Tharus_similarMAF --freqx  --out Basu_Tharus_similarMAF_freqx

plink --bfile Basu_Tharus_similarMAF --freq --out Basu_Tharus_similarMAF_freq



TRIPURI
plink --bfile Basu_India367 --keep Basu_Tripuri_samples.txt --make-bed --out Basu_Tripuris

plink --bfile Basu_Tripuris --hardy --from rs1426654 --to rs1426654 --out Basu_Tripuris_HWE_output


plink --bfile Basu_Tripuris --freq --from rs1426654 --to rs1426654 --out Basu_Tripuris_freq_output


plink --bfile Basu_Tripuris --freqx --from rs1426654 --to rs1426654 --out Basu_Tripuris_freqx_output

plink --bfile Basu_Tripuris --maf 0.13 --max-maf 0.14 --make-bed --out Basu_Tripuris_similarMAF
 

plink --bfile Basu_Tripuris_similarMAF --hardy --out Basu_Tripuris_similarMAF_hardy

plink --bfile Basu_Tripuris_similarMAF --freqx  --out Basu_Tripuris_similarMAF_freqx

plink --bfile Basu_Tripuris_similarMAF --freq --out Basu_Tripuris_similarMAF_freq




WEST BENGAL BRAHMINS
plink --bfile Basu_India367 --keep Basu_WestBengalBrahmins_samples.txt --make-bed --out Basu_WestBengalBrahmins

plink --bfile Basu_WestBengalBrahmins --hardy --from rs1426654 --to rs1426654 --out Basu_WestBengalBrahmins_HWE_output


plink --bfile Basu_WestBengalBrahmins --freq --from rs1426654 --to rs1426654 --out Basu_WestBengalBrahmins_freq_output


plink --bfile Basu_WestBengalBrahmins --freqx --from rs1426654 --to rs1426654 --out Basu_WestBengalBrahmins_freqx_output

plink --bfile Basu_WestBengalBrahmins --maf 0.11 --max-maf 0.12 --make-bed --out Basu_WestBengalBrahmins_similarMAF
 

plink --bfile Basu_WestBengalBrahmins_similarMAF --hardy --out Basu_WestBengalBrahmins_similarMAF_hardy

plink --bfile Basu_WestBengalBrahmins_similarMAF --freqx --out Basu_WestBengalBrahmins_similarMAF_freqx

plink --bfile Basu_WestBengalBrahmins_similarMAF --out Basu_WestBengalBrahmins_similarMAF_freq





#Getting India-wide genome-wide Z scores to compare position of rs1426654 SNP - using similar MAF and variance for “similar” SNPs
#Need to get 14 individuals of each pop separately (excluding Maratha), make bed file for the 14 samples, then make freq and hwe file 

plink --bfile Basu_India367 --keep Birhors_14.txt --hwe 1.27e-24 --make-bed --out Basu_Birhors_14
plink --bfile Basu_Birhors_14 --freq --out Basu_Birhors_14_allSNPs_freq_output

plink --bfile Basu_India367 --keep Gonds_14.txt --hwe 1.27e-24 --make-bed --out Basu_Gonds_14
plink --bfile Basu_Gonds_14 --freq --out Basu_Gonds_14_allSNPs_freq_output

plink --bfile Basu_India367 --keep GujuratiBrahmins_14.txt --hwe 1.27e-24 --make-bed --out Basu_GujuratiBrahmins_14
plink --bfile Basu_GujuratiBrahmins_14 --freq --out Basu_GujuratiBrahmins_14_allSNPs_freq_output

plink --bfile Basu_India367 --keep Hos_14.txt --hwe 1.27e-24 --make-bed --out Basu_Hos_14
plink --bfile Basu_Hos_14 --freq --out Basu_Hos_14_allSNPs_freq_output

plink --bfile Basu_India367 --keep Irulas_14.txt --hwe 1.27e-24 --make-bed --out Basu_Irulas_14
plink --bfile Basu_Irulas_14 --freq --out Basu_Irulas_14_allSNPs_freq_output

plink --bfile Basu_India367 --keep Iyers_14.txt --hwe 1.27e-24 --make-bed --out Basu_Iyers_14
plink --bfile Basu_Iyers_14 --freq --out Basu_Iyers_14_allSNPs_freq_output

plink --bfile Basu_India367 --keep Jamatias_14.txt --hwe 1.27e-24 --make-bed --out Basu_Jamatias_14
plink --bfile Basu_Jamatias_14 --freq --out Basu_Jamatias_14_allSNPs_freq_output

plink --bfile Basu_India367 --keep Jarawas_14.txt --hwe 1.27e-24 --make-bed --out Basu_Jarawas_14
plink --bfile Basu_Jarawas_14 --freq --out Basu_Jarawas_14_allSNPs_freq_output

plink --bfile Basu_India367 --keep Kadars_14.txt --hwe 1.27e-24 --make-bed --out Basu_Kadars_14
plink --bfile Basu_Kadars_14 --freq --out Basu_Kadars_14_allSNPs_freq_output

plink --bfile Basu_India367 --keep Khatris_14.txt --hwe 1.27e-24 --make-bed --out Basu_Khatris_14
plink --bfile Basu_Khatris_14 --freq --out Basu_Khatris_14_allSNPs_freq_output

plink --bfile Basu_India367 --keep Korwas_14.txt --hwe 1.27e-24 --make-bed --out Basu_Korwas_14
plink --bfile Basu_Korwas_14 --freq --out Basu_Korwas_14_allSNPs_freq_output

plink --bfile Basu_India367 --keep ManipuriBrahmins_14.txt --hwe 1.27e-24 --make-bed --out Basu_ManipuriBrahmins_14
plink --bfile Basu_ManipuriBrahmins_14 --freq --out Basu_ManipuriBrahmins_14_allSNPs_freq_output

plink --bfile Basu_India367 --keep Onges_14.txt --hwe 1.27e-24 --make-bed --out Basu_Onges_14
plink --bfile Basu_Onges_14 --freq --out Basu_Onges_14_allSNPs_freq_output

plink --bfile Basu_India367 --keep Pallans_14.txt --hwe 1.27e-24 --make-bed --out Basu_Pallans_14
plink --bfile Basu_Pallans_14 --freq --out Basu_Pallans_14_allSNPs_freq_output

plink --bfile Basu_India367 --keep Paniyas_14.txt --hwe 1.27e-24 --make-bed --out Basu_Paniyas_14
plink --bfile Basu_Paniyas_14 --freq --out Basu_Paniyas_14_allSNPs_freq_output

plink --bfile Basu_India367 --keep Santals_14.txt --hwe 1.27e-24 --make-bed --out Basu_Santals_14
plink --bfile Basu_Santals_14 --freq --out Basu_Santals_14_allSNPs_freq_output

plink --bfile Basu_India367 --keep Tharus_14.txt --hwe 1.27e-24 --make-bed --out Basu_Tharus_14
plink --bfile Basu_Tharus_14 --freq --out Basu_Tharus_14_allSNPs_freq_output

plink --bfile Basu_India367 --keep Tripuris_14.txt --hwe 1.27e-24 --make-bed --out Basu_Tripuris_14
plink --bfile Basu_Tripuris_14 --freq --out Basu_Tripuris_14_allSNPs_freq_output

plink --bfile Basu_India367 --keep WestBengalBrahmins_14.txt --hwe 1.27e-24 --make-bed --out Basu_WestBengalBrahmins_14
plink --bfile Basu_WestBengalBrahmins_14 --freq --out Basu_WestBengalBrahmins_14_allSNPs_freq_output

plink --bfile Pathak_2018 --keep Gujjars_14.txt --hwe 1.27e-24 --make-bed --out Pathak_Gujjars_14
plink --bfile Pathak_Gujjars_14 --freq --out Pathak_Gujjars_14_allSNPs_freq_output

plink --bfile Pathak_2018 --keep Kambojs_14.txt --hwe 1.27e-24 --make-bed --out Pathak_Kambojs_14
plink --bfile Pathak_Kambojs_14 --freq --out Pathak_Kambojs_14_allSNPs_freq_output

plink --bfile Pathak_2018 --keep Rors_14.txt --hwe 1.27e-24 --make-bed --out Pathak_Rors_14
plink --bfile Pathak_Rors_14 --freq --out Pathak_Rors_14_allSNPs_freq_output







Exploratory analysis 2: Finding evidence of excess Wahlund effect in 40okb wide region of rs1426654 in each population individually → did this in all populations except the 5 with MAF=0
plink --bfile Basu_Gonds --hardy --out Basu_Gond_allSNPs_HWE_output
plink --bfile Basu_Gonds --freq --out Basu_Gond_allSNPs_freq_output


plink --bfile Basu_GujuratiBrahmins --hardy --out Basu_GujuratiBrahmin_allSNPs_HWE_output
plink --bfile Basu_GujuratiBrahmins --freq --out Basu_GujuratiBrahmin_allSNPs_freq_output


plink --bfile Basu_Hos --hardy --out Basu_Ho_allSNPs_HWE_output
plink --bfile Basu_Hos --freq --out Basu_Ho_allSNPs_freq_output


plink --bfile Basu_Irulas --hardy --out Basu_Irula_allSNPs_HWE_output
plink --bfile Basu_Irulas --freq --out Basu_Irula_allSNPs_freq_output


plink --bfile Basu_Iyers --hardy --out Basu_Iyer_allSNPs_HWE_output
plink --bfile Basu_Iyers --freq --out Basu_Iyer_allSNPs_freq_output


plink --bfile Basu_Jamatias --hardy --out Basu_Jamatia_allSNPs_HWE_output
plink --bfile Basu_Jamatias --freq --out Basu_Jamatia_allSNPs_freq_output


plink --bfile Basu_Kadars --hardy --out Basu_Kadar_allSNPs_HWE_output
plink --bfile Basu_Kadars --freq --out Basu_Kadar_allSNPs_freq_output


plink --bfile Basu_Khatris --hardy --out Basu_Khatri_allSNPs_HWE_output
plink --bfile Basu_Khatris --freq --out Basu_Khatri_allSNPs_freq_output


plink --bfile Basu_Korwas --hardy --out Basu_Korwa_allSNPs_HWE_output
plink --bfile Basu_Korwas --freq --out Basu_Korwa_allSNPs_freq_output


In case required for later: info also for extra pops (with MAF0 of functional SNPs) 

plink --bfile Basu_Birhors --hardy --out Basu_Birhor_allSNPs_HWE_output
plink --bfile Basu_Birhors --freq --out Basu_Birhor_allSNPs_freq_output

plink --bfile Basu_Jarawas --hardy --out Basu_Jarawa_allSNPs_HWE_output
plink --bfile Basu_Jarawas --freq --out Basu_Jarawa_allSNPs_freq_output

plink --bfile Basu_Santals --hardy --out Basu_Santal_allSNPs_HWE_output
plink --bfile Basu_Santals --freq --out Basu_Santal_allSNPs_freq_output

plink --bfile Basu_Onges --hardy --out Basu_Onge_allSNPs_HWE_output
plink --bfile Basu_Onges --freq --out Basu_Onge_allSNPs_freq_output

plink --bfile Pathak_Rors --hardy --out Pathak_Ror_allSNPs_HWE_output
plink --bfile Pathak_Rors --freq --out Pathak_Ror_allSNPs_freq_output
plink --bfile Basu_ManipuriBrahmins --hardy --out Basu_ManipuriBrahmin_allSNPs_HWE_output
plink --bfile Basu_ManipuriBrahmins --freq --out Basu_ManipuriBrahmin_allSNPs_freq_output


plink --bfile Basu_Marathas --hardy --out Basu_Maratha_allSNPs_HWE_output
plink --bfile Basu_Marathas --freq --out Basu_Maratha_allSNPs_freq_output


plink --bfile Basu_Pallans --hardy --out Basu_Pallan_allSNPs_HWE_output
plink --bfile Basu_Pallans --freq --out Basu_Pallan_allSNPs_freq_output


plink --bfile Basu_Paniyas --hardy --out Basu_Paniya_allSNPs_HWE_output
plink --bfile Basu_Paniyas --freq --out Basu_Paniya_allSNPs_freq_output


plink --bfile Basu_Tharus --hardy --out Basu_Tharu_allSNPs_HWE_output
plink --bfile Basu_Tharus --freq --out Basu_Tharu_allSNPs_freq_output


plink --bfile Basu_Tripuris --hardy --out Basu_Tripuri_allSNPs_HWE_output
plink --bfile Basu_Tripuris --freq --out Basu_Tripuri_allSNPs_freq_output

plink --bfile Basu_WestBengalBrahmins --hardy --out Basu_WestBengalBrahmin_allSNPs_HWE_output
plink --bfile Basu_WestBengalBrahmins --freq --out Basu_WestBengalBrahmin_allSNPs_freq_output

plink --bfile Pathak_Gujjars --hardy --out Pathak_Gujjar_allSNPs_HWE_output
plink --bfile Pathak_Gujjars --freq --out Pathak_Gujjar_allSNPs_freq_output

plink --bfile Pathak_Kambojs --hardy --out Pathak_Kamboj_allSNPs_HWE_output
plink --bfile Pathak_Kambojs --freq --out Pathak_Kamboj_allSNPs_freq_output


Exploratory analysis 3: produce bim file for all Basu India and Pathak samples merged (i.e. not limited to 14 per pop and including pops even with MAF0 for functional SNP) - needed for PCA using pcadapt package:
plink --bfile Basu_India367 --bmerge Pathak_2018 --make-bed --out Basu_Pathak_merged
plink --bfile Basu_India367 --flip  Basu_Pathak_merged-merge.missnp. --make-bed --out Basu_Pathak_merged2
plink --bfile Basu_Pathak_merged2 --bmerge Pathak_2018 --make-bed --out Basu_Pathak_merged3
plink --bfile Basu_Pathak_merged3 --hardy --out Basu_Pathak_merged3_HWE_output
plink --bfile Basu_Pathak_merged3 --freq --out Basu_Pathak_merged3_freq_output
 





#in R, now load all freq, freqx and hwe files
#BIRHORS
Basu_Birhors_freq_table=read.table("Basu_Birhors_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Birhors_freqx_table=read.table("Basu_Birhors_freqx_output.frqx", sep="\t",stringsAsFactors = FALSE, header=TRUE)
Basu_Birhors_hwe_table=read.table("Basu_Birhors_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header = TRUE)
#similar MAF
Basu_Birhors_similarMAF_freq_table=read.table("Basu_Birhors_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Birhors_similarMAF_freqx_table=read.table("Basu_Birhors_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Basu_Birhors_similarMAF_hwe_table=read.table("Basu_Birhors_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)




#GOND
Basu_Gonds_freq_table=read.table("Basu_Gonds_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Gonds_freqx_table=read.table("Basu_Gonds_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Basu_Gonds_hwe_table=read.table("Basu_Gonds_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Basu_Gonds_similarMAF_freq_table=read.table("Basu_Gonds_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Gonds_similarMAF_freqx_table=read.table("Basu_Gonds_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Basu_Gonds_similarMAF_hwe_table=read.table("Basu_Gonds_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)




#GujuratiBrahminS (Brahmin)
Basu_GujuratiBrahmins_freq_table=read.table("Basu_GujuratiBrahmins_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_GujuratiBrahmins_freqx_table=read.table("Basu_GujuratiBrahmins_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Basu_GujuratiBrahmins_hwe_table=read.table("Basu_GujuratiBrahmins_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Basu_GujuratiBrahmins_similarMAF_freq_table=read.table("Basu_GujuratiBrahmins_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_GujuratiBrahmins_similarMAF_freqx_table=read.table("Basu_GujuratiBrahmins_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Basu_GujuratiBrahmins_similarMAF_hwe_table=read.table("Basu_GujuratiBrahmins_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)


#HOs
Basu_Hos_freq_table=read.table("Basu_Hos_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Hos_freqx_table=read.table("Basu_Hos_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Basu_Hos_hwe_table=read.table("Basu_Hos_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Basu_Hos_similarMAF_freq_table=read.table("Basu_Hos_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Hos_similarMAF_freqx_table=read.table("Basu_Hos_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Basu_Hos_similarMAF_hwe_table=read.table("Basu_Hos_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)




#IRULAs
Basu_Irulas_freq_table=read.table("Basu_Irulas_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Irulas_freqx_table=read.table("Basu_Irulas_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Basu_Irulas_hwe_table=read.table("Basu_Irulas_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Basu_Irulas_similarMAF_freq_table=read.table("Basu_Irulas_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Irulas_similarMAF_freqx_table=read.table("Basu_Irulas_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Basu_Irulas_similarMAF_hwe_table=read.table("Basu_Irulas_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)




#IYER 
Basu_Iyers_freq_table=read.table("Basu_Iyers_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Iyers_freqx_table=read.table("Basu_Iyers_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Basu_Iyers_hwe_table=read.table("Basu_Iyers_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Basu_Iyers_similarMAF_freq_table=read.table("Basu_Iyers_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Iyers_similarMAF_freqx_table=read.table("Basu_Iyers_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Basu_Iyers_similarMAF_hwe_table=read.table("Basu_Iyers_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)




#JAMATIAs
Basu_Jamatias_freq_table=read.table("Basu_Jamatias_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Jamatias_freqx_table=read.table("Basu_Jamatias_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Basu_Jamatias_hwe_table=read.table("Basu_Jamatias_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Basu_Jamatias_similarMAF_freq_table=read.table("Basu_Jamatias_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Jamatias_similarMAF_freqx_table=read.table("Basu_Jamatias_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Basu_Jamatias_similarMAF_hwe_table=read.table("Basu_Jamatias_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)




#JARAWAS
Basu_Jarawas_freq_table=read.table("Basu_Jarawas_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Jarawas_freqx_table=read.table("Basu_Jarawas_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Basu_Jarawas_hwe_table=read.table("Basu_Jarawas_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Basu_Jarawas_similarMAF_freq_table=read.table("Basu_Jarawas_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Jarawas_similarMAF_freqx_table=read.table("Basu_Jarawas_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Basu_Jarawas_similarMAF_hwe_table=read.table("Basu_Jarawas_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)




#KADARS
Basu_Kadars_freq_table=read.table("Basu_Kadars_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Kadars_freqx_table=read.table("Basu_Kadars_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Basu_Kadars_hwe_table=read.table("Basu_Kadars_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Basu_Kadars_similarMAF_freq_table=read.table("Basu_Kadars_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Kadars_similarMAF_freqx_table=read.table("Basu_Kadars_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Basu_Kadars_similarMAF_hwe_table=read.table("Basu_Kadars_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)




#KHATRIS 
Basu_Khatris_freq_table=read.table("Basu_Khatris_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Khatris_freqx_table=read.table("Basu_Khatris_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Basu_Khatris_hwe_table=read.table("Basu_Khatris_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Basu_Khatris_similarMAF_freq_table=read.table("Basu_Khatris_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Khatris_similarMAF_freqx_table=read.table("Basu_Khatris_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Basu_Khatris_similarMAF_hwe_table=read.table("Basu_Khatris_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)



#KORWAS
Basu_Korwas_freq_table=read.table("Basu_Korwas_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Korwas_freqx_table=read.table("Basu_Korwas_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Basu_Korwas_hwe_table=read.table("Basu_Korwas_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Basu_Korwas_similarMAF_freq_table=read.table("Basu_Korwas_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Korwas_similarMAF_freqx_table=read.table("Basu_Korwas_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Basu_Korwas_similarMAF_hwe_table=read.table("Basu_Korwas_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)




#MANIPURI BRAHMINS
Basu_ManipuriBrahmins_freq_table=read.table("Basu_ManipuriBrahmins_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_ManipuriBrahmins_freqx_table=read.table("Basu_ManipuriBrahmins_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Basu_ManipuriBrahmins_hwe_table=read.table("Basu_ManipuriBrahmins_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Basu_ManipuriBrahmins_similarMAF_freq_table=read.table("Basu_ManipuriBrahmins_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_ManipuriBrahmins_similarMAF_freqx_table=read.table("Basu_ManipuriBrahmins_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Basu_ManipuriBrahmins_similarMAF_hwe_table=read.table("Basu_ManipuriBrahmins_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)




#MARATHAS
Basu_Marathas_freq_table=read.table("Basu_Marathas_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Marathas_freqx_table=read.table("Basu_Marathas_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Basu_Marathas_hwe_table=read.table("Basu_Marathas_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Basu_Marathas_similarMAF_freq_table=read.table("Basu_Marathas_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Marathas_similarMAF_freqx_table=read.table("Basu_Marathas_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Basu_Marathas_similarMAF_hwe_table=read.table("Basu_Marathas_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)




#ONGE
Basu_Onges_freq_table=read.table("Basu_Onges_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Onges_freqx_table=read.table("Basu_Onges_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Basu_Onges_hwe_table=read.table("Basu_Onges_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Basu_Onges_similarMAF_freq_table=read.table("Basu_Onges_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Onges_similarMAF_freqx_table=read.table("Basu_Onges_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Basu_Onges_similarMAF_hwe_table=read.table("Basu_Onges_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)





#PALLANS
Basu_Pallans_freq_table=read.table("Basu_Pallans_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Pallans_freqx_table=read.table("Basu_Pallans_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Basu_Pallans_hwe_table=read.table("Basu_Pallans_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Basu_Pallans_similarMAF_freq_table=read.table("Basu_Pallans_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Pallans_similarMAF_freqx_table=read.table("Basu_Pallans_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Basu_Pallans_similarMAF_hwe_table=read.table("Basu_Pallans_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)



#PANIYAS
Basu_Paniyas_freq_table=read.table("Basu_Paniyas_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Paniyas_freqx_table=read.table("Basu_Paniyas_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Basu_Paniyas_hwe_table=read.table("Basu_Paniyas_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Basu_Paniyas_similarMAF_freq_table=read.table("Basu_Paniyas_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Paniyas_similarMAF_freqx_table=read.table("Basu_Paniyas_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Basu_Paniyas_similarMAF_hwe_table=read.table("Basu_Paniyas_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)





#SantalS
Basu_Santals_freq_table=read.table("Basu_Santals_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Santals_freqx_table=read.table("Basu_Santals_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Basu_Santals_hwe_table=read.table("Basu_Santals_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Basu_Santals_similarMAF_freq_table=read.table("Basu_Santals_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Santals_similarMAF_freqx_table=read.table("Basu_Santals_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Basu_Santals_similarMAF_hwe_table=read.table("Basu_Santals_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)





#THARUS
Basu_Tharus_freq_table=read.table("Basu_Tharus_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Tharus_freqx_table=read.table("Basu_Tharus_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Basu_Tharus_hwe_table=read.table("Basu_Tharus_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Basu_Tharus_similarMAF_freq_table=read.table("Basu_Tharus_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Tharus_similarMAF_freqx_table=read.table("Basu_Tharus_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Basu_Tharus_similarMAF_hwe_table=read.table("Basu_Tharus_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)





#TRIPURI
Basu_Tripuris_freq_table=read.table("Basu_Tripuris_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Tripuris_freqx_table=read.table("Basu_Tripuris_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Basu_Tripuris_hwe_table=read.table("Basu_Tripuris_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Basu_Tripuris_similarMAF_freq_table=read.table("Basu_Tripuris_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_Tripuris_similarMAF_freqx_table=read.table("Basu_Tripuris_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Basu_Tripuris_similarMAF_hwe_table=read.table("Basu_Tripuris_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)





#WEST BENGAL BRAHMINS
Basu_WestBengalBrahmins_freq_table=read.table("Basu_WestBengalBrahmins_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_WestBengalBrahmins_freqx_table=read.table("Basu_WestBengalBrahmins_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Basu_WestBengalBrahmins_hwe_table=read.table("Basu_WestBengalBrahmins_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Basu_WestBengalBrahmins_similarMAF_freq_table=read.table("Basu_WestBengalBrahmins_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Basu_WestBengalBrahmins_similarMAF_freqx_table=read.table("Basu_WestBengalBrahmins_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Basu_WestBengalBrahmins_similarMAF_hwe_table=read.table("Basu_WestBengalBrahmins_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)




#GUJJARS 
Pathak_Gujjars_freq_table=read.table("Pathak_Gujjars_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Pathak_Gujjars_freqx_table=read.table("Pathak_Gujjars_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Pathak_Gujjars_hwe_table=read.table("Pathak_Gujjars_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Pathak_Gujjars_similarMAF_freq_table=read.table("Pathak_Gujjars_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Pathak_Gujjars_similarMAF_freqx_table=read.table("Pathak_Gujjars_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Pathak_Gujjars_similarMAF_hwe_table=read.table("Pathak_Gujjars_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)



#KAMBOJS
Pathak_Kambojs_freq_table=read.table("Pathak_Kambojs_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Pathak_Kambojs_freqx_table=read.table("Pathak_Kambojs_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Pathak_Kambojs_hwe_table=read.table("Pathak_Kambojs_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Pathak_Kambojs_similarMAF_freq_table=read.table("Pathak_Kambojs_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Pathak_Kambojs_similarMAF_freqx_table=read.table("Pathak_Kambojs_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Pathak_Kambojs_similarMAF_hwe_table=read.table("Pathak_Kambojs_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)



#RORS
Pathak_Rors_freq_table=read.table("Pathak_Rors_freq_output.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Pathak_Rors_freqx_table=read.table("Pathak_Rors_freqx_output.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE )
Pathak_Rors_hwe_table=read.table("Pathak_Rors_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)
#similar MAF
Pathak_Rors_similarMAF_freq_table=read.table("Pathak_Rors_similarMAF_freq.frq", sep="", stringsAsFactors = FALSE, header=TRUE)
Pathak_Rors_similarMAF_freqx_table=read.table("Pathak_Rors_similarMAF_freqx.frqx",  sep="\t", stringsAsFactors = FALSE, header=TRUE)
Pathak_Rors_similarMAF_hwe_table=read.table("Pathak_Rors_similarMAF_hardy.hwe", sep="", stringsAsFactors = FALSE, header=TRUE)





######COMPARE HWE values for functional SNP compared to similar SNPs in each population
##--> note that Birhors, Jarawas, Onges, Rors and Santals have MAF 0 for functional SNP and need to be excluded
#BIRHOR
BirhorHWEvalue=Basu_Birhors_hwe_table$P
BirhorHWEproportion=nrow(subset(Basu_Birhors_similarMAF_hwe_table, P<BirhorHWEvalue))/(nrow(Basu_Birhors_similarMAF_hwe_table))

#GOND
GondHWEvalue=Basu_Gonds_hwe_table$P
GondHWEproportion=nrow(subset(Basu_Gonds_similarMAF_hwe_table, P<GondHWEvalue))/(nrow(Basu_Gonds_similarMAF_hwe_table))

#GujuratiBrahminS
GujuratiBrahminHWEvalue=Basu_GujuratiBrahmins_hwe_table$P
GujuratiBrahminHWEproportion=nrow(subset(Basu_GujuratiBrahmins_similarMAF_hwe_table, P<GujuratiBrahminHWEvalue))/(nrow(Basu_GujuratiBrahmins_similarMAF_hwe_table))

#HO
HoHWEvalue=Basu_Hos_hwe_table$P
HoHWEproportion=nrow(subset(Basu_Hos_similarMAF_hwe_table, P<HoHWEvalue))/(nrow(Basu_Hos_similarMAF_hwe_table))

#IRULA
IrulaHWEvalue=Basu_Irulas_hwe_table$P
IrulaHWEproportion=nrow(subset(Basu_Irulas_similarMAF_hwe_table, P<IrulaHWEvalue))/(nrow(Basu_Irulas_similarMAF_hwe_table))

#IYER
IyerHWEvalue=Basu_Iyers_hwe_table$P
IyerHWEproportion=nrow(subset(Basu_Iyers_similarMAF_hwe_table, P<IyerHWEvalue))/(nrow(Basu_Iyers_similarMAF_hwe_table))

#JAMATIA
JamatiaHWEvalue=Basu_Jamatias_hwe_table$P
JamatiaHWEproportion=nrow(subset(Basu_Jamatias_similarMAF_hwe_table, P<JamatiaHWEvalue))/(nrow(Basu_Jamatias_similarMAF_hwe_table))

#JARAWA
JarawaHWEvalue=Basu_Jarawas_hwe_table$P
JarawaHWEproportion=nrow(subset(Basu_Jarawas_similarMAF_hwe_table, P<JarawaHWEvalue))/(nrow(Basu_Jarawas_similarMAF_hwe_table))

#KADAR
KadarHWEvalue=Basu_Kadars_hwe_table$P
KadarHWEproportion=nrow(subset(Basu_Kadars_similarMAF_hwe_table, P<KadarHWEvalue))/(nrow(Basu_Kadars_similarMAF_hwe_table))

#KHATRI
KhatriHWEvalue=Basu_Khatris_hwe_table$P
KhatriHWEproportion=nrow(subset(Basu_Khatris_similarMAF_hwe_table, P<KhatriHWEvalue))/(nrow(Basu_Khatris_similarMAF_hwe_table))

#KORWA
KorwaHWEvalue=Basu_Korwas_hwe_table$P
KorwaHWEproportion=nrow(subset(Basu_Korwas_similarMAF_hwe_table, P<KorwaHWEvalue))/(nrow(Basu_Korwas_similarMAF_hwe_table))

#MANIPURI BRAHMIN
ManipuriBrahminHWEvalue=Basu_ManipuriBrahmins_hwe_table$P
ManipuriBrahminHWEproportion=nrow(subset(Basu_ManipuriBrahmins_similarMAF_hwe_table, P<ManipuriBrahminHWEvalue))/(nrow(Basu_ManipuriBrahmins_similarMAF_hwe_table))

#MARATHA
MarathaHWEvalue=Basu_Marathas_hwe_table$P
MarathaHWEproportion=nrow(subset(Basu_Marathas_similarMAF_hwe_table, P<MarathaHWEvalue))/(nrow(Basu_Marathas_similarMAF_hwe_table))

#ONGES
OngesHWEvalue=Basu_Onges_hwe_table$P
OngesHWEproportion=nrow(subset(Basu_Onges_similarMAF_hwe_table, P<OngesHWEvalue))/(nrow(Basu_Onges_similarMAF_hwe_table))

#PALLAN
PallanHWEvalue=Basu_Pallans_hwe_table$P
PallanHWEproportion=nrow(subset(Basu_Pallans_similarMAF_hwe_table, P<PallanHWEvalue))/(nrow(Basu_Pallans_similarMAF_hwe_table))

#PANIYA 
PaniyaHWEvalue=Basu_Paniyas_hwe_table$P
PaniyaHWEproportion=nrow(subset(Basu_Paniyas_similarMAF_hwe_table, P<PaniyaHWEvalue))/(nrow(Basu_Paniyas_similarMAF_hwe_table))

#SANTAL
SantalHWEvalue=Basu_Santals_hwe_table$P
SantalHWEproportion=nrow(subset(Basu_Santals_similarMAF_hwe_table, P<SantalHWEvalue))/(nrow(Basu_Santals_similarMAF_hwe_table))

#THARUS
TharuHWEvalue=Basu_Tharus_hwe_table$P
TharuHWEproportion=nrow(subset(Basu_Tharus_similarMAF_hwe_table, P<TharuHWEvalue))/(nrow(Basu_Tharus_similarMAF_hwe_table))

#TRIPURI
TripuriHWEvalue=Basu_Tripuris_hwe_table$P
TripuriHWEproportion=nrow(subset(Basu_Tripuris_similarMAF_hwe_table, P<TripuriHWEvalue))/(nrow(Basu_Tripuris_similarMAF_hwe_table))

#WEST BENGAL BRAHMINS
WestBengalBrahminHWEvalue=Basu_WestBengalBrahmins_hwe_table$P
WestBengalBrahminHWEproportion=nrow(subset(Basu_WestBengalBrahmins_similarMAF_hwe_table, P<WestBengalBrahminHWEvalue))/(nrow(Basu_WestBengalBrahmins_similarMAF_hwe_table))

#GUJJARS
GujjarHWEvalue=Pathak_Gujjars_hwe_table$P
GujjarHWEproportion=nrow(subset(Pathak_Gujjars_similarMAF_hwe_table, P<GujjarHWEvalue))/(nrow(Pathak_Gujjars_similarMAF_hwe_table))

#KAMBOJ
KambojHWEvalue=Pathak_Kambojs_hwe_table$P
KambojHWEproportion=nrow(subset(Pathak_Kambojs_similarMAF_hwe_table, P<KambojHWEvalue))/(nrow(Pathak_Kambojs_similarMAF_hwe_table))

#ROR
RorHWEvalue=Pathak_Rors_hwe_table$P
RorHWEproportion=nrow(subset(Pathak_Rors_similarMAF_hwe_table, P<RorHWEvalue))/(nrow(Pathak_Rors_similarMAF_hwe_table))
##->Ho, Iyer, Kadar, Kamboj, Khatri, Korwa, Pallan, Paniya and Tripuri all have proportion <0.05 (and so do birhors, etc. with MAF 0 for SNP, but this is to be ignored)


#now work out excess homozygosity in functional SNP for each population 
Birhor_excesshomo=1-((Basu_Birhors_hwe_table$O.HET.)/((Basu_Birhors_hwe_table$E.HET.)))

Gond_excesshomo=1-((Basu_Gonds_hwe_table$O.HET.)/((Basu_Gonds_hwe_table$E.HET.)))

GujuratiBrahmin_excesshomo=1-((Basu_GujuratiBrahmins_hwe_table$O.HET.)/((Basu_GujuratiBrahmins_hwe_table$E.HET.)))

Ho_excesshomo=1-((Basu_Hos_hwe_table$O.HET.)/((Basu_Hos_hwe_table$E.HET.)))

Irula_excesshomo=1-((Basu_Irulas_hwe_table$O.HET.)/((Basu_Irulas_hwe_table$E.HET.)))

Iyer_excesshomo=1-((Basu_Iyers_hwe_table$O.HET.)/((Basu_Iyers_hwe_table$E.HET.)))

Jamatia_excesshomo=1-((Basu_Jamatias_hwe_table$O.HET.)/((Basu_Jamatias_hwe_table$E.HET.)))

Jarawa_excesshomo=1-((Basu_Jarawas_hwe_table$O.HET.)/((Basu_Jarawas_hwe_table$E.HET.)))

Kadar_excesshomo=1-((Basu_Kadars_hwe_table$O.HET.)/((Basu_Kadars_hwe_table$E.HET.)))

Khatri_excesshomo=1-((Basu_Khatris_hwe_table$O.HET.)/((Basu_Khatris_hwe_table$E.HET.)))

Korwa_excesshomo=1-((Basu_Korwas_hwe_table$O.HET.)/((Basu_Korwas_hwe_table$E.HET.)))

ManipuriBrahmin_excesshomo=1-((Basu_ManipuriBrahmins_hwe_table$O.HET.)/((Basu_ManipuriBrahmins_hwe_table$E.HET.)))

Maratha_excesshomo=1-((Basu_Marathas_hwe_table$O.HET.)/((Basu_Marathas_hwe_table$E.HET.)))

Onge_excesshomo=1-((Basu_Onges_hwe_table$O.HET.)/((Basu_Onges_hwe_table$E.HET.)))

Pallan_excesshomo=1-((Basu_Pallans_hwe_table$O.HET.)/((Basu_Pallans_hwe_table$E.HET.)))

Paniya_excesshomo=1-((Basu_Paniyas_hwe_table$O.HET.)/((Basu_Paniyas_hwe_table$E.HET.)))

Santal_excesshomo=1-((Basu_Santals_hwe_table$O.HET.)/((Basu_Santals_hwe_table$E.HET.)))

Tharu_excesshomo=1-((Basu_Tharus_hwe_table$O.HET.)/((Basu_Tharus_hwe_table$E.HET.)))

Tripuri_excesshomo=1-((Basu_Tripuris_hwe_table$O.HET.)/((Basu_Tripuris_hwe_table$E.HET.)))

WestBengalBrahmin_excesshomo=1-((Basu_WestBengalBrahmins_hwe_table$O.HET.)/((Basu_WestBengalBrahmins_hwe_table$E.HET.)))

Gujjar_excesshomo=1-((Pathak_Gujjars_hwe_table$O.HET.)/((Pathak_Gujjars_hwe_table$E.HET.)))

Kamboj_excesshomo=1-((Pathak_Kambojs_hwe_table$O.HET.)/((Pathak_Kambojs_hwe_table$E.HET.)))

Ror_excesshomo=1-((Pathak_Rors_hwe_table$O.HET.)/((Pathak_Rors_hwe_table$E.HET.)))
#5 populations have NAN results - leave out of analysis: Birhor, Jarawa, Onge, Ror, Santal



#now see if correlation between excess homozygosity and degree of preference for fair skin in each pop with matrimonial data 
#add excess homo results to matrimonial genetic with SNP table
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$excess_homo=NA
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$excess_homo[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujarati Brahmin"]=GujuratiBrahmin_excesshomo
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$excess_homo[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujjar"]=Gujjar_excesshomo
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$excess_homo[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Iyer"]=Iyer_excesshomo
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$excess_homo[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Kamboj"]=Kamboj_excesshomo
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$excess_homo[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Khatri"]=Khatri_excesshomo
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$excess_homo[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Maratha"]=Maratha_excesshomo
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$excess_homo[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="West Bengal Brahmin"]=WestBengalBrahmin_excesshomo

#now add comp2 average in each population to table (note this has "Doesn't Matter" as NA)
#states4 already has complexion1 and complexion2 codes:
castes30plus_only_new_andstates4$complexion1code=NA
castes30plus_only_new_andstates4$complexion2code=NA
castes30plus_only_new_andstates4$complexion1code[which(castes30plus_only_new_andstates4$complexion1=="Very Fair")]=1
castes30plus_only_new_andstates4$complexion2code[which(castes30plus_only_new_andstates4$complexion2=="Very Fair")]=1
castes30plus_only_new_andstates4$complexion1code[which(castes30plus_only_new_andstates4$complexion1=="Fair")]=2
castes30plus_only_new_andstates4$complexion2code[which(castes30plus_only_new_andstates4$complexion2=="Fair")]=2
castes30plus_only_new_andstates4$complexion1code[which(castes30plus_only_new_andstates4$complexion1=="Wheatish")]=3
castes30plus_only_new_andstates4$complexion2code[which(castes30plus_only_new_andstates4$complexion2=="Wheatish")]=3
castes30plus_only_new_andstates4$complexion1code[which(castes30plus_only_new_andstates4$complexion1=="Whetish Medium")]=4
castes30plus_only_new_andstates4$complexion2code[which(castes30plus_only_new_andstates4$complexion2=="Whetish Medium")]=4
castes30plus_only_new_andstates4$complexion1code[which(castes30plus_only_new_andstates4$complexion1=="Dark")]=5
castes30plus_only_new_andstates4$complexion2code[which(castes30plus_only_new_andstates4$complexion2=="Dark")]=5
castes30plus_only_new_andstates4$complexion1code[which(castes30plus_only_new_andstates4$complexion1=="Doesn't Matter")]=NA
castes30plus_only_new_andstates4$complexion2code[which(castes30plus_only_new_andstates4$complexion2=="Doesn't Matter")]=NA
#but run subsets again to make sure subsets have these columns too
GujuratiBrahmin=subset(castes30plus_only_new_andstates4, caste1_1=="Brahmin" & State=="Gujarat")
Gujjar=castes30plus_only_new_andstates4[grep("Gujjar", castes30plus_only_new_andstates4$caste1), ]
Iyer=castes30plus_only_new_andstates4[grep("Iyer", castes30plus_only_new_andstates4$caste1), ]
Kamboj=castes30plus_only_new_andstates4[grep("Kamboj", castes30plus_only_new_andstates4$caste1), ]
Khatri=castes30plus_only_new_andstates4[grep("Khatri", castes30plus_only_new_andstates4$caste1), ]
Maratha=castes30plus_only_new_andstates4[grep("Maratha", castes30plus_only_new_andstates4$caste1), ]
WestBengalBrahmin=subset(castes30plus_only_new_andstates4, caste1_1=="Brahmin" & State=="West Bengal")
#now create objects with the metric for average comp2 in each pop
GujuratiBrahmin_comp2_average=sum(GujuratiBrahmin$complexion2code, na.rm=TRUE)/nrow(GujuratiBrahmin[!is.na(GujuratiBrahmin$complexion2code)])
Gujjar_comp2_average=sum(Gujjar$complexion2code, na.rm=TRUE)/nrow(Gujjar[!is.na(Gujjar$complexion2code)])
Iyer_comp2_average=sum(Iyer$complexion2code, na.rm=TRUE)/nrow(Iyer[!is.na(Iyer$complexion2code)])
Kamboj_comp2_average=sum(Kamboj$complexion2code, na.rm=TRUE)/nrow(Kamboj[!is.na(Kamboj$complexion2code)])
Khatri_comp2_average=sum(Khatri$complexion2code, na.rm=TRUE)/nrow(Khatri[!is.na(Khatri$complexion2code)])
Maratha_comp2_average=sum(Maratha$complexion2code, na.rm=TRUE)/nrow(Maratha[!is.na(Maratha$complexion2code)])
WestBengalBrahmin_comp2_average=sum(WestBengalBrahmin$complexion2code, na.rm=TRUE)/nrow(WestBengalBrahmin[!is.na(WestBengalBrahmin$complexion2code)])
#now add average_comp2 to matrimonialwithSNP table 
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_average=NA
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_average[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujarati Brahmin"]=GujuratiBrahmin_comp2_average
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_average[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujjar"]=Gujjar_comp2_average
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_average[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Iyer"]=Iyer_comp2_average
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_average[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Kamboj"]=Kamboj_comp2_average
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_average[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Khatri"]=Khatri_comp2_average
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_average[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Maratha"]=Maratha_comp2_average
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_average[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="West Bengal Brahmin"]=WestBengalBrahmin_comp2_average


####see if correlation significant if doesnt matter included as answer, but counts as 0
GujuratiBrahmin_comp2_average_new=sum(GujuratiBrahmin$complexion2code, na.rm=TRUE)/nrow(GujuratiBrahmin[!is.na(GujuratiBrahmin$complexion2)])
Gujjar_comp2_average_new=sum(Gujjar$complexion2code, na.rm=TRUE)/nrow(Gujjar[!is.na(Gujjar$complexion2)])
Iyer_comp2_average_new=sum(Iyer$complexion2code, na.rm=TRUE)/nrow(Iyer[!is.na(Iyer$complexion2)])
Kamboj_comp2_average_new=sum(Kamboj$complexion2code, na.rm=TRUE)/nrow(Kamboj[!is.na(Kamboj$complexion2)])
Khatri_comp2_average_new=sum(Khatri$complexion2code, na.rm=TRUE)/nrow(Khatri[!is.na(Khatri$complexion2)])
Maratha_comp2_average_new=sum(Maratha$complexion2code, na.rm=TRUE)/nrow(Maratha[!is.na(Maratha$complexion2)])
WestBengalBrahmin_comp2_average_new=sum(WestBengalBrahmin$complexion2code, na.rm=TRUE)/nrow(WestBengalBrahmin[!is.na(WestBengalBrahmin$complexion2)])
#now add average_comp2 to matrimonialwithSNP table 
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_average_new=NA
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_average_new[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujarati Brahmin"]=GujuratiBrahmin_comp2_average_new
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_average_new[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujjar"]=Gujjar_comp2_average_new
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_average_new[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Iyer"]=Iyer_comp2_average_new
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_average_new[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Kamboj"]=Kamboj_comp2_average_new
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_average_new[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Khatri"]=Khatri_comp2_average_new
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_average_new[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Maratha"]=Maratha_comp2_average_new
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_average_new[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="West Bengal Brahmin"]=WestBengalBrahmin_comp2_average_new


#add proportion of caste wanting light or very light spouse to matrimonial table for correlations with excess_homo
#first make object with this proportion for each pop
GujuratiBrahmin_comp2_fairORvfair_prop=nrow(subset(GujuratiBrahmin, complexion2code==1|complexion2code==2))/nrow(GujuratiBrahmin[!is.na(GujuratiBrahmin$complexion2code)])
Gujjar_comp2_fairORvfair_prop=nrow(subset(Gujjar, complexion2code==1|complexion2code==2))/nrow(Gujjar[!is.na(Gujjar$complexion2code)])
Iyer_comp2_fairORvfair_prop=nrow(subset(Iyer, complexion2code==1|complexion2code==2))/nrow(Iyer[!is.na(Iyer$complexion2code)])
Kamboj_comp2_fairORvfair_prop=nrow(subset(Kamboj, complexion2code==1|complexion2code==2))/nrow(Kamboj[!is.na(Kamboj$complexion2code)])
Khatri_comp2_fairORvfair_prop=nrow(subset(Khatri, complexion2code==1|complexion2code==2))/nrow(Khatri[!is.na(Khatri$complexion2code)])
Maratha_comp2_fairORvfair_prop=nrow(subset(Maratha, complexion2code==1|complexion2code==2))/nrow(Maratha[!is.na(Maratha$complexion2code)])
WestBengalBrahmin_comp2_fairORvfair_prop=nrow(subset(WestBengalBrahmin, complexion2code==1|complexion2code==2))/nrow(WestBengalBrahmin[!is.na(WestBengalBrahmin$complexion2code)])
#now add pops to matrimonial with SNP pop
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_fairORvfair_prop=NA
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_fairORvfair_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujarati Brahmin"]=GujuratiBrahmin_comp2_fairORvfair_prop
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_fairORvfair_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujjar"]=Gujjar_comp2_fairORvfair_prop
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_fairORvfair_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Iyer"]=Iyer_comp2_fairORvfair_prop
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_fairORvfair_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Kamboj"]=Kamboj_comp2_fairORvfair_prop
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_fairORvfair_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Khatri"]=Khatri_comp2_fairORvfair_prop
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_fairORvfair_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Maratha"]=Maratha_comp2_fairORvfair_prop
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_fairORvfair_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="West Bengal Brahmin"]=WestBengalBrahmin_comp2_fairORvfair_prop


#see if correlation signicant if we include "Doesn't Matter" individuals
#change coding in main dataframe
castes30plus_only_new_andstates4$complexion1code[which(castes30plus_only_new_andstates4$complexion1=="Doesn't Matter")]=6
castes30plus_only_new_andstates4$complexion2code[which(castes30plus_only_new_andstates4$complexion2=="Doesn't Matter")]=6
#run subsets again to have new code 
GujuratiBrahmin=subset(castes30plus_only_new_andstates4, caste1_1=="Brahmin" & State=="Gujarat")
Gujjar=castes30plus_only_new_andstates4[grep("Gujjar", castes30plus_only_new_andstates4$caste1), ]
Iyer=castes30plus_only_new_andstates4[grep("Iyer", castes30plus_only_new_andstates4$caste1), ]
Kamboj=castes30plus_only_new_andstates4[grep("Kamboj", castes30plus_only_new_andstates4$caste1), ]
Khatri=castes30plus_only_new_andstates4[grep("Khatri", castes30plus_only_new_andstates4$caste1), ]
Maratha=castes30plus_only_new_andstates4[grep("Maratha", castes30plus_only_new_andstates4$caste1), ]
WestBengalBrahmin=subset(castes30plus_only_new_andstates4, caste1_1=="Brahmin" & State=="West Bengal")
#now make new object for new proportions with change of code, where doesn't matter =6
GujuratiBrahmin_comp2_fairORvfair_prop_new=nrow(subset(GujuratiBrahmin, complexion2code==1|complexion2code==2))/nrow(GujuratiBrahmin[!is.na(GujuratiBrahmin$complexion2code)])
Gujjar_comp2_fairORvfair_prop_new=nrow(subset(Gujjar, complexion2code==1|complexion2code==2))/nrow(Gujjar[!is.na(Gujjar$complexion2code)])
Iyer_comp2_fairORvfair_prop_new=nrow(subset(Iyer, complexion2code==1|complexion2code==2))/nrow(Iyer[!is.na(Iyer$complexion2code)])
Kamboj_comp2_fairORvfair_prop_new=nrow(subset(Kamboj, complexion2code==1|complexion2code==2))/nrow(Kamboj[!is.na(Kamboj$complexion2code)])
Khatri_comp2_fairORvfair_prop_new=nrow(subset(Khatri, complexion2code==1|complexion2code==2))/nrow(Khatri[!is.na(Khatri$complexion2code)])
Maratha_comp2_fairORvfair_prop_new=nrow(subset(Maratha, complexion2code==1|complexion2code==2))/nrow(Maratha[!is.na(Maratha$complexion2code)])
WestBengalBrahmin_comp2_fairORvfair_prop_new=nrow(subset(WestBengalBrahmin, complexion2code==1|complexion2code==2))/nrow(WestBengalBrahmin[!is.na(WestBengalBrahmin$complexion2code)])
#now add pops to matrimonial with SNP pop
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_fairORvfair_prop_new=NA
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_fairORvfair_prop_new[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujarati Brahmin"]=GujuratiBrahmin_comp2_fairORvfair_prop_new
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_fairORvfair_prop_new[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujjar"]=Gujjar_comp2_fairORvfair_prop_new
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_fairORvfair_prop_new[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Iyer"]=Iyer_comp2_fairORvfair_prop_new
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_fairORvfair_prop_new[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Kamboj"]=Kamboj_comp2_fairORvfair_prop_new
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_fairORvfair_prop_new[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Khatri"]=Khatri_comp2_fairORvfair_prop_new
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_fairORvfair_prop_new[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Maratha"]=Maratha_comp2_fairORvfair_prop_new
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp2_fairORvfair_prop_new[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="West Bengal Brahmin"]=WestBengalBrahmin_comp2_fairORvfair_prop_new


#find degree of assortative mating within each caste to determine correlation of assortative mating and excess homozygosity
#reset complexion codes to have "doesn't matter" as NA
castes30plus_only_new_andstates4$complexion1code[which(castes30plus_only_new_andstates4$complexion1=="Doesn't Matter")]=NA
castes30plus_only_new_andstates4$complexion2code[which(castes30plus_only_new_andstates4$complexion2=="Doesn't Matter")]=NA
#run subsets again to have new doesnt matter set to NA 
GujuratiBrahmin=subset(castes30plus_only_new_andstates4, caste1_1=="Brahmin" & State=="Gujarat")
Gujjar=castes30plus_only_new_andstates4[grep("Gujjar", castes30plus_only_new_andstates4$caste1), ]
Iyer=castes30plus_only_new_andstates4[grep("Iyer", castes30plus_only_new_andstates4$caste1), ]
Kamboj=castes30plus_only_new_andstates4[grep("Kamboj", castes30plus_only_new_andstates4$caste1), ]
Khatri=castes30plus_only_new_andstates4[grep("Khatri", castes30plus_only_new_andstates4$caste1), ]
Maratha=castes30plus_only_new_andstates4[grep("Maratha", castes30plus_only_new_andstates4$caste1), ]
WestBengalBrahmin=subset(castes30plus_only_new_andstates4, caste1_1=="Brahmin" & State=="West Bengal")
#find proportion of each caste who want exact assortative mating by skin complexion 1 and 2
#first need to make subset of individuals who gave answers for both 
GujuratiBrahmin_comp1and2=GujuratiBrahmin[!is.na(GujuratiBrahmin$complexion1code) & !is.na(GujuratiBrahmin$complexion2code)]
Gujjar_comp1and2=Gujjar[!is.na(Gujjar$complexion1code) & !is.na(Gujjar$complexion2code)]
Iyer_comp1and2=Iyer[!is.na(Iyer$complexion1code) & !is.na(Iyer$complexion2code)]
Kamboj_comp1and2=Kamboj[!is.na(Kamboj$complexion1code) & !is.na(Kamboj$complexion2code)]
Khatri_comp1and2=Khatri[!is.na(Khatri$complexion1code) & !is.na(Khatri$complexion2code)]
Maratha_comp1and2=Maratha[!is.na(Maratha$complexion1code) & !is.na(Maratha$complexion2code)]
WestBengalBrahmin_comp1and2=WestBengalBrahmin[!is.na(WestBengalBrahmin$complexion1code) & !is.na(WestBengalBrahmin$complexion2code)]
#now make object for prop of assortative mating in each caste
GujuratiBrahmin_comp_assortative_prop=nrow(subset(GujuratiBrahmin_comp1and2, complexion1code==complexion2code))/nrow(GujuratiBrahmin_comp1and2)
Gujjar_comp_assortative_prop=nrow(subset(Gujjar_comp1and2, complexion1code==complexion2code))/nrow(Gujjar_comp1and2)
Iyer_comp_assortative_prop=nrow(subset(Iyer_comp1and2, complexion1code==complexion2code))/nrow(Iyer_comp1and2)
Kamboj_comp_assortative_prop=nrow(subset(Kamboj_comp1and2, complexion1code==complexion2code))/nrow(Kamboj_comp1and2)
Khatri_comp_assortative_prop=nrow(subset(Khatri_comp1and2, complexion1code==complexion2code))/nrow(Khatri_comp1and2)
Maratha_comp_assortative_prop=nrow(subset(Maratha_comp1and2, complexion1code==complexion2code))/nrow(Maratha_comp1and2)
WestBengalBrahmin_comp_assortative_prop=nrow(subset(WestBengalBrahmin_comp1and2, complexion1code==complexion2code))/nrow(WestBengalBrahmin_comp1and2)
#add comp assortative mating to matrimonial table 
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_prop=NA
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujarati Brahmin"]=GujuratiBrahmin_comp_assortative_prop
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujjar"]=Gujjar_comp_assortative_prop
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Iyer"]=Iyer_comp_assortative_prop
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Kamboj"]=Kamboj_comp_assortative_prop
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Khatri"]=Khatri_comp_assortative_prop
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Maratha"]=Maratha_comp_assortative_prop
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="West Bengal Brahmin"]=WestBengalBrahmin_comp_assortative_prop


#try to see if correlation of assortative mating includes within 1 step either side 
GujuratiBrahmin_comp_assortative_prop_new=nrow(subset(GujuratiBrahmin_comp1and2, (complexion1code-1==complexion2code)|(complexion1code+1==complexion2code)|(complexion1code==complexion2code)))/nrow(GujuratiBrahmin_comp1and2)
Gujjar_comp_assortative_prop_new=nrow(subset(Gujjar_comp1and2, (complexion1code-1==complexion2code)|(complexion1code+1==complexion2code)|(complexion1code==complexion2code)))/nrow(Gujjar_comp1and2)
Iyer_comp_assortative_prop_new=nrow(subset(Iyer_comp1and2, (complexion1code-1==complexion2code)|(complexion1code+1==complexion2code)|(complexion1code==complexion2code)))/nrow(Iyer_comp1and2)
Kamboj_comp_assortative_prop_new=nrow(subset(Kamboj_comp1and2, (complexion1code-1==complexion2code)|(complexion1code+1==complexion2code)|(complexion1code==complexion2code)))/nrow(Kamboj_comp1and2)
Khatri_comp_assortative_prop_new=nrow(subset(Khatri_comp1and2, (complexion1code-1==complexion2code)|(complexion1code+1==complexion2code)|(complexion1code==complexion2code)))/nrow(Khatri_comp1and2)
Maratha_comp_assortative_prop_new=nrow(subset(Maratha_comp1and2, (complexion1code-1==complexion2code)|(complexion1code+1==complexion2code)|(complexion1code==complexion2code)))/nrow(Maratha_comp1and2)
WestBengalBrahmin_comp_assortative_prop_new=nrow(subset(WestBengalBrahmin_comp1and2, (complexion1code-1==complexion2code)|(complexion1code+1==complexion2code)|(complexion1code==complexion2code)))/nrow(WestBengalBrahmin_comp1and2)
#add assortative new props to matrimonial table 
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1eitherside_prop=NA
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1eitherside_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujarati Brahmin"]=GujuratiBrahmin_comp_assortative_prop_new
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1eitherside_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujjar"]=Gujjar_comp_assortative_prop_new
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1eitherside_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Iyer"]=Iyer_comp_assortative_prop_new
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1eitherside_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Kamboj"]=Kamboj_comp_assortative_prop_new
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1eitherside_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Khatri"]=Khatri_comp_assortative_prop_new
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1eitherside_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Maratha"]=Maratha_comp_assortative_prop_new
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1eitherside_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="West Bengal Brahmin"]=WestBengalBrahmin_comp_assortative_prop_new

#see if correlation when assortative when allow one step below as well as own
GujuratiBrahmin_comp_assortative_prop_new2=nrow(subset(GujuratiBrahmin_comp1and2, (complexion1code-1==complexion2code)|(complexion1code==complexion2code)))/nrow(GujuratiBrahmin_comp1and2)
Gujjar_comp_assortative_prop_new2=nrow(subset(Gujjar_comp1and2, (complexion1code-1==complexion2code)|(complexion1code==complexion2code)))/nrow(Gujjar_comp1and2)
Iyer_comp_assortative_prop_new2=nrow(subset(Iyer_comp1and2, (complexion1code-1==complexion2code)|(complexion1code==complexion2code)))/nrow(Iyer_comp1and2)
Kamboj_comp_assortative_prop_new2=nrow(subset(Kamboj_comp1and2, (complexion1code-1==complexion2code)|(complexion1code==complexion2code)))/nrow(Kamboj_comp1and2)
Khatri_comp_assortative_prop_new2=nrow(subset(Khatri_comp1and2, (complexion1code-1==complexion2code)|(complexion1code==complexion2code)))/nrow(Khatri_comp1and2)
Maratha_comp_assortative_prop_new2=nrow(subset(Maratha_comp1and2, (complexion1code-1==complexion2code)|(complexion1code==complexion2code)))/nrow(Maratha_comp1and2)
WestBengalBrahmin_comp_assortative_prop_new2=nrow(subset(WestBengalBrahmin_comp1and2, (complexion1code-1==complexion2code)|(complexion1code==complexion2code)))/nrow(WestBengalBrahmin_comp1and2)
#add assortative new props to matrimonial table 
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1below_prop=NA
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1below_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujarati Brahmin"]=GujuratiBrahmin_comp_assortative_prop_new2
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1below_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujjar"]=Gujjar_comp_assortative_prop_new2
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1below_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Iyer"]=Iyer_comp_assortative_prop_new2
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1below_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Kamboj"]=Kamboj_comp_assortative_prop_new2
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1below_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Khatri"]=Khatri_comp_assortative_prop_new2
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1below_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Maratha"]=Maratha_comp_assortative_prop_new2
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1below_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="West Bengal Brahmin"]=WestBengalBrahmin_comp_assortative_prop_new2


#see if correlation when assortative when allow one step above as well as own
GujuratiBrahmin_comp_assortative_prop_new3=nrow(subset(GujuratiBrahmin_comp1and2, (complexion1code+1==complexion2code)|(complexion1code==complexion2code)))/nrow(GujuratiBrahmin_comp1and2)
Gujjar_comp_assortative_prop_new3=nrow(subset(Gujjar_comp1and2, (complexion1code+1==complexion2code)|(complexion1code==complexion2code)))/nrow(Gujjar_comp1and2)
Iyer_comp_assortative_prop_new3=nrow(subset(Iyer_comp1and2, (complexion1code+1==complexion2code)|(complexion1code==complexion2code)))/nrow(Iyer_comp1and2)
Kamboj_comp_assortative_prop_new3=nrow(subset(Kamboj_comp1and2, (complexion1code+1==complexion2code)|(complexion1code==complexion2code)))/nrow(Kamboj_comp1and2)
Khatri_comp_assortative_prop_new3=nrow(subset(Khatri_comp1and2, (complexion1code+1==complexion2code)|(complexion1code==complexion2code)))/nrow(Khatri_comp1and2)
Maratha_comp_assortative_prop_new3=nrow(subset(Maratha_comp1and2, (complexion1code+1==complexion2code)|(complexion1code==complexion2code)))/nrow(Maratha_comp1and2)
WestBengalBrahmin_comp_assortative_prop_new3=nrow(subset(WestBengalBrahmin_comp1and2, (complexion1code+1==complexion2code)|(complexion1code==complexion2code)))/nrow(WestBengalBrahmin_comp1and2)
#add assortative new props to matrimonial table 
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1above_prop=NA
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1above_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujarati Brahmin"]=GujuratiBrahmin_comp_assortative_prop_new3
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1above_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujjar"]=Gujjar_comp_assortative_prop_new3
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1above_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Iyer"]=Iyer_comp_assortative_prop_new3
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1above_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Kamboj"]=Kamboj_comp_assortative_prop_new3
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1above_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Khatri"]=Khatri_comp_assortative_prop_new3
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1above_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Maratha"]=Maratha_comp_assortative_prop_new3
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_1above_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="West Bengal Brahmin"]=WestBengalBrahmin_comp_assortative_prop_new3


#see if correlation when preference is lighter than oneself 
GujuratiBrahmin_comp_assortative_prop_new4=nrow(subset(GujuratiBrahmin_comp1and2, (complexion1code>complexion2code)))/nrow(GujuratiBrahmin_comp1and2)
Gujjar_comp_assortative_prop_new4=nrow(subset(Gujjar_comp1and2, (complexion1code>complexion2code)))/nrow(Gujjar_comp1and2)
Iyer_comp_assortative_prop_new4=nrow(subset(Iyer_comp1and2, (complexion1code>complexion2code)))/nrow(Iyer_comp1and2)
Kamboj_comp_assortative_prop_new4=nrow(subset(Kamboj_comp1and2, (complexion1code>complexion2code)))/nrow(Kamboj_comp1and2)
Khatri_comp_assortative_prop_new4=nrow(subset(Khatri_comp1and2, (complexion1code>complexion2code)))/nrow(Khatri_comp1and2)
Maratha_comp_assortative_prop_new4=nrow(subset(Maratha_comp1and2, (complexion1code>complexion2code)))/nrow(Maratha_comp1and2)
WestBengalBrahmin_comp_assortative_prop_new4=nrow(subset(WestBengalBrahmin_comp1and2, (complexion1code>complexion2code)))/nrow(WestBengalBrahmin_comp1and2)
#add assortative new props to matrimonial table 
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_belowONLY_prop=NA
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_belowONLY_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujarati Brahmin"]=GujuratiBrahmin_comp_assortative_prop_new4
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_belowONLY_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujjar"]=Gujjar_comp_assortative_prop_new4
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_belowONLY_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Iyer"]=Iyer_comp_assortative_prop_new4
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_belowONLY_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Kamboj"]=Kamboj_comp_assortative_prop_new4
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_belowONLY_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Khatri"]=Khatri_comp_assortative_prop_new4
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_belowONLY_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Maratha"]=Maratha_comp_assortative_prop_new4
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_belowONLY_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="West Bengal Brahmin"]=WestBengalBrahmin_comp_assortative_prop_new4

#check if correlation when preference is darker than oneself 
GujuratiBrahmin_comp_assortative_prop_new5=nrow(subset(GujuratiBrahmin_comp1and2, (complexion1code<complexion2code)))/nrow(GujuratiBrahmin_comp1and2)
Gujjar_comp_assortative_prop_new5=nrow(subset(Gujjar_comp1and2, (complexion1code<complexion2code)))/nrow(Gujjar_comp1and2)
Iyer_comp_assortative_prop_new5=nrow(subset(Iyer_comp1and2, (complexion1code<complexion2code)))/nrow(Iyer_comp1and2)
Kamboj_comp_assortative_prop_new5=nrow(subset(Kamboj_comp1and2, (complexion1code<complexion2code)))/nrow(Kamboj_comp1and2)
Khatri_comp_assortative_prop_new5=nrow(subset(Khatri_comp1and2, (complexion1code<complexion2code)))/nrow(Khatri_comp1and2)
Maratha_comp_assortative_prop_new5=nrow(subset(Maratha_comp1and2, (complexion1code<complexion2code)))/nrow(Maratha_comp1and2)
WestBengalBrahmin_comp_assortative_prop_new5=nrow(subset(WestBengalBrahmin_comp1and2, (complexion1code<complexion2code)))/nrow(WestBengalBrahmin_comp1and2)
#add assortative new props to matrimonial table 
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_aboveONLY_prop=NA
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_aboveONLY_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujarati Brahmin"]=GujuratiBrahmin_comp_assortative_prop_new5
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_aboveONLY_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujjar"]=Gujjar_comp_assortative_prop_new5
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_aboveONLY_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Iyer"]=Iyer_comp_assortative_prop_new5
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_aboveONLY_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Kamboj"]=Kamboj_comp_assortative_prop_new5
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_aboveONLY_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Khatri"]=Khatri_comp_assortative_prop_new5
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_aboveONLY_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Maratha"]=Maratha_comp_assortative_prop_new5
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp_assortative_aboveONLY_prop[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="West Bengal Brahmin"]=WestBengalBrahmin_comp_assortative_prop_new5




#correlations with allele freq
#first create object of allele freq in each population - need to be careful as MA is sometimes A, sometimes G
if(Basu_GujuratiBrahmins_freq_table$A1=="G"){
  GujuratiBrahmin_allelefreq <- 1-Basu_GujuratiBrahmins_freq_table$MAF
} else if(Basu_GujuratiBrahmins_freq_table$A1=="A"){
  GujuratiBrahmin_allelefreq <- Basu_GujuratiBrahmins_freq_table$MAF
}


if(Pathak_Gujjars_freq_table$A1=="G"){
  Gujjar_allelefreq <- 1-Pathak_Gujjars_freq_table$MAF
} else if(Pathak_Gujjars_freq_table$A1=="A"){
  Gujjar_allelefreq <- Pathak_Gujjars_freq_table$MAF
}


if(Basu_Iyers_freq_table$A1=="G"){
  Iyer_allelefreq <- 1-Basu_Iyers_freq_table$MAF
} else if(Basu_Iyers_freq_table$A1=="A"){
  Iyer_allelefreq <- Basu_Iyers_freq_table$MAF
}

if(Pathak_Kambojs_freq_table$A1=="G"){
  Kamboj_allelefreq <- 1-Pathak_Kambojs_freq_table$MAF
} else if(Pathak_Kambojs_freq_table$A1=="A"){
  Kamboj_allelefreq <- Pathak_Kambojs_freq_table$MAF
}

if(Basu_Khatris_freq_table$A1=="G"){
  Khatri_allelefreq <- 1-Basu_Khatris_freq_table$MAF
} else if(Basu_Khatris_freq_table$A1=="A"){
  Khatri_allelefreq <- Basu_Khatris_freq_table$MAF
}

if(Basu_Marathas_freq_table$A1=="G"){
  Maratha_allelefreq <- 1-Basu_Marathas_freq_table$MAF
} else if(Basu_Marathas_freq_table$A1=="A"){
  Maratha_allelefreq <- Basu_Marathas_freq_table$MAF
}

if(Basu_WestBengalBrahmins_freq_table$A1=="G"){
  WestBengalBrahmin_allelefreq <- 1-Basu_WestBengalBrahmins_freq_table$MAF
} else if(Basu_WestBengalBrahmins_freq_table$A1=="A"){
  WestBengalBrahmin_allelefreq <- Basu_WestBengalBrahmins_freq_table$MAF
}

#now add allele freq to matrimonial table
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$allelefreq=NA
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$allelefreq[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujarati Brahmin"]=GujuratiBrahmin_allelefreq
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$allelefreq[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujjar"]=Gujjar_allelefreq
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$allelefreq[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Iyer"]=Iyer_allelefreq
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$allelefreq[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Kamboj"]=Kamboj_allelefreq
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$allelefreq[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Khatri"]=Khatri_allelefreq
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$allelefreq[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Maratha"]=Maratha_allelefreq
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$allelefreq[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="West Bengal Brahmin"]=WestBengalBrahmin_allelefreq


#now create objects with the metric for average comp1 in each pop
GujuratiBrahmin_comp1_average=sum(GujuratiBrahmin$complexion1code, na.rm=TRUE)/nrow(GujuratiBrahmin[!is.na(GujuratiBrahmin$complexion1code)])
Gujjar_comp1_average=sum(Gujjar$complexion1code, na.rm=TRUE)/nrow(Gujjar[!is.na(Gujjar$complexion1code)])
Iyer_comp1_average=sum(Iyer$complexion1code, na.rm=TRUE)/nrow(Iyer[!is.na(Iyer$complexion1code)])
Kamboj_comp1_average=sum(Kamboj$complexion1code, na.rm=TRUE)/nrow(Kamboj[!is.na(Kamboj$complexion1code)])
Khatri_comp1_average=sum(Khatri$complexion1code, na.rm=TRUE)/nrow(Khatri[!is.na(Khatri$complexion1code)])
Maratha_comp1_average=sum(Maratha$complexion1code, na.rm=TRUE)/nrow(Maratha[!is.na(Maratha$complexion1code)])
WestBengalBrahmin_comp1_average=sum(WestBengalBrahmin$complexion1code, na.rm=TRUE)/nrow(WestBengalBrahmin[!is.na(WestBengalBrahmin$complexion1code)])
#now add average_comp1 to matrimonialwithSNP table 
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp1_average=NA
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp1_average[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujarati Brahmin"]=GujuratiBrahmin_comp1_average
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp1_average[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujjar"]=Gujjar_comp1_average
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp1_average[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Iyer"]=Iyer_comp1_average
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp1_average[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Kamboj"]=Kamboj_comp1_average
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp1_average[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Khatri"]=Khatri_comp1_average
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp1_average[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Maratha"]=Maratha_comp1_average
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$comp1_average[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="West Bengal Brahmin"]=WestBengalBrahmin_comp1_average


#add HWE values to matrimonial table
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$HWE_pvalue=NA
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$HWE_pvalue[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujarati Brahmin"]=GujuratiBrahminHWEvalue
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$HWE_pvalue[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujjar"]=GujjarHWEvalue
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$HWE_pvalue[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Iyer"]=IyerHWEvalue
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$HWE_pvalue[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Kamboj"]=KambojHWEvalue
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$HWE_pvalue[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Khatri"]=KhatriHWEvalue
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$HWE_pvalue[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Maratha"]=MarathaHWEvalue
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$HWE_pvalue[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="West Bengal Brahmin"]=WestBengalBrahminHWEvalue

#add HWEproportion values to table 
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$HWEproportion=NA
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$HWEproportion[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujarati Brahmin"]=GujuratiBrahminHWEproportion
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$HWEproportion[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujjar"]=GujjarHWEproportion
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$HWEproportion[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Iyer"]=IyerHWEproportion
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$HWEproportion[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Kamboj"]=KambojHWEproportion
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$HWEproportion[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Khatri"]=KhatriHWEproportion
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$HWEproportion[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Maratha"]=MarathaHWEproportion
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$HWEproportion[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="West Bengal Brahmin"]=WestBengalBrahminHWEproportion







#compare F value deviation in SNP compared to other SNPs to account for excess homozygosity across the genome by creating Z score
#first make column in similarMAF HWE tables in each population for F score 
Basu_Birhors_similarMAF_hwe_table$F_score=1-(ifelse(Basu_Birhors_similarMAF_hwe_table$O.HET.==0, 0, Basu_Birhors_similarMAF_hwe_table$O.HET./Basu_Birhors_similarMAF_hwe_table$E.HET.))
Basu_Gonds_similarMAF_hwe_table$F_score=1-(ifelse(Basu_Gonds_similarMAF_hwe_table$O.HET.==0, 0, Basu_Gonds_similarMAF_hwe_table$O.HET./Basu_Gonds_similarMAF_hwe_table$E.HET.))
Basu_GujuratiBrahmins_similarMAF_hwe_table$F_score=1-(ifelse(Basu_GujuratiBrahmins_similarMAF_hwe_table$O.HET.==0, 0, Basu_GujuratiBrahmins_similarMAF_hwe_table$O.HET./Basu_GujuratiBrahmins_similarMAF_hwe_table$E.HET.))
Basu_Hos_similarMAF_hwe_table$F_score=1-(ifelse(Basu_Hos_similarMAF_hwe_table$O.HET.==0, 0, Basu_Hos_similarMAF_hwe_table$O.HET./Basu_Hos_similarMAF_hwe_table$E.HET.))
Basu_Irulas_similarMAF_hwe_table$F_score=1-(ifelse(Basu_Irulas_similarMAF_hwe_table$O.HET.==0, 0, Basu_Irulas_similarMAF_hwe_table$O.HET./Basu_Irulas_similarMAF_hwe_table$E.HET.))
Basu_Iyers_similarMAF_hwe_table$F_score=1-(ifelse(Basu_Iyers_similarMAF_hwe_table$O.HET.==0, 0, Basu_Iyers_similarMAF_hwe_table$O.HET./Basu_Iyers_similarMAF_hwe_table$E.HET.))
Basu_Irulas_similarMAF_hwe_table$F_score=1-(ifelse(Basu_Irulas_similarMAF_hwe_table$O.HET.==0, 0, Basu_Irulas_similarMAF_hwe_table$O.HET./Basu_Irulas_similarMAF_hwe_table$E.HET.))
Basu_Jamatias_similarMAF_hwe_table$F_score=1-(ifelse(Basu_Jamatias_similarMAF_hwe_table$O.HET.==0, 0, Basu_Jamatias_similarMAF_hwe_table$O.HET./Basu_Jamatias_similarMAF_hwe_table$E.HET.))
Basu_Jarawas_similarMAF_hwe_table$F_score=1-(ifelse(Basu_Jarawas_similarMAF_hwe_table$O.HET.==0, 0, Basu_Jarawas_similarMAF_hwe_table$O.HET./Basu_Jarawas_similarMAF_hwe_table$E.HET.))
Basu_Kadars_similarMAF_hwe_table$F_score=1-(ifelse(Basu_Kadars_similarMAF_hwe_table$O.HET.==0, 0, Basu_Kadars_similarMAF_hwe_table$O.HET./Basu_Kadars_similarMAF_hwe_table$E.HET.))
Basu_Khatris_similarMAF_hwe_table$F_score=1-(ifelse(Basu_Khatris_similarMAF_hwe_table$O.HET.==0, 0, Basu_Khatris_similarMAF_hwe_table$O.HET./Basu_Khatris_similarMAF_hwe_table$E.HET.))
Basu_Korwas_similarMAF_hwe_table$F_score=1-(ifelse(Basu_Korwas_similarMAF_hwe_table$O.HET.==0, 0, Basu_Korwas_similarMAF_hwe_table$O.HET./Basu_Korwas_similarMAF_hwe_table$E.HET.))
Basu_ManipuriBrahmins_similarMAF_hwe_table$F_score=1-(ifelse(Basu_ManipuriBrahmins_similarMAF_hwe_table$O.HET.==0, 0, Basu_ManipuriBrahmins_similarMAF_hwe_table$O.HET./Basu_ManipuriBrahmins_similarMAF_hwe_table$E.HET.))
Basu_Marathas_similarMAF_hwe_table$F_score=1-(ifelse(Basu_Marathas_similarMAF_hwe_table$O.HET.==0, 0, Basu_Marathas_similarMAF_hwe_table$O.HET./Basu_Marathas_similarMAF_hwe_table$E.HET.))
Basu_Onges_similarMAF_hwe_table$F_score=1-(ifelse(Basu_Onges_similarMAF_hwe_table$O.HET.==0, 0, Basu_Onges_similarMAF_hwe_table$O.HET./Basu_Onges_similarMAF_hwe_table$E.HET.))
Basu_Pallans_similarMAF_hwe_table$F_score=1-(ifelse(Basu_Pallans_similarMAF_hwe_table$O.HET.==0, 0, Basu_Pallans_similarMAF_hwe_table$O.HET./Basu_Pallans_similarMAF_hwe_table$E.HET.))
Basu_Paniyas_similarMAF_hwe_table$F_score=1-(ifelse(Basu_Paniyas_similarMAF_hwe_table$O.HET.==0, 0, Basu_Paniyas_similarMAF_hwe_table$O.HET./Basu_Paniyas_similarMAF_hwe_table$E.HET.))
Basu_Santals_similarMAF_hwe_table$F_score=1-(ifelse(Basu_Santals_similarMAF_hwe_table$O.HET.==0, 0, Basu_Santals_similarMAF_hwe_table$O.HET./Basu_Santals_similarMAF_hwe_table$E.HET.))
Basu_Tharus_similarMAF_hwe_table$F_score=1-(ifelse(Basu_Tharus_similarMAF_hwe_table$O.HET.==0, 0, Basu_Tharus_similarMAF_hwe_table$O.HET./Basu_Tharus_similarMAF_hwe_table$E.HET.))
Basu_Tripuris_similarMAF_hwe_table$F_score=1-(ifelse(Basu_Tripuris_similarMAF_hwe_table$O.HET.==0, 0, Basu_Tripuris_similarMAF_hwe_table$O.HET./Basu_Tripuris_similarMAF_hwe_table$E.HET.))
Basu_WestBengalBrahmins_similarMAF_hwe_table$F_score=1-(ifelse(Basu_WestBengalBrahmins_similarMAF_hwe_table$O.HET.==0, 0, Basu_WestBengalBrahmins_similarMAF_hwe_table$O.HET./Basu_WestBengalBrahmins_similarMAF_hwe_table$E.HET.))
Pathak_Gujjars_similarMAF_hwe_table$F_score=1-(ifelse(Pathak_Gujjars_similarMAF_hwe_table$O.HET.==0, 0, Pathak_Gujjars_similarMAF_hwe_table$O.HET./Pathak_Gujjars_similarMAF_hwe_table$E.HET.))
Pathak_Kambojs_similarMAF_hwe_table$F_score=1-(ifelse(Pathak_Kambojs_similarMAF_hwe_table$O.HET.==0, 0, Pathak_Kambojs_similarMAF_hwe_table$O.HET./Pathak_Kambojs_similarMAF_hwe_table$E.HET.))
Pathak_Rors_similarMAF_hwe_table$F_score=1-(ifelse(Pathak_Rors_similarMAF_hwe_table$O.HET.==0, 0, Pathak_Rors_similarMAF_hwe_table$O.HET./Pathak_Rors_similarMAF_hwe_table$E.HET.))
#find F value proportions in each pop (i.e. proportion of similar SNPs with F score less than in functional SNP)
Birhor_Fproportion=nrow(subset(Basu_Birhors_similarMAF_hwe_table, F_score<Birhor_excesshomo))/(nrow(Basu_Birhors_similarMAF_hwe_table))
Gond_Fproportion=nrow(subset(Basu_Gonds_similarMAF_hwe_table, F_score<Gond_excesshomo))/(nrow(Basu_Gonds_similarMAF_hwe_table))
GujuratiBrahmin_Fproportion=nrow(subset(Basu_GujuratiBrahmins_similarMAF_hwe_table, F_score<GujuratiBrahmin_excesshomo))/(nrow(Basu_GujuratiBrahmins_similarMAF_hwe_table))
Ho_Fproportion=nrow(subset(Basu_Hos_similarMAF_hwe_table, F_score<Ho_excesshomo))/(nrow(Basu_Hos_similarMAF_hwe_table))
Irula_Fproportion=nrow(subset(Basu_Irulas_similarMAF_hwe_table, F_score<Irula_excesshomo))/(nrow(Basu_Irulas_similarMAF_hwe_table))
Iyer_Fproportion=nrow(subset(Basu_Iyers_similarMAF_hwe_table, F_score<Iyer_excesshomo))/(nrow(Basu_Iyers_similarMAF_hwe_table))
Jamatia_Fproportion=nrow(subset(Basu_Jamatias_similarMAF_hwe_table, F_score<Jamatia_excesshomo))/(nrow(Basu_Jamatias_similarMAF_hwe_table))
Jarawa_Fproportion=nrow(subset(Basu_Jarawas_similarMAF_hwe_table, F_score<Jarawa_excesshomo))/(nrow(Basu_Jarawas_similarMAF_hwe_table))
Kadar_Fproportion=nrow(subset(Basu_Kadars_similarMAF_hwe_table, F_score<Kadar_excesshomo))/(nrow(Basu_Kadars_similarMAF_hwe_table))
Khatri_Fproportion=nrow(subset(Basu_Khatris_similarMAF_hwe_table, F_score<Khatri_excesshomo))/(nrow(Basu_Khatris_similarMAF_hwe_table))
Korwa_Fproportion=nrow(subset(Basu_Korwas_similarMAF_hwe_table, F_score<Korwa_excesshomo))/(nrow(Basu_Korwas_similarMAF_hwe_table))
ManipuriBrahmin_Fproportion=nrow(subset(Basu_ManipuriBrahmins_similarMAF_hwe_table, F_score<ManipuriBrahmin_excesshomo))/(nrow(Basu_ManipuriBrahmins_similarMAF_hwe_table))
Maratha_Fproportion=nrow(subset(Basu_Marathas_similarMAF_hwe_table, F_score<Maratha_excesshomo))/(nrow(Basu_Marathas_similarMAF_hwe_table))
Onges_Fproportion=nrow(subset(Basu_Onges_similarMAF_hwe_table, F_score<Onge_excesshomo))/(nrow(Basu_Onges_similarMAF_hwe_table))
Pallan_Fproportion=nrow(subset(Basu_Pallans_similarMAF_hwe_table, F_score<Pallan_excesshomo))/(nrow(Basu_Pallans_similarMAF_hwe_table))
Paniya_Fproportion=nrow(subset(Basu_Paniyas_similarMAF_hwe_table, F_score<Paniya_excesshomo))/(nrow(Basu_Paniyas_similarMAF_hwe_table))
Santal_Fproportion=nrow(subset(Basu_Santals_similarMAF_hwe_table, F_score<Santal_excesshomo))/(nrow(Basu_Santals_similarMAF_hwe_table))
Tharu_Fproportion=nrow(subset(Basu_Tharus_similarMAF_hwe_table, F_score<Tharu_excesshomo))/(nrow(Basu_Tharus_similarMAF_hwe_table))
Tripuri_Fproportion=nrow(subset(Basu_Tripuris_similarMAF_hwe_table, F_score<Tripuri_excesshomo))/(nrow(Basu_Tripuris_similarMAF_hwe_table))
WestBengalBrahmin_Fproportion=nrow(subset(Basu_WestBengalBrahmins_similarMAF_hwe_table, F_score<WestBengalBrahmin_excesshomo))/(nrow(Basu_WestBengalBrahmins_similarMAF_hwe_table))
Gujjar_Fproportion=nrow(subset(Pathak_Gujjars_similarMAF_hwe_table, F_score<Gujjar_excesshomo))/(nrow(Pathak_Gujjars_similarMAF_hwe_table))
Kamboj_Fproportion=nrow(subset(Pathak_Kambojs_similarMAF_hwe_table, F_score<Kamboj_excesshomo))/(nrow(Pathak_Kambojs_similarMAF_hwe_table))
Ror_Fproportion=nrow(subset(Pathak_Rors_similarMAF_hwe_table, F_score<Ror_excesshomo))/(nrow(Pathak_Rors_similarMAF_hwe_table))
#--> GujuratiBrahmins, Ho, Jamatia, Kamboj, Khatri, Korwa,Tharu,WestBengalBrahmins have less than 0.05 for F value proportions, and then Birhor, Gujjar, Iyer, Jarawa, Maratha, Ror, Onges, Santal have 0 for proportion


#convert F value to Z score 
#first calculate average excess homozygosity for each population's bfile of SNPs with similar MAF
Birhor_similarSNPs_averageF=1-(mean(Basu_Birhors_similarMAF_hwe_table$O.HET.)/mean(Basu_Birhors_similarMAF_hwe_table$E.HET.))
Gond_similarSNPs_averageF=1-(mean(Basu_Gonds_similarMAF_hwe_table$O.HET.)/((mean(Basu_Gonds_similarMAF_hwe_table$E.HET.))))
GujuratiBrahmin_similarSNPs_averageF=1-(mean(Basu_GujuratiBrahmins_similarMAF_hwe_table$O.HET.)/((mean(Basu_GujuratiBrahmins_similarMAF_hwe_table$E.HET.))))
Ho_similarSNPs_averageF=1-(mean(Basu_Hos_similarMAF_hwe_table$O.HET.)/((mean(Basu_Hos_similarMAF_hwe_table$E.HET.))))
Irula_similarSNPs_averageF=1-(mean(Basu_Irulas_similarMAF_hwe_table$O.HET.)/((mean(Basu_Irulas_similarMAF_hwe_table$E.HET.))))
Iyer_similarSNPs_averageF=1-(mean(Basu_Iyers_similarMAF_hwe_table$O.HET.)/((mean(Basu_Iyers_similarMAF_hwe_table$E.HET.))))
Jamatia_similarSNPs_averageF=1-(mean(Basu_Jamatias_similarMAF_hwe_table$O.HET.)/((mean(Basu_Jamatias_similarMAF_hwe_table$E.HET.))))
Jarawa_similarSNPs_averageF=1-(mean(Basu_Jarawas_similarMAF_hwe_table$O.HET.)/((mean(Basu_Jarawas_similarMAF_hwe_table$E.HET.))))
Kadar_similarSNPs_averageF=1-(mean(Basu_Kadars_similarMAF_hwe_table$O.HET.)/((mean(Basu_Kadars_similarMAF_hwe_table$E.HET.))))
Khatri_similarSNPs_averageF=1-(mean(Basu_Khatris_similarMAF_hwe_table$O.HET.)/((mean(Basu_Khatris_similarMAF_hwe_table$E.HET.))))
Korwa_similarSNPs_averageF=1-(mean(Basu_Korwas_similarMAF_hwe_table$O.HET.)/((mean(Basu_Korwas_similarMAF_hwe_table$E.HET.))))
ManipuriBrahmin_similarSNPs_averageF=1-(mean(Basu_ManipuriBrahmins_similarMAF_hwe_table$O.HET.)/((mean(Basu_ManipuriBrahmins_similarMAF_hwe_table$E.HET.))))
Maratha_similarSNPs_averageF=1-(mean(Basu_Marathas_similarMAF_hwe_table$O.HET.)/((mean(Basu_Marathas_similarMAF_hwe_table$E.HET.))))
Onge_similarSNPs_averageF=1-(mean(Basu_Onges_similarMAF_hwe_table$O.HET.)/((mean(Basu_Onges_similarMAF_hwe_table$E.HET.))))
Pallan_similarSNPs_averageF=1-(mean(Basu_Pallans_similarMAF_hwe_table$O.HET.)/((mean(Basu_Pallans_similarMAF_hwe_table$E.HET.))))
Paniya_similarSNPs_averageF=1-(mean(Basu_Paniyas_similarMAF_hwe_table$O.HET.)/((mean(Basu_Paniyas_similarMAF_hwe_table$E.HET.))))
Santal_similarSNPs_averageF=1-(mean(Basu_Santals_similarMAF_hwe_table$O.HET.)/((mean(Basu_Santals_similarMAF_hwe_table$E.HET.))))
Tharu_similarSNPs_averageF=1-(mean(Basu_Tharus_similarMAF_hwe_table$O.HET.)/((mean(Basu_Tharus_similarMAF_hwe_table$E.HET.))))
Tripuri_similarSNPs_averageF=1-(mean(Basu_Tripuris_similarMAF_hwe_table$O.HET.)/((mean(Basu_Tripuris_similarMAF_hwe_table$E.HET.))))
WestBengalBrahmin_similarSNPs_averageF=1-(mean(Basu_WestBengalBrahmins_similarMAF_hwe_table$O.HET.)/((mean(Basu_WestBengalBrahmins_similarMAF_hwe_table$E.HET.))))
Gujjar_similarSNPs_averageF=1-(mean(Pathak_Gujjars_similarMAF_hwe_table$O.HET.)/((mean(Pathak_Gujjars_similarMAF_hwe_table$E.HET.))))
Kamboj_similarSNPs_averageF=1-(mean(Pathak_Kambojs_similarMAF_hwe_table$O.HET.)/((mean(Pathak_Kambojs_similarMAF_hwe_table$E.HET.))))
Ror_similarSNPs_averageF=1-(mean(Pathak_Rors_similarMAF_hwe_table$O.HET.)/((mean(Pathak_Rors_similarMAF_hwe_table$E.HET.))))
#now create object for SD of F value in SNPs with similar MAFs
Birhor_similarSNPs_F_sd=sd(Basu_Birhors_similarMAF_hwe_table$F_score)
Gond_similarSNPs_F_sd=sd(Basu_Gonds_similarMAF_hwe_table$F_score)
GujuratiBrahmin_similarSNPs_F_sd=sd(Basu_GujuratiBrahmins_similarMAF_hwe_table$F_score)
Ho_similarSNPs_F_sd=sd(Basu_Hos_similarMAF_hwe_table$F_score)
Irula_similarSNPs_F_sd=sd(Basu_Irulas_similarMAF_hwe_table$F_score)
Iyer_similarSNPs_F_sd=sd(Basu_Iyers_similarMAF_hwe_table$F_score)
Jamatia_similarSNPs_F_sd=sd(Basu_Jamatias_similarMAF_hwe_table$F_score)
Jarawa_similarSNPs_F_sd=sd(Basu_Jarawas_similarMAF_hwe_table$F_score)
Kadar_similarSNPs_F_sd=sd(Basu_Kadars_similarMAF_hwe_table$F_score)
Khatri_similarSNPs_F_sd=sd(Basu_Khatris_similarMAF_hwe_table$F_score)
Korwa_similarSNPs_F_sd=sd(Basu_Korwas_similarMAF_hwe_table$F_score)
ManipuriBrahmin_similarSNPs_F_sd=sd(Basu_ManipuriBrahmins_similarMAF_hwe_table$F_score)
Maratha_similarSNPs_F_sd=sd(Basu_Marathas_similarMAF_hwe_table$F_score)
Onge_similarSNPs_F_sd=sd(Basu_Onges_similarMAF_hwe_table$F_score)
Pallan_similarSNPs_F_sd=sd(Basu_Pallans_similarMAF_hwe_table$F_score)
Paniya_similarSNPs_F_sd=sd(Basu_Paniyas_similarMAF_hwe_table$F_score)
Santal_similarSNPs_F_sd=sd(Basu_Santals_similarMAF_hwe_table$F_score)
Tharu_similarSNPs_F_sd=sd(Basu_Tharus_similarMAF_hwe_table$F_score)
Tripuri_similarSNPs_F_sd=sd(Basu_Tripuris_similarMAF_hwe_table$F_score)
WestBengalBrahmin_similarSNPs_F_sd=sd(Basu_WestBengalBrahmins_similarMAF_hwe_table$F_score)
Gujjar_similarSNPs_F_sd=sd(Pathak_Gujjars_similarMAF_hwe_table$F_score)
Kamboj_similarSNPs_F_sd=sd(Pathak_Kambojs_similarMAF_hwe_table$F_score)
Ror_similarSNPs_F_sd=sd(Pathak_Rors_similarMAF_hwe_table$F_score)
#make Z score for F values 
Birhor_F_Zscore=(Birhor_excesshomo-Birhor_similarSNPs_averageF)/Birhor_similarSNPs_F_sd
Gond_F_Zscore=(Gond_excesshomo-Gond_similarSNPs_averageF)/Gond_similarSNPs_F_sd
GujuratiBrahmin_F_Zscore=(GujuratiBrahmin_excesshomo-GujuratiBrahmin_similarSNPs_averageF)/GujuratiBrahmin_similarSNPs_F_sd
Ho_F_Zscore=(Ho_excesshomo-Ho_similarSNPs_averageF)/Ho_similarSNPs_F_sd
Irula_F_Zscore=(Irula_excesshomo-Irula_similarSNPs_averageF)/Irula_similarSNPs_F_sd
Iyer_F_Zscore=(Iyer_excesshomo-Iyer_similarSNPs_averageF)/Iyer_similarSNPs_F_sd
Jamatia_F_Zscore=(Jamatia_excesshomo-Jamatia_similarSNPs_averageF)/Jamatia_similarSNPs_F_sd
Jarawa_F_Zscore=(Jarawa_excesshomo-Jarawa_similarSNPs_averageF)/Jarawa_similarSNPs_F_sd
Kadar_F_Zscore=(Kadar_excesshomo-Kadar_similarSNPs_averageF)/Kadar_similarSNPs_F_sd
Khatri_F_Zscore=(Khatri_excesshomo-Khatri_similarSNPs_averageF)/Khatri_similarSNPs_F_sd
Korwa_F_Zscore=(Korwa_excesshomo-Korwa_similarSNPs_averageF)/Korwa_similarSNPs_F_sd
ManipuriBrahmin_F_Zscore=(ManipuriBrahmin_excesshomo-ManipuriBrahmin_similarSNPs_averageF)/ManipuriBrahmin_similarSNPs_F_sd
Maratha_F_Zscore=(Maratha_excesshomo-Maratha_similarSNPs_averageF)/Maratha_similarSNPs_F_sd
Onge_F_Zscore=(Onge_excesshomo-Onge_similarSNPs_averageF)/Onge_similarSNPs_F_sd
Pallan_F_Zscore=(Pallan_excesshomo-Pallan_similarSNPs_averageF)/Pallan_similarSNPs_F_sd
Paniya_F_Zscore=(Paniya_excesshomo-Paniya_similarSNPs_averageF)/Paniya_similarSNPs_F_sd
Santal_F_Zscore=(Santal_excesshomo-Santal_similarSNPs_averageF)/Santal_similarSNPs_F_sd
Tharu_F_Zscore=(Tharu_excesshomo-Tharu_similarSNPs_averageF)/Tharu_similarSNPs_F_sd
Tripuri_F_Zscore=(Tripuri_excesshomo-Tripuri_similarSNPs_averageF)/Tripuri_similarSNPs_F_sd
WestBengalBrahmin_F_Zscore=(WestBengalBrahmin_excesshomo-WestBengalBrahmin_similarSNPs_averageF)/WestBengalBrahmin_similarSNPs_F_sd
Gujjar_F_Zscore=(Gujjar_excesshomo-Gujjar_similarSNPs_averageF)/Gujjar_similarSNPs_F_sd
Kamboj_F_Zscore=(Kamboj_excesshomo-Kamboj_similarSNPs_averageF)/Kamboj_similarSNPs_F_sd
Ror_F_Zscore=(Ror_excesshomo-Ror_similarSNPs_averageF)/Ror_similarSNPs_F_sd


#add Z scores to matrimonial table
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$F_Zscore=NA
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$F_Zscore[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujarati Brahmin"]=GujuratiBrahmin_F_Zscore
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$F_Zscore[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Gujjar"]=Gujjar_F_Zscore
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$F_Zscore[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Iyer"]=Iyer_F_Zscore
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$F_Zscore[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Kamboj"]=Kamboj_F_Zscore
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$F_Zscore[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Khatri"]=Khatri_F_Zscore
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$F_Zscore[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="Maratha"]=Maratha_F_Zscore
matrimonial30plus_genetic_withSNP_withcomp2answers_dt$F_Zscore[matrimonial30plus_genetic_withSNP_withcomp2answers_dt$caste1_1=="West Bengal Brahmin"]=WestBengalBrahmin_F_Zscore




###exporting tables to put in methods and results section in diss
#1. table of HW p-values and HW proportions for each genetic population
genetic_samples_table_with_HWEprops=matrimonial30plus_genetic_withSNP_withcomp2answers_dt[c(1,4,18,19)]
genetic_samples_table_with_HWEprops=genetic_samples_table_with_HWEprops[-c(10),]
genetic_samples_table_with_HWEprops$HWE_pvalue[genetic_samples_table_with_HWEprops$caste1_1=="Birhor"]=BirhorHWEvalue
genetic_samples_table_with_HWEprops$HWEproportion[genetic_samples_table_with_HWEprops$caste1_1=="Birhor"]=BirhorHWEproportion
genetic_samples_table_with_HWEprops$HWE_pvalue[genetic_samples_table_with_HWEprops$caste1_1=="Jarawa"]=JarawaHWEvalue
genetic_samples_table_with_HWEprops$HWEproportion[genetic_samples_table_with_HWEprops$caste1_1=="Jarawa"]=JarawaHWEproportion
genetic_samples_table_with_HWEprops$HWE_pvalue[genetic_samples_table_with_HWEprops$caste1_1=="Onge"]=OngesHWEvalue
genetic_samples_table_with_HWEprops$HWEproportion[genetic_samples_table_with_HWEprops$caste1_1=="Onge"]=OngesHWEproportion
genetic_samples_table_with_HWEprops$HWE_pvalue[genetic_samples_table_with_HWEprops$caste1_1=="Ror"]=RorHWEvalue
genetic_samples_table_with_HWEprops$HWEproportion[genetic_samples_table_with_HWEprops$caste1_1=="Ror"]=RorHWEproportion
genetic_samples_table_with_HWEprops$HWE_pvalue[genetic_samples_table_with_HWEprops$caste1_1=="Santal"]=SantalHWEvalue
genetic_samples_table_with_HWEprops$HWEproportion[genetic_samples_table_with_HWEprops$caste1_1=="Santal"]=SantalHWEproportion
genetic_samples_table_with_HWEprops$HWE_pvalue[genetic_samples_table_with_HWEprops$caste1_1=="Gond"]=GondHWEvalue
genetic_samples_table_with_HWEprops$HWEproportion[genetic_samples_table_with_HWEprops$caste1_1=="Gond"]=GondHWEproportion
genetic_samples_table_with_HWEprops$HWE_pvalue[genetic_samples_table_with_HWEprops$caste1_1=="Ho"]=HoHWEvalue
genetic_samples_table_with_HWEprops$HWEproportion[genetic_samples_table_with_HWEprops$caste1_1=="Ho"]=HoHWEproportion
genetic_samples_table_with_HWEprops$HWE_pvalue[genetic_samples_table_with_HWEprops$caste1_1=="Irula"]=IrulaHWEvalue
genetic_samples_table_with_HWEprops$HWEproportion[genetic_samples_table_with_HWEprops$caste1_1=="Irula"]=IrulaHWEproportion
genetic_samples_table_with_HWEprops$HWE_pvalue[genetic_samples_table_with_HWEprops$caste1_1=="Jamatia"]=JamatiaHWEvalue
genetic_samples_table_with_HWEprops$HWEproportion[genetic_samples_table_with_HWEprops$caste1_1=="Jamatia"]=JamatiaHWEproportion
genetic_samples_table_with_HWEprops$HWE_pvalue[genetic_samples_table_with_HWEprops$caste1_1=="Kadar"]=KadarHWEvalue
genetic_samples_table_with_HWEprops$HWEproportion[genetic_samples_table_with_HWEprops$caste1_1=="Kadar"]=KadarHWEproportion
genetic_samples_table_with_HWEprops$HWE_pvalue[genetic_samples_table_with_HWEprops$caste1_1=="Korwa"]=KorwaHWEvalue
genetic_samples_table_with_HWEprops$HWEproportion[genetic_samples_table_with_HWEprops$caste1_1=="Korwa"]=KorwaHWEproportion
genetic_samples_table_with_HWEprops$HWE_pvalue[genetic_samples_table_with_HWEprops$caste1_1=="Manipuri Brahmin"]=ManipuriBrahminHWEvalue
genetic_samples_table_with_HWEprops$HWEproportion[genetic_samples_table_with_HWEprops$caste1_1=="Manipuri Brahmin"]=ManipuriBrahminHWEproportion
genetic_samples_table_with_HWEprops$HWE_pvalue[genetic_samples_table_with_HWEprops$caste1_1=="Pallan"]=PallanHWEvalue
genetic_samples_table_with_HWEprops$HWEproportion[genetic_samples_table_with_HWEprops$caste1_1=="Pallan"]=PallanHWEproportion
genetic_samples_table_with_HWEprops$HWE_pvalue[genetic_samples_table_with_HWEprops$caste1_1=="Paniya"]=PaniyaHWEvalue
genetic_samples_table_with_HWEprops$HWEproportion[genetic_samples_table_with_HWEprops$caste1_1=="Paniya"]=PaniyaHWEproportion
genetic_samples_table_with_HWEprops$HWE_pvalue[genetic_samples_table_with_HWEprops$caste1_1=="Tharu"]=TharuHWEvalue
genetic_samples_table_with_HWEprops$HWEproportion[genetic_samples_table_with_HWEprops$caste1_1=="Tharu"]=TharuHWEproportion
genetic_samples_table_with_HWEprops$HWE_pvalue[genetic_samples_table_with_HWEprops$caste1_1=="Tripuri"]=TripuriHWEvalue
genetic_samples_table_with_HWEprops$HWEproportion[genetic_samples_table_with_HWEprops$caste1_1=="Tripuri"]=TripuriHWEproportion
#make allele freq values for all pops not in matrimonial dataset (which already have allelefreqs)
if(Basu_Birhors_freq_table$A1=="G"){
  Birhor_allelefreq <- 1-Basu_Birhors_freq_table$MAF
} else if(Basu_Birhors_freq_table$A1=="A"){
  Birhor_allelefreq <- Basu_Birhors_freq_table$MAF
}

if(Basu_Gonds_freq_table$A1=="G"){
  Gond_allelefreq <- 1-Basu_Gonds_freq_table$MAF
} else if(Basu_Gonds_freq_table$A1=="A"){
  Gond_allelefreq <- Basu_Gonds_freq_table$MAF
}

if(Basu_Hos_freq_table$A1=="G"){
  Ho_allelefreq <- 1-Basu_Hos_freq_table$MAF
} else if(Basu_Hos_freq_table$A1=="A"){
  Ho_allelefreq <- Basu_Hos_freq_table$MAF
}

if(Basu_Irulas_freq_table$A1=="G"){
  Irula_allelefreq <- 1-Basu_Irulas_freq_table$MAF
} else if(Basu_Irulas_freq_table$A1=="A"){
  Irula_allelefreq <- Basu_Irulas_freq_table$MAF
}

if(Basu_Jamatias_freq_table$A1=="G"){
  Jamatia_allelefreq <- 1-Basu_Jamatias_freq_table$MAF
} else if(Basu_Jamatias_freq_table$A1=="A"){
  Jamatia_allelefreq <- Basu_Jamatias_freq_table$MAF
}

if(Basu_Jarawas_freq_table$A1=="G"){
  Jarawa_allelefreq <- 1-Basu_Jarawas_freq_table$MAF
} else if(Basu_Jarawas_freq_table$A1=="A"){
  Jarawa_allelefreq <- Basu_Jarawas_freq_table$MAF
}

if(Basu_Kadars_freq_table$A1=="G"){
  Kadar_allelefreq <- 1-Basu_Kadars_freq_table$MAF
} else if(Basu_Kadars_freq_table$A1=="A"){
  Kadar_allelefreq <- Basu_Kadars_freq_table$MAF
}

if(Basu_Korwas_freq_table$A1=="G"){
  Korwa_allelefreq <- 1-Basu_Korwas_freq_table$MAF
} else if(Basu_Korwas_freq_table$A1=="A"){
  Korwa_allelefreq <- Basu_Korwas_freq_table$MAF
}

if(Basu_ManipuriBrahmins_freq_table$A1=="G"){
  ManipuriBrahmin_allelefreq <- 1-Basu_ManipuriBrahmins_freq_table$MAF
} else if(Basu_ManipuriBrahmins_freq_table$A1=="A"){
  ManipuriBrahmin_allelefreq <- Basu_ManipuriBrahmins_freq_table$MAF
}

if(Basu_Onges_freq_table$A1=="G"){
  Onge_allelefreq <- 1-Basu_Onges_freq_table$MAF
} else if(Basu_Onges_freq_table$A1=="A"){
  Onge_allelefreq <- Basu_Onges_freq_table$MAF
}

if(Basu_Pallans_freq_table$A1=="G"){
  Pallan_allelefreq <- 1-Basu_Pallans_freq_table$MAF
} else if(Basu_Pallans_freq_table$A1=="A"){
  Pallan_allelefreq <- Basu_Pallans_freq_table$MAF
}

if(Basu_Paniyas_freq_table$A1=="G"){
  Paniya_allelefreq <- 1-Basu_Paniyas_freq_table$MAF
} else if(Basu_Paniyas_freq_table$A1=="A"){
  Paniya_allelefreq <- Basu_Paniyas_freq_table$MAF
}

if(Pathak_Rors_freq_table$A1=="G"){
  Ror_allelefreq <- 1-Pathak_Rors_freq_table$MAF
} else if(Pathak_Rors_freq_table$A1=="A"){
  Ror_allelefreq <- Pathak_Rors_freq_table$MAF
}

if(Basu_Santals_freq_table$A1=="G"){
  Santal_allelefreq <- 1-Basu_Santals_freq_table$MAF
} else if(Basu_Santals_freq_table$A1=="A"){
  Santal_allelefreq <- Basu_Santals_freq_table$MAF
}

if(Basu_Tharus_freq_table$A1=="G"){
  Tharu_allelefreq <- 1-Basu_Tharus_freq_table$MAF
} else if(Basu_Tharus_freq_table$A1=="A"){
  Tharu_allelefreq <- Basu_Tharus_freq_table$MAF
}
if(Basu_Tripuris_freq_table$A1=="G"){
  Tripuri_allelefreq <- 1-Basu_Tripuris_freq_table$MAF
} else if(Basu_Tripuris_freq_table$A1=="A"){
  Tripuri_allelefreq <- Basu_Tripuris_freq_table$MAF
}

#add allele freqs to table
genetic_samples_table_with_HWEprops$allelefreq=NA
genetic_samples_table_with_HWEprops$allelefreq[genetic_samples_table_with_HWEprops$Population=="Birhor"]=Birhor_allelefreq
genetic_samples_table_with_HWEprops$allelefreq[genetic_samples_table_with_HWEprops$Population=="Jarawa"]=Jarawa_allelefreq
genetic_samples_table_with_HWEprops$allelefreq[genetic_samples_table_with_HWEprops$Population=="Onge"]=Onge_allelefreq
genetic_samples_table_with_HWEprops$allelefreq[genetic_samples_table_with_HWEprops$Population=="Ror"]=Ror_allelefreq
genetic_samples_table_with_HWEprops$allelefreq[genetic_samples_table_with_HWEprops$Population=="Santal"]=Santal_allelefreq
genetic_samples_table_with_HWEprops$allelefreq[genetic_samples_table_with_HWEprops$Population=="Gond"]=Gond_allelefreq
genetic_samples_table_with_HWEprops$allelefreq[genetic_samples_table_with_HWEprops$Population=="Ho"]=Ho_allelefreq
genetic_samples_table_with_HWEprops$allelefreq[genetic_samples_table_with_HWEprops$Population=="Irula"]=Irula_allelefreq
genetic_samples_table_with_HWEprops$allelefreq[genetic_samples_table_with_HWEprops$Population=="Jamatia"]=Jamatia_allelefreq
genetic_samples_table_with_HWEprops$allelefreq[genetic_samples_table_with_HWEprops$Population=="Kadar"]=Kadar_allelefreq
genetic_samples_table_with_HWEprops$allelefreq[genetic_samples_table_with_HWEprops$Population=="Korwa"]=Korwa_allelefreq
genetic_samples_table_with_HWEprops$allelefreq[genetic_samples_table_with_HWEprops$Population=="Manipuri Brahmin"]=ManipuriBrahmin_allelefreq
genetic_samples_table_with_HWEprops$allelefreq[genetic_samples_table_with_HWEprops$Population=="Pallan"]=Pallan_allelefreq
genetic_samples_table_with_HWEprops$allelefreq[genetic_samples_table_with_HWEprops$Population=="Paniya"]=Paniya_allelefreq
genetic_samples_table_with_HWEprops$allelefreq[genetic_samples_table_with_HWEprops$Population=="Tharu"]=Tharu_allelefreq
genetic_samples_table_with_HWEprops$allelefreq[genetic_samples_table_with_HWEprops$Population=="Tripuri"]=Tripuri_allelefreq
#export
colnames(genetic_samples_table_with_HWEprops)=c("Caste", "Genetic Sample Size", "Hardy-Weinberg p-value", "Hardy-Weinberg proportion", "rs1426654-A allele frequency")
genetic_samples_table_with_HWEprops=genetic_samples_table_with_HWEprops[, c(1,3,4,2,5)]
write.csv(genetic_samples_table_with_HWEprops, "genetic_samples_table_with_HWEprops.csv")





# now make table of 7 genetic-matri populations with all measures of F, Z, assortative mating and light skin preference - to be used to make correlation matrix
#remove comp2 answers, matri and genetic pop size
fullgenetictable=matrimonial30plus_genetic_withSNP_withcomp2answers_dt[, c(1,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)]
colnames(fullgenetictable)=c("Population", "F-score", "Average desired skin index, excluding 'Doesn’t Matter' answers", "Average desired skin index, including 'Doesn’t Matter' answers", "Proportion of caste wanting spouse to have 'Fair' or 'Very Fair' skin, excluding 'Doesn’t Matter' answers" ,"Proportion of caste wanting spouse to have 'Fair' or 'Very Fair' skin, including 'Doesn’t Matter' answers", "Proportion of each caste wanting their future spouse to be of the same skin complexion as themselves", "Proportion of caste wanting their future spouse to be within one step of their own skin complexion index", "Proportion of caste wanting their future spouse to be the same skin complexion or one index step lighter than themselves","Proportion of caste wanting their future spouse to be the same skin complexion or one index step darker than themselves","Proportion of caste who wanted to marry individuals who were only lighter in complexion compared to themselves", "Proportion of caste who wanted to marry individuals who were only darker in complexion compared to themselves", "SNP rs142665-A allele frequency", "Average self-stated skin index", "Hardy-Weinberg p-value", "Hardy-Weinberg proportion", "Z-score (of F)" )
##remove Jatt and populations with MAF 0 for SNP
fullgenetictable=fullgenetictable[-c(1,2,5,6,8,9,10,11,14,15,17,18,19,20,21,22,23), ]
#reorder columns so F followed straight by Z score
fullgenetictable=fullgenetictable[,c(1,2,17,3,4,5,6,7,8,9,10,11,12,13,14,15,16)]
#remove F score and HW_p value from table 
fullgenetictable=fullgenetictable[, c(1,3:15, 17)]
#give HW prop for Kamboj as NA
fullgenetictable$`Hardy-Weinberg proportion`[fullgenetictable$Population=="Kamboj"]=NA
#export
write.csv(fullgenetictable,"fullgenetictable.csv")
##################redo new table with F score included
fullgenetictable2=matrimonial30plus_genetic_withSNP_withcomp2answers_dt[, c(1,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)]
colnames(fullgenetictable2)=c("Population", "F-score", "Average desired skin index, excluding 'Doesn’t Matter' answers", "Average desired skin index, including 'Doesn’t Matter' answers", "Proportion of caste wanting spouse to have 'Fair' or 'Very Fair' skin, excluding 'Doesn’t Matter' answers" ,"Proportion of caste wanting spouse to have 'Fair' or 'Very Fair' skin, including 'Doesn’t Matter' answers", "Proportion of each caste wanting their future spouse to be of the same skin complexion as themselves", "Proportion of caste wanting their future spouse to be within one step of their own skin complexion index", "Proportion of caste wanting their future spouse to be the same skin complexion or one index step lighter than themselves","Proportion of caste wanting their future spouse to be the same skin complexion or one index step darker than themselves","Proportion of caste who wanted to marry individuals who were only lighter in complexion compared to themselves", "Proportion of caste who wanted to marry individuals who were only darker in complexion compared to themselves", "SNP rs142665-A allele frequency", "Average self-stated skin index", "Hardy-Weinberg p-value", "Hardy-Weinberg proportion", "Z-score (of F)" )
##remove Jatt and populations with MAF 0 for SNP
fullgenetictable2=fullgenetictable2[-c(1,2,5,6,8,9,10,11,14,15,17,18,19,20,21,22,23), ]
#reorder columns so F followed straight by Z score
fullgenetictable2=fullgenetictable2[,c(1,2,17,3,4,5,6,7,8,9,10,11,12,13,14,15,16)]
#remove only HW_p value from table 
fullgenetictable2=fullgenetictable2[, c(1:15, 17)]
#give HW prop for Kamboj as NA
fullgenetictable2$`Hardy-Weinberg proportion`[fullgenetictable2$Population=="Kamboj"]=NA
#export
write.csv(fullgenetictable2,"fullgenetictable2.csv")
#add proportion of caste wanting dark to matrimonial table for correlations with excess_homo
#first make object with this proportion for each pop
GujuratiBrahmin_comp2_dark_prop=nrow(subset(GujuratiBrahmin, complexion2code==5))/nrow(GujuratiBrahmin[!is.na(GujuratiBrahmin$complexion2code)])
Gujjar_comp2_dark_prop=nrow(subset(Gujjar, complexion2code==5))/nrow(Gujjar[!is.na(Gujjar$complexion2code)])
Iyer_comp2_dark_prop=nrow(subset(Iyer, complexion2code==5))/nrow(Iyer[!is.na(Iyer$complexion2code)])
Kamboj_comp2_dark_prop=nrow(subset(Kamboj, complexion2code==5))/nrow(Kamboj[!is.na(Kamboj$complexion2code)])
Khatri_comp2_dark_prop=nrow(subset(Khatri, complexion2code==5))/nrow(Khatri[!is.na(Khatri$complexion2code)])
Maratha_comp2_dark_prop=nrow(subset(Maratha, complexion2code==5))/nrow(Maratha[!is.na(Maratha$complexion2code)])
WestBengalBrahmin_comp2_dark_prop=nrow(subset(WestBengalBrahmin, complexion2code==5))/nrow(WestBengalBrahmin[!is.na(WestBengalBrahmin$complexion2code)])
#now add  to full genetics table
#now add pops to full genetics table
fullgenetictable2$comp2_dark_prop=NA
fullgenetictable2$comp2_dark_prop[fullgenetictable2$Population=="Gujarati Brahmin"]=GujuratiBrahmin_comp2_dark_prop
fullgenetictable2$comp2_dark_prop[fullgenetictable2$Population=="Gujjar"]=Gujjar_comp2_dark_prop
fullgenetictable2$comp2_dark_prop[fullgenetictable2$Population=="Iyer"]=Iyer_comp2_dark_prop
fullgenetictable2$comp2_dark_prop[fullgenetictable2$Population=="Kamboj"]=Kamboj_comp2_dark_prop
fullgenetictable2$comp2_dark_prop[fullgenetictable2$Population=="Khatri"]=Khatri_comp2_dark_prop
fullgenetictable2$comp2_dark_prop[fullgenetictable2$Population=="Maratha"]=Maratha_comp2_dark_prop
fullgenetictable2$comp2_dark_prop[fullgenetictable2$Population=="West Bengal Brahmin"]=WestBengalBrahmin_comp2_dark_prop
#reorder columns
fullgenetictable2=fullgenetictable2[,c(1:7,17,8,9,10,11,12,13,14,15,16)]
#rename column comp2_av
colnames(fullgenetictable2)[8]="Proportion of caste wanting spouse to have 'Dark' skin"

#for correlation matrix:
cortable=fullgenetictable2
library(rstatix)
corMatrix <- cortable %>% cor_mat("Z-score (of F)":"Hardy-Weinberg proportion", method = "kendall")
corMatrix
corMatrix = pull_lower_triangle(corMatrix)
write.csv(corMatrix, "corMatrix.csv")
#for correlations' p values
corMatrix_p <- corMatrix %>% cor_get_pval()
corMatrix_p
corMatrix_p = pull_lower_triangle(corMatrix_p)
write.csv(corMatrix_p, "corMatrix_p.csv")
#visualization --> needs to have full matrices i.e. before triangles were pulled
corMatrix <- cortable %>% cor_mat("Z-score (of F)":"Hardy-Weinberg proportion", method = "kendall")
corMatrix
corMatrix_p <- corMatrix %>% cor_get_pval()
corMatrix_p
library("ggcorrplot")
tiff("corMatrix.tiff", res=96, height=3200, width=3600)
ggcorrplot(corMatrix, hc.order = FALSE, type = "lower", p.mat = corMatrix_p, tl.cex = 32, insig = "blank", sig.level = 0.05, lab=TRUE, lab_col = "white", title = "Correlation Plot of all Genetic and Social Metrics", lab_size = 10)
dev.off()







###############do multivariate analysis on genetics to test assortative mating hypoth 3
cortableformulti=(cortable)
#now rename colnames
colnames(cortableformulti)=c("caste", "Zscore", "comp2_av", "comp2_av2", "Propfairorvfair","Propfairorvfair2","Propdark", "AM1", "AM2", "AM3", "AM4", "AM5", "AM6", "allelefreq", "comp1_av", "HW_prop" )
genetics_assort_testfull15=glm(Zscore~AM1+AM5+AM6, data=cortableformulti, family=gaussian)
summary(genetics_assort_testfull15)
vif(genetics_assort_testfull15)
genetics_assort_best15=stepAIC(genetics_assort_testfull15)
summary(genetics_assort_best15)
#AM5 remains but not significant 
#try with comp1_av and comp2_av added
genetics_assort_testfull13=glm(Zscore~AM1+AM5+AM6+comp1_av+comp2_av, data=cortableformulti, family=gaussian)
summary(genetics_assort_testfull13)
genetics_assort_best13=stepAIC(genetics_assort_testfull13)
summary(genetics_assort_best13)
vif(genetics_assort_best13)





####do multivariate analysis on genetics to test sexual selection hypoth
#use all preference independent variables 
genetics_ss_testfull_test3=glm(allelefreq~Propfairorvfair+comp2_av+Propdark, data=cortableformulti, family=gaussian)
summary(genetics_ss_testfull_test3)
genetics_ss_testfull_test3best=stepAIC(genetics_ss_testfull_test3)
summary(genetics_ss_testfull_test3best)

##do just allelefreq against comp2 av
genetics_ss_testfull_test=glm(allelefreq~comp2_av, data=cortableformulti, family=gaussian)
write.csv(summary(genetics_ss_testfull_test)["coefficients"], "allelefreq_ss_full_onlycomp2av.csv")
##do just allelefreq against prop fair or v fair 
genetics_ss_testfull_test2=glm(allelefreq~Propfairorvfair, data=cortableformulti, family=gaussian)
write.csv(summary(genetics_ss_testfull_test2)["coefficients"], "allelefreq_ss_full_onlypropfairvfair.csv")
##do just allelefreq against prop dark
genetics_ss_testfull_test4=glm(allelefreq~Propdark, data=cortableformulti, family=gaussian)
summary(genetics_ss_testfull_test4)









#############further genetic analyses 
#1. create Z scores for India-wide F using Basu and 3 Pathak samples containing SNP, limited to 14 per pop (therefore excluding Maratha with only)
#first load HWE table created in plink 
Basu_Pathak_collective_samples_hwe_table=read.table("Basu_Pathak_collective_samples3_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header = TRUE)
#then make F score column
Basu_Pathak_collective_samples_hwe_table$F_score=1-(Basu_Pathak_collective_samples_hwe_table$O.HET./Basu_Pathak_collective_samples_hwe_table$E.HET.)
#now merge freq table with hwe table to give allele freqs
Basu_Pathak_collective_samples_freq_table=read.table("Basu_Pathak_collective_samples3_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Pathak_collective_samples_hweandfreq_table=merge(Basu_Pathak_collective_samples_hwe_table, Basu_Pathak_collective_samples_freq_table, by="SNP", all.x=TRUE)
Basu_Pathak_collective_samples_hweandfreq_table=Basu_Pathak_collective_samples_hweandfreq_table[, c(1,2,3,4,5,6,7,8,9,10,14,15)]
#now create bins by MAF
bins <- seq(0, 1, by=0.05)
Basu_Pathak_collective_samples_hweandfreq_table$Bin_MAF=cut(Basu_Pathak_collective_samples_hweandfreq_table$MAF, bins)
#then make Z score for each SNP, using the bins to make the "average F for SNPs of similar MAF" part of the equation first
library(dplyr)
Basu_Pathak_collective_samples_hweandfreq_table=Basu_Pathak_collective_samples_hweandfreq_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr:: mutate(Bin_F_average = mean(F_score))
Basu_Pathak_collective_samples_hweandfreq_table=Basu_Pathak_collective_samples_hweandfreq_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr:: mutate(Bin_F_sd = sd(F_score))
Basu_Pathak_collective_samples_hweandfreq_table$Z_score=(Basu_Pathak_collective_samples_hweandfreq_table$F_score-Basu_Pathak_collective_samples_hweandfreq_table$Bin_F_average)/Basu_Pathak_collective_samples_hweandfreq_table$Bin_F_sd
#now see where rs1426654 SNP sits 
Basu_Pathak_collective_samples_hweandfreq_table$Z_score[which(Basu_Pathak_collective_samples_hweandfreq_table$SNP=="rs1426654")]
#--> Z score is 3.60! Significantly high!

##more refined analysis: India-wide Z scores - need to add variance of MAF across sub-populations to collective samples table for more refined measure
#first load 14_allSNPs tables for each population (except Maratha because only 7 samples)
Basu_Birhors_14_allSNPs_freq_table=read.table("Basu_Birhors_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Jarawas_14_allSNPs_freq_table=read.table("Basu_Jarawas_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Santals_14_allSNPs_freq_table=read.table("Basu_Santals_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Onges_14_allSNPs_freq_table=read.table("Basu_Onges_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Pathak_Rors_14_allSNPs_freq_table=read.table("Pathak_Rors_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_WestBengalBrahmins_14_allSNPs_freq_table=read.table("Basu_WestBengalBrahmins_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Tripuris_14_allSNPs_freq_table=read.table("Basu_Tripuris_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Tharus_14_allSNPs_freq_table=read.table("Basu_Tharus_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Paniyas_14_allSNPs_freq_table=read.table("Basu_Paniyas_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Pallans_14_allSNPs_freq_table=read.table("Basu_Pallans_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_ManipuriBrahmins_14_allSNPs_freq_table=read.table("Basu_ManipuriBrahmins_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Korwas_14_allSNPs_freq_table=read.table("Basu_Korwas_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Pathak_Kambojs_14_allSNPs_freq_table=read.table("Pathak_Kambojs_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Khatris_14_allSNPs_freq_table=read.table("Basu_Khatris_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Kadars_14_allSNPs_freq_table=read.table("Basu_Kadars_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Jamatias_14_allSNPs_freq_table=read.table("Basu_Jamatias_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Iyers_14_allSNPs_freq_table=read.table("Basu_Iyers_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Irulas_14_allSNPs_freq_table=read.table("Basu_Irulas_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Hos_14_allSNPs_freq_table=read.table("Basu_Hos_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Pathak_Gujjars_14_allSNPs_freq_table=read.table("Pathak_Gujjars_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_GujuratiBrahmins_14_allSNPs_freq_table=read.table("Basu_GujuratiBrahmins_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Gonds_14_allSNPs_freq_table=read.table("Basu_Gonds_14_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
#in each sub-population's allSNPs_freq_table, need to add column determining allele freq of (arbitrarily chosen) one of two alleles: G-A, G-T, G-C --> sameallele is the not G one, A-T, A-C --> sameallele is the A, T-C --> sameallele is the T (in other words, A always dominates, T dominates over G and C, C dominates over G, 0 dominates over all)
#BIRHOR
for (i in 1:nrow(Basu_Birhors_14_allSNPs_freq_table)){
  if (Basu_Birhors_14_allSNPs_freq_table$A1[i]=="G" & (Basu_Birhors_14_allSNPs_freq_table$A2[i]=="A"|Basu_Birhors_14_allSNPs_freq_table$A2[i]=="T"|Basu_Birhors_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Birhors_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Birhors_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Birhors_14_allSNPs_freq_table$A1[i]=="A"|Basu_Birhors_14_allSNPs_freq_table$A1[i]=="T"|Basu_Birhors_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Birhors_14_allSNPs_freq_table$A2[i]=="G")){
    Basu_Birhors_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Birhors_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Birhors_14_allSNPs_freq_table$A1[i]=="A" & (Basu_Birhors_14_allSNPs_freq_table$A2[i]=="T"|Basu_Birhors_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Birhors_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Birhors_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Birhors_14_allSNPs_freq_table$A1[i]=="T"|Basu_Birhors_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Birhors_14_allSNPs_freq_table$A2[i]=="A")){
    Basu_Birhors_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Birhors_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Birhors_14_allSNPs_freq_table$A1[i]=="T" & (Basu_Birhors_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Birhors_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Birhors_14_allSNPs_freq_table$MAF[i]
  } else if(Basu_Birhors_14_allSNPs_freq_table$A1[i]=="C" & (Basu_Birhors_14_allSNPs_freq_table$A2[i]=="T")){
    Basu_Birhors_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Birhors_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Basu_Birhors_14_allSNPs_freq_table, "Basu_Birhors_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Basu_Birhors_14_allSNPs_freq_table= read.table("Basu_Birhors_14_allSNPs_freq_table", sep=" ")

#GOND
for (i in 1:nrow(Basu_Gonds_14_allSNPs_freq_table)){
  if (Basu_Gonds_14_allSNPs_freq_table$A1[i]=="G" & (Basu_Gonds_14_allSNPs_freq_table$A2[i]=="A"|Basu_Gonds_14_allSNPs_freq_table$A2[i]=="T"|Basu_Gonds_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Gonds_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Gonds_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Gonds_14_allSNPs_freq_table$A1[i]=="A"|Basu_Gonds_14_allSNPs_freq_table$A1[i]=="T"|Basu_Gonds_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Gonds_14_allSNPs_freq_table$A2[i]=="G")){
    Basu_Gonds_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Gonds_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Gonds_14_allSNPs_freq_table$A1[i]=="A" & (Basu_Gonds_14_allSNPs_freq_table$A2[i]=="T"|Basu_Gonds_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Gonds_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Gonds_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Gonds_14_allSNPs_freq_table$A1[i]=="T"|Basu_Gonds_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Gonds_14_allSNPs_freq_table$A2[i]=="A")){
    Basu_Gonds_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Gonds_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Gonds_14_allSNPs_freq_table$A1[i]=="T" & (Basu_Gonds_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Gonds_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Gonds_14_allSNPs_freq_table$MAF[i]
  } else if(Basu_Gonds_14_allSNPs_freq_table$A1[i]=="C" & (Basu_Gonds_14_allSNPs_freq_table$A2[i]=="T")){
    Basu_Gonds_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Gonds_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Basu_Gonds_14_allSNPs_freq_table, "Basu_Gonds_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Basu_Gonds_14_allSNPs_freq_table= read.table("Basu_Gonds_14_allSNPs_freq_table", sep=" ")

#GujuratiBrahmin
for (i in 1:nrow(Basu_GujuratiBrahmins_14_allSNPs_freq_table)){
  if (Basu_GujuratiBrahmins_14_allSNPs_freq_table$A1[i]=="G" & (Basu_GujuratiBrahmins_14_allSNPs_freq_table$A2[i]=="A"|Basu_GujuratiBrahmins_14_allSNPs_freq_table$A2[i]=="T"|Basu_GujuratiBrahmins_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_GujuratiBrahmins_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_GujuratiBrahmins_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_GujuratiBrahmins_14_allSNPs_freq_table$A1[i]=="A"|Basu_GujuratiBrahmins_14_allSNPs_freq_table$A1[i]=="T"|Basu_GujuratiBrahmins_14_allSNPs_freq_table$A1[i]=="C") & (Basu_GujuratiBrahmins_14_allSNPs_freq_table$A2[i]=="G")){
    Basu_GujuratiBrahmins_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_GujuratiBrahmins_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_GujuratiBrahmins_14_allSNPs_freq_table$A1[i]=="A" & (Basu_GujuratiBrahmins_14_allSNPs_freq_table$A2[i]=="T"|Basu_GujuratiBrahmins_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_GujuratiBrahmins_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_GujuratiBrahmins_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_GujuratiBrahmins_14_allSNPs_freq_table$A1[i]=="T"|Basu_GujuratiBrahmins_14_allSNPs_freq_table$A1[i]=="C") & (Basu_GujuratiBrahmins_14_allSNPs_freq_table$A2[i]=="A")){
    Basu_GujuratiBrahmins_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_GujuratiBrahmins_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_GujuratiBrahmins_14_allSNPs_freq_table$A1[i]=="T" & (Basu_GujuratiBrahmins_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_GujuratiBrahmins_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_GujuratiBrahmins_14_allSNPs_freq_table$MAF[i]
  } else if(Basu_GujuratiBrahmins_14_allSNPs_freq_table$A1[i]=="C" & (Basu_GujuratiBrahmins_14_allSNPs_freq_table$A2[i]=="T")){
    Basu_GujuratiBrahmins_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_GujuratiBrahmins_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Basu_GujuratiBrahmins_14_allSNPs_freq_table, "Basu_GujuratiBrahmins_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Basu_GujuratiBrahmins_14_allSNPs_freq_table= read.table("Basu_GujuratiBrahmins_14_allSNPs_freq_table", sep=" ")

#Pathak_Gujjar
Pathak_Gujjars_14_allSNPs_freq_table=subset(Pathak_Gujjars_14_allSNPs_freq_table, CHR!=0)
for (i in 1:nrow(Pathak_Gujjars_14_allSNPs_freq_table)){
  if (Pathak_Gujjars_14_allSNPs_freq_table$A1[i]=="G" & (Pathak_Gujjars_14_allSNPs_freq_table$A2[i]=="A"|Pathak_Gujjars_14_allSNPs_freq_table$A2[i]=="T"|Pathak_Gujjars_14_allSNPs_freq_table$A2[i]=="C")){
    Pathak_Gujjars_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Pathak_Gujjars_14_allSNPs_freq_table$MAF[i]
  } else if((Pathak_Gujjars_14_allSNPs_freq_table$A1[i]=="A"|Pathak_Gujjars_14_allSNPs_freq_table$A1[i]=="T"|Pathak_Gujjars_14_allSNPs_freq_table$A1[i]=="C") & (Pathak_Gujjars_14_allSNPs_freq_table$A2[i]=="G")){
    Pathak_Gujjars_14_allSNPs_freq_table$sameallele_freq[i] <- Pathak_Gujjars_14_allSNPs_freq_table$MAF[i]
    
  } else if(Pathak_Gujjars_14_allSNPs_freq_table$A1[i]=="A" & (Pathak_Gujjars_14_allSNPs_freq_table$A2[i]=="T"|Pathak_Gujjars_14_allSNPs_freq_table$A2[i]=="C")){
    Pathak_Gujjars_14_allSNPs_freq_table$sameallele_freq[i] <- Pathak_Gujjars_14_allSNPs_freq_table$MAF[i]
  } else if((Pathak_Gujjars_14_allSNPs_freq_table$A1[i]=="T"|Pathak_Gujjars_14_allSNPs_freq_table$A1[i]=="C") & (Pathak_Gujjars_14_allSNPs_freq_table$A2[i]=="A")){
    Pathak_Gujjars_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Pathak_Gujjars_14_allSNPs_freq_table$MAF[i]
    
  } else if(Pathak_Gujjars_14_allSNPs_freq_table$A1[i]=="T" & (Pathak_Gujjars_14_allSNPs_freq_table$A2[i]=="C")){
    Pathak_Gujjars_14_allSNPs_freq_table$sameallele_freq[i] <- Pathak_Gujjars_14_allSNPs_freq_table$MAF[i]
  } else if(Pathak_Gujjars_14_allSNPs_freq_table$A1[i]=="C" & (Pathak_Gujjars_14_allSNPs_freq_table$A2[i]=="T")){
    Pathak_Gujjars_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Pathak_Gujjars_14_allSNPs_freq_table$MAF[i]
    
  } else if(Pathak_Gujjars_14_allSNPs_freq_table$A1[i]=="0" & (Pathak_Gujjars_14_allSNPs_freq_table$A2[i]=="C"|Pathak_Gujjars_14_allSNPs_freq_table$A2[i]=="G"|Pathak_Gujjars_14_allSNPs_freq_table$A2[i]=="A"|Pathak_Gujjars_14_allSNPs_freq_table$A2[i]=="T")){
    Pathak_Gujjars_14_allSNPs_freq_table$sameallele_freq[i] <- Pathak_Gujjars_14_allSNPs_freq_table$MAF[i]
  } else if((Pathak_Gujjars_14_allSNPs_freq_table$A1[i]=="C"|Pathak_Gujjars_14_allSNPs_freq_table$A1[i]=="G"|Pathak_Gujjars_14_allSNPs_freq_table$A1[i]=="A"|Pathak_Gujjars_14_allSNPs_freq_table$A1[i]=="T") & (Pathak_Gujjars_14_allSNPs_freq_table$A2[i]=="0")){
    Pathak_Gujjars_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Pathak_Gujjars_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Pathak_Gujjars_14_allSNPs_freq_table, "Pathak_Gujjars_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Pathak_Gujjars_14_allSNPs_freq_table= read.table("Pathak_Gujjars_14_allSNPs_freq_table", sep=" ")


#Basu_Ho
for (i in 1:nrow(Basu_Hos_14_allSNPs_freq_table)){
  if (Basu_Hos_14_allSNPs_freq_table$A1[i]=="G" & (Basu_Hos_14_allSNPs_freq_table$A2[i]=="A"|Basu_Hos_14_allSNPs_freq_table$A2[i]=="T"|Basu_Hos_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Hos_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Hos_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Hos_14_allSNPs_freq_table$A1[i]=="A"|Basu_Hos_14_allSNPs_freq_table$A1[i]=="T"|Basu_Hos_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Hos_14_allSNPs_freq_table$A2[i]=="G")){
    Basu_Hos_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Hos_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Hos_14_allSNPs_freq_table$A1[i]=="A" & (Basu_Hos_14_allSNPs_freq_table$A2[i]=="T"|Basu_Hos_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Hos_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Hos_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Hos_14_allSNPs_freq_table$A1[i]=="T"|Basu_Hos_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Hos_14_allSNPs_freq_table$A2[i]=="A")){
    Basu_Hos_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Hos_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Hos_14_allSNPs_freq_table$A1[i]=="T" & (Basu_Hos_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Hos_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Hos_14_allSNPs_freq_table$MAF[i]
  } else if(Basu_Hos_14_allSNPs_freq_table$A1[i]=="C" & (Basu_Hos_14_allSNPs_freq_table$A2[i]=="T")){
    Basu_Hos_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Hos_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Basu_Hos_14_allSNPs_freq_table, "Basu_Hos_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Basu_Hos_14_allSNPs_freq_table= read.table("Basu_Hos_14_allSNPs_freq_table", sep=" ")

#Basu_Irula
for (i in 1:nrow(Basu_Irulas_14_allSNPs_freq_table)){
  if (Basu_Irulas_14_allSNPs_freq_table$A1[i]=="G" & (Basu_Irulas_14_allSNPs_freq_table$A2[i]=="A"|Basu_Irulas_14_allSNPs_freq_table$A2[i]=="T"|Basu_Irulas_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Irulas_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Irulas_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Irulas_14_allSNPs_freq_table$A1[i]=="A"|Basu_Irulas_14_allSNPs_freq_table$A1[i]=="T"|Basu_Irulas_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Irulas_14_allSNPs_freq_table$A2[i]=="G")){
    Basu_Irulas_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Irulas_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Irulas_14_allSNPs_freq_table$A1[i]=="A" & (Basu_Irulas_14_allSNPs_freq_table$A2[i]=="T"|Basu_Irulas_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Irulas_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Irulas_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Irulas_14_allSNPs_freq_table$A1[i]=="T"|Basu_Irulas_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Irulas_14_allSNPs_freq_table$A2[i]=="A")){
    Basu_Irulas_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Irulas_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Irulas_14_allSNPs_freq_table$A1[i]=="T" & (Basu_Irulas_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Irulas_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Irulas_14_allSNPs_freq_table$MAF[i]
  } else if(Basu_Irulas_14_allSNPs_freq_table$A1[i]=="C" & (Basu_Irulas_14_allSNPs_freq_table$A2[i]=="T")){
    Basu_Irulas_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Irulas_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Basu_Irulas_14_allSNPs_freq_table, "Basu_Irulas_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Basu_Irulas_14_allSNPs_freq_table= read.table("Basu_Irulas_14_allSNPs_freq_table", sep=" ")


#Basu_Iyer
for (i in 1:nrow(Basu_Iyers_14_allSNPs_freq_table)){
  if (Basu_Iyers_14_allSNPs_freq_table$A1[i]=="G" & (Basu_Iyers_14_allSNPs_freq_table$A2[i]=="A"|Basu_Iyers_14_allSNPs_freq_table$A2[i]=="T"|Basu_Iyers_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Iyers_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Iyers_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Iyers_14_allSNPs_freq_table$A1[i]=="A"|Basu_Iyers_14_allSNPs_freq_table$A1[i]=="T"|Basu_Iyers_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Iyers_14_allSNPs_freq_table$A2[i]=="G")){
    Basu_Iyers_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Iyers_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Iyers_14_allSNPs_freq_table$A1[i]=="A" & (Basu_Iyers_14_allSNPs_freq_table$A2[i]=="T"|Basu_Iyers_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Iyers_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Iyers_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Iyers_14_allSNPs_freq_table$A1[i]=="T"|Basu_Iyers_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Iyers_14_allSNPs_freq_table$A2[i]=="A")){
    Basu_Iyers_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Iyers_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Iyers_14_allSNPs_freq_table$A1[i]=="T" & (Basu_Iyers_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Iyers_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Iyers_14_allSNPs_freq_table$MAF[i]
  } else if(Basu_Iyers_14_allSNPs_freq_table$A1[i]=="C" & (Basu_Iyers_14_allSNPs_freq_table$A2[i]=="T")){
    Basu_Iyers_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Iyers_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Basu_Iyers_14_allSNPs_freq_table, "Basu_Iyers_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Basu_Iyers_14_allSNPs_freq_table= read.table("Basu_Iyers_14_allSNPs_freq_table", sep=" ")


#Basu_Jamatia
for (i in 1:nrow(Basu_Jamatias_14_allSNPs_freq_table)){
  if (Basu_Jamatias_14_allSNPs_freq_table$A1[i]=="G" & (Basu_Jamatias_14_allSNPs_freq_table$A2[i]=="A"|Basu_Jamatias_14_allSNPs_freq_table$A2[i]=="T"|Basu_Jamatias_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Jamatias_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Jamatias_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Jamatias_14_allSNPs_freq_table$A1[i]=="A"|Basu_Jamatias_14_allSNPs_freq_table$A1[i]=="T"|Basu_Jamatias_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Jamatias_14_allSNPs_freq_table$A2[i]=="G")){
    Basu_Jamatias_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Jamatias_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Jamatias_14_allSNPs_freq_table$A1[i]=="A" & (Basu_Jamatias_14_allSNPs_freq_table$A2[i]=="T"|Basu_Jamatias_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Jamatias_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Jamatias_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Jamatias_14_allSNPs_freq_table$A1[i]=="T"|Basu_Jamatias_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Jamatias_14_allSNPs_freq_table$A2[i]=="A")){
    Basu_Jamatias_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Jamatias_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Jamatias_14_allSNPs_freq_table$A1[i]=="T" & (Basu_Jamatias_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Jamatias_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Jamatias_14_allSNPs_freq_table$MAF[i]
  } else if(Basu_Jamatias_14_allSNPs_freq_table$A1[i]=="C" & (Basu_Jamatias_14_allSNPs_freq_table$A2[i]=="T")){
    Basu_Jamatias_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Jamatias_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Basu_Jamatias_14_allSNPs_freq_table, "Basu_Jamatias_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Basu_Jamatias_14_allSNPs_freq_table= read.table("Basu_Jamatias_14_allSNPs_freq_table", sep=" ")

#Basu_Jarawa
for (i in 1:nrow(Basu_Jarawas_14_allSNPs_freq_table)){
  if (Basu_Jarawas_14_allSNPs_freq_table$A1[i]=="G" & (Basu_Jarawas_14_allSNPs_freq_table$A2[i]=="A"|Basu_Jarawas_14_allSNPs_freq_table$A2[i]=="T"|Basu_Jarawas_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Jarawas_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Jarawas_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Jarawas_14_allSNPs_freq_table$A1[i]=="A"|Basu_Jarawas_14_allSNPs_freq_table$A1[i]=="T"|Basu_Jarawas_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Jarawas_14_allSNPs_freq_table$A2[i]=="G")){
    Basu_Jarawas_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Jarawas_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Jarawas_14_allSNPs_freq_table$A1[i]=="A" & (Basu_Jarawas_14_allSNPs_freq_table$A2[i]=="T"|Basu_Jarawas_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Jarawas_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Jarawas_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Jarawas_14_allSNPs_freq_table$A1[i]=="T"|Basu_Jarawas_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Jarawas_14_allSNPs_freq_table$A2[i]=="A")){
    Basu_Jarawas_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Jarawas_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Jarawas_14_allSNPs_freq_table$A1[i]=="T" & (Basu_Jarawas_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Jarawas_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Jarawas_14_allSNPs_freq_table$MAF[i]
  } else if(Basu_Jarawas_14_allSNPs_freq_table$A1[i]=="C" & (Basu_Jarawas_14_allSNPs_freq_table$A2[i]=="T")){
    Basu_Jarawas_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Jarawas_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Basu_Jarawas_14_allSNPs_freq_table, "Basu_Jarawas_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Basu_Jarawas_14_allSNPs_freq_table= read.table("Basu_Jarawas_14_allSNPs_freq_table", sep=" ")


#Basu_Kadar
for (i in 1:nrow(Basu_Kadars_14_allSNPs_freq_table)){
  if (Basu_Kadars_14_allSNPs_freq_table$A1[i]=="G" & (Basu_Kadars_14_allSNPs_freq_table$A2[i]=="A"|Basu_Kadars_14_allSNPs_freq_table$A2[i]=="T"|Basu_Kadars_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Kadars_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Kadars_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Kadars_14_allSNPs_freq_table$A1[i]=="A"|Basu_Kadars_14_allSNPs_freq_table$A1[i]=="T"|Basu_Kadars_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Kadars_14_allSNPs_freq_table$A2[i]=="G")){
    Basu_Kadars_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Kadars_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Kadars_14_allSNPs_freq_table$A1[i]=="A" & (Basu_Kadars_14_allSNPs_freq_table$A2[i]=="T"|Basu_Kadars_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Kadars_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Kadars_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Kadars_14_allSNPs_freq_table$A1[i]=="T"|Basu_Kadars_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Kadars_14_allSNPs_freq_table$A2[i]=="A")){
    Basu_Kadars_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Kadars_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Kadars_14_allSNPs_freq_table$A1[i]=="T" & (Basu_Kadars_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Kadars_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Kadars_14_allSNPs_freq_table$MAF[i]
  } else if(Basu_Kadars_14_allSNPs_freq_table$A1[i]=="C" & (Basu_Kadars_14_allSNPs_freq_table$A2[i]=="T")){
    Basu_Kadars_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Kadars_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Basu_Kadars_14_allSNPs_freq_table, "Basu_Kadars_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Basu_Kadars_14_allSNPs_freq_table= read.table("Basu_Kadars_14_allSNPs_freq_table", sep=" ")


#Pathak_Kamboj
Pathak_Kambojs_14_allSNPs_freq_table=subset(Pathak_Kambojs_14_allSNPs_freq_table, CHR!=0)
for (i in 1:nrow(Pathak_Kambojs_14_allSNPs_freq_table)){
  if (Pathak_Kambojs_14_allSNPs_freq_table$A1[i]=="G" & (Pathak_Kambojs_14_allSNPs_freq_table$A2[i]=="A"|Pathak_Kambojs_14_allSNPs_freq_table$A2[i]=="T"|Pathak_Kambojs_14_allSNPs_freq_table$A2[i]=="C")){
    Pathak_Kambojs_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Pathak_Kambojs_14_allSNPs_freq_table$MAF[i]
  } else if((Pathak_Kambojs_14_allSNPs_freq_table$A1[i]=="A"|Pathak_Kambojs_14_allSNPs_freq_table$A1[i]=="T"|Pathak_Kambojs_14_allSNPs_freq_table$A1[i]=="C") & (Pathak_Kambojs_14_allSNPs_freq_table$A2[i]=="G")){
    Pathak_Kambojs_14_allSNPs_freq_table$sameallele_freq[i] <- Pathak_Kambojs_14_allSNPs_freq_table$MAF[i]
    
  } else if(Pathak_Kambojs_14_allSNPs_freq_table$A1[i]=="A" & (Pathak_Kambojs_14_allSNPs_freq_table$A2[i]=="T"|Pathak_Kambojs_14_allSNPs_freq_table$A2[i]=="C")){
    Pathak_Kambojs_14_allSNPs_freq_table$sameallele_freq[i] <- Pathak_Kambojs_14_allSNPs_freq_table$MAF[i]
  } else if((Pathak_Kambojs_14_allSNPs_freq_table$A1[i]=="T"|Pathak_Kambojs_14_allSNPs_freq_table$A1[i]=="C") & (Pathak_Kambojs_14_allSNPs_freq_table$A2[i]=="A")){
    Pathak_Kambojs_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Pathak_Kambojs_14_allSNPs_freq_table$MAF[i]
    
  } else if(Pathak_Kambojs_14_allSNPs_freq_table$A1[i]=="T" & (Pathak_Kambojs_14_allSNPs_freq_table$A2[i]=="C")){
    Pathak_Kambojs_14_allSNPs_freq_table$sameallele_freq[i] <- Pathak_Kambojs_14_allSNPs_freq_table$MAF[i]
  } else if(Pathak_Kambojs_14_allSNPs_freq_table$A1[i]=="C" & (Pathak_Kambojs_14_allSNPs_freq_table$A2[i]=="T")){
    Pathak_Kambojs_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Pathak_Kambojs_14_allSNPs_freq_table$MAF[i]
    
  } else if(Pathak_Kambojs_14_allSNPs_freq_table$A1[i]=="0" & (Pathak_Kambojs_14_allSNPs_freq_table$A2[i]=="C"|Pathak_Kambojs_14_allSNPs_freq_table$A2[i]=="G"|Pathak_Kambojs_14_allSNPs_freq_table$A2[i]=="A"|Pathak_Kambojs_14_allSNPs_freq_table$A2[i]=="T")){
    Pathak_Kambojs_14_allSNPs_freq_table$sameallele_freq[i] <- Pathak_Kambojs_14_allSNPs_freq_table$MAF[i]
  } else if((Pathak_Kambojs_14_allSNPs_freq_table$A1[i]=="C"|Pathak_Kambojs_14_allSNPs_freq_table$A1[i]=="G"|Pathak_Kambojs_14_allSNPs_freq_table$A1[i]=="A"|Pathak_Kambojs_14_allSNPs_freq_table$A1[i]=="T") & (Pathak_Kambojs_14_allSNPs_freq_table$A2[i]=="0")){
    Pathak_Kambojs_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Pathak_Kambojs_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Pathak_Kambojs_14_allSNPs_freq_table, "Pathak_Kambojs_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Pathak_Kambojs_14_allSNPs_freq_table= read.table("Pathak_Kambojs_14_allSNPs_freq_table", sep=" ")

#Basu_Khatri
for (i in 1:nrow(Basu_Khatris_14_allSNPs_freq_table)){
  if (Basu_Khatris_14_allSNPs_freq_table$A1[i]=="G" & (Basu_Khatris_14_allSNPs_freq_table$A2[i]=="A"|Basu_Khatris_14_allSNPs_freq_table$A2[i]=="T"|Basu_Khatris_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Khatris_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Khatris_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Khatris_14_allSNPs_freq_table$A1[i]=="A"|Basu_Khatris_14_allSNPs_freq_table$A1[i]=="T"|Basu_Khatris_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Khatris_14_allSNPs_freq_table$A2[i]=="G")){
    Basu_Khatris_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Khatris_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Khatris_14_allSNPs_freq_table$A1[i]=="A" & (Basu_Khatris_14_allSNPs_freq_table$A2[i]=="T"|Basu_Khatris_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Khatris_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Khatris_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Khatris_14_allSNPs_freq_table$A1[i]=="T"|Basu_Khatris_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Khatris_14_allSNPs_freq_table$A2[i]=="A")){
    Basu_Khatris_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Khatris_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Khatris_14_allSNPs_freq_table$A1[i]=="T" & (Basu_Khatris_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Khatris_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Khatris_14_allSNPs_freq_table$MAF[i]
  } else if(Basu_Khatris_14_allSNPs_freq_table$A1[i]=="C" & (Basu_Khatris_14_allSNPs_freq_table$A2[i]=="T")){
    Basu_Khatris_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Khatris_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Basu_Khatris_14_allSNPs_freq_table, "Basu_Khatris_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Basu_Khatris_14_allSNPs_freq_table= read.table("Basu_Khatris_14_allSNPs_freq_table", sep=" ")

#Basu_Korwa
for (i in 1:nrow(Basu_Korwas_14_allSNPs_freq_table)){
  if (Basu_Korwas_14_allSNPs_freq_table$A1[i]=="G" & (Basu_Korwas_14_allSNPs_freq_table$A2[i]=="A"|Basu_Korwas_14_allSNPs_freq_table$A2[i]=="T"|Basu_Korwas_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Korwas_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Korwas_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Korwas_14_allSNPs_freq_table$A1[i]=="A"|Basu_Korwas_14_allSNPs_freq_table$A1[i]=="T"|Basu_Korwas_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Korwas_14_allSNPs_freq_table$A2[i]=="G")){
    Basu_Korwas_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Korwas_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Korwas_14_allSNPs_freq_table$A1[i]=="A" & (Basu_Korwas_14_allSNPs_freq_table$A2[i]=="T"|Basu_Korwas_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Korwas_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Korwas_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Korwas_14_allSNPs_freq_table$A1[i]=="T"|Basu_Korwas_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Korwas_14_allSNPs_freq_table$A2[i]=="A")){
    Basu_Korwas_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Korwas_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Korwas_14_allSNPs_freq_table$A1[i]=="T" & (Basu_Korwas_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Korwas_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Korwas_14_allSNPs_freq_table$MAF[i]
  } else if(Basu_Korwas_14_allSNPs_freq_table$A1[i]=="C" & (Basu_Korwas_14_allSNPs_freq_table$A2[i]=="T")){
    Basu_Korwas_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Korwas_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Basu_Korwas_14_allSNPs_freq_table, "Basu_Korwas_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Basu_Korwas_14_allSNPs_freq_table= read.table("Basu_Korwas_14_allSNPs_freq_table", sep=" ")

#Basu_ManipuriBrahmin
for (i in 1:nrow(Basu_ManipuriBrahmins_14_allSNPs_freq_table)){
  if (Basu_ManipuriBrahmins_14_allSNPs_freq_table$A1[i]=="G" & (Basu_ManipuriBrahmins_14_allSNPs_freq_table$A2[i]=="A"|Basu_ManipuriBrahmins_14_allSNPs_freq_table$A2[i]=="T"|Basu_ManipuriBrahmins_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_ManipuriBrahmins_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_ManipuriBrahmins_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_ManipuriBrahmins_14_allSNPs_freq_table$A1[i]=="A"|Basu_ManipuriBrahmins_14_allSNPs_freq_table$A1[i]=="T"|Basu_ManipuriBrahmins_14_allSNPs_freq_table$A1[i]=="C") & (Basu_ManipuriBrahmins_14_allSNPs_freq_table$A2[i]=="G")){
    Basu_ManipuriBrahmins_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_ManipuriBrahmins_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_ManipuriBrahmins_14_allSNPs_freq_table$A1[i]=="A" & (Basu_ManipuriBrahmins_14_allSNPs_freq_table$A2[i]=="T"|Basu_ManipuriBrahmins_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_ManipuriBrahmins_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_ManipuriBrahmins_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_ManipuriBrahmins_14_allSNPs_freq_table$A1[i]=="T"|Basu_ManipuriBrahmins_14_allSNPs_freq_table$A1[i]=="C") & (Basu_ManipuriBrahmins_14_allSNPs_freq_table$A2[i]=="A")){
    Basu_ManipuriBrahmins_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_ManipuriBrahmins_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_ManipuriBrahmins_14_allSNPs_freq_table$A1[i]=="T" & (Basu_ManipuriBrahmins_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_ManipuriBrahmins_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_ManipuriBrahmins_14_allSNPs_freq_table$MAF[i]
  } else if(Basu_ManipuriBrahmins_14_allSNPs_freq_table$A1[i]=="C" & (Basu_ManipuriBrahmins_14_allSNPs_freq_table$A2[i]=="T")){
    Basu_ManipuriBrahmins_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_ManipuriBrahmins_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Basu_ManipuriBrahmins_14_allSNPs_freq_table, "Basu_ManipuriBrahmins_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Basu_ManipuriBrahmins_14_allSNPs_freq_table= read.table("Basu_ManipuriBrahmins_14_allSNPs_freq_table", sep=" ")

#Basu_Onge
for (i in 1:nrow(Basu_Onges_14_allSNPs_freq_table)){
  if (Basu_Onges_14_allSNPs_freq_table$A1[i]=="G" & (Basu_Onges_14_allSNPs_freq_table$A2[i]=="A"|Basu_Onges_14_allSNPs_freq_table$A2[i]=="T"|Basu_Onges_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Onges_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Onges_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Onges_14_allSNPs_freq_table$A1[i]=="A"|Basu_Onges_14_allSNPs_freq_table$A1[i]=="T"|Basu_Onges_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Onges_14_allSNPs_freq_table$A2[i]=="G")){
    Basu_Onges_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Onges_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Onges_14_allSNPs_freq_table$A1[i]=="A" & (Basu_Onges_14_allSNPs_freq_table$A2[i]=="T"|Basu_Onges_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Onges_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Onges_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Onges_14_allSNPs_freq_table$A1[i]=="T"|Basu_Onges_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Onges_14_allSNPs_freq_table$A2[i]=="A")){
    Basu_Onges_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Onges_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Onges_14_allSNPs_freq_table$A1[i]=="T" & (Basu_Onges_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Onges_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Onges_14_allSNPs_freq_table$MAF[i]
  } else if(Basu_Onges_14_allSNPs_freq_table$A1[i]=="C" & (Basu_Onges_14_allSNPs_freq_table$A2[i]=="T")){
    Basu_Onges_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Onges_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Basu_Onges_14_allSNPs_freq_table, "Basu_Onges_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Basu_Onges_14_allSNPs_freq_table= read.table("Basu_Onges_14_allSNPs_freq_table", sep=" ")


#Basu_Pallan
for (i in 1:nrow(Basu_Pallans_14_allSNPs_freq_table)){
  if (Basu_Pallans_14_allSNPs_freq_table$A1[i]=="G" & (Basu_Pallans_14_allSNPs_freq_table$A2[i]=="A"|Basu_Pallans_14_allSNPs_freq_table$A2[i]=="T"|Basu_Pallans_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Pallans_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Pallans_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Pallans_14_allSNPs_freq_table$A1[i]=="A"|Basu_Pallans_14_allSNPs_freq_table$A1[i]=="T"|Basu_Pallans_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Pallans_14_allSNPs_freq_table$A2[i]=="G")){
    Basu_Pallans_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Pallans_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Pallans_14_allSNPs_freq_table$A1[i]=="A" & (Basu_Pallans_14_allSNPs_freq_table$A2[i]=="T"|Basu_Pallans_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Pallans_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Pallans_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Pallans_14_allSNPs_freq_table$A1[i]=="T"|Basu_Pallans_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Pallans_14_allSNPs_freq_table$A2[i]=="A")){
    Basu_Pallans_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Pallans_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Pallans_14_allSNPs_freq_table$A1[i]=="T" & (Basu_Pallans_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Pallans_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Pallans_14_allSNPs_freq_table$MAF[i]
  } else if(Basu_Pallans_14_allSNPs_freq_table$A1[i]=="C" & (Basu_Pallans_14_allSNPs_freq_table$A2[i]=="T")){
    Basu_Pallans_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Pallans_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Basu_Pallans_14_allSNPs_freq_table, "Basu_Pallans_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Basu_Pallans_14_allSNPs_freq_table= read.table("Basu_Pallans_14_allSNPs_freq_table", sep=" ")


#Basu_Paniya
for (i in 1:nrow(Basu_Paniyas_14_allSNPs_freq_table)){
  if (Basu_Paniyas_14_allSNPs_freq_table$A1[i]=="G" & (Basu_Paniyas_14_allSNPs_freq_table$A2[i]=="A"|Basu_Paniyas_14_allSNPs_freq_table$A2[i]=="T"|Basu_Paniyas_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Paniyas_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Paniyas_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Paniyas_14_allSNPs_freq_table$A1[i]=="A"|Basu_Paniyas_14_allSNPs_freq_table$A1[i]=="T"|Basu_Paniyas_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Paniyas_14_allSNPs_freq_table$A2[i]=="G")){
    Basu_Paniyas_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Paniyas_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Paniyas_14_allSNPs_freq_table$A1[i]=="A" & (Basu_Paniyas_14_allSNPs_freq_table$A2[i]=="T"|Basu_Paniyas_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Paniyas_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Paniyas_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Paniyas_14_allSNPs_freq_table$A1[i]=="T"|Basu_Paniyas_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Paniyas_14_allSNPs_freq_table$A2[i]=="A")){
    Basu_Paniyas_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Paniyas_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Paniyas_14_allSNPs_freq_table$A1[i]=="T" & (Basu_Paniyas_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Paniyas_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Paniyas_14_allSNPs_freq_table$MAF[i]
  } else if(Basu_Paniyas_14_allSNPs_freq_table$A1[i]=="C" & (Basu_Paniyas_14_allSNPs_freq_table$A2[i]=="T")){
    Basu_Paniyas_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Paniyas_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Basu_Paniyas_14_allSNPs_freq_table, "Basu_Paniyas_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Basu_Paniyas_14_allSNPs_freq_table= read.table("Basu_Paniyas_14_allSNPs_freq_table", sep=" ")

#Pathak_Ror
Pathak_Rors_14_allSNPs_freq_table=subset(Pathak_Rors_14_allSNPs_freq_table, CHR!=0)
for (i in 1:nrow(Pathak_Rors_14_allSNPs_freq_table)){
  if (Pathak_Rors_14_allSNPs_freq_table$A1[i]=="G" & (Pathak_Rors_14_allSNPs_freq_table$A2[i]=="A"|Pathak_Rors_14_allSNPs_freq_table$A2[i]=="T"|Pathak_Rors_14_allSNPs_freq_table$A2[i]=="C")){
    Pathak_Rors_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Pathak_Rors_14_allSNPs_freq_table$MAF[i]
  } else if((Pathak_Rors_14_allSNPs_freq_table$A1[i]=="A"|Pathak_Rors_14_allSNPs_freq_table$A1[i]=="T"|Pathak_Rors_14_allSNPs_freq_table$A1[i]=="C") & (Pathak_Rors_14_allSNPs_freq_table$A2[i]=="G")){
    Pathak_Rors_14_allSNPs_freq_table$sameallele_freq[i] <- Pathak_Rors_14_allSNPs_freq_table$MAF[i]
    
  } else if(Pathak_Rors_14_allSNPs_freq_table$A1[i]=="A" & (Pathak_Rors_14_allSNPs_freq_table$A2[i]=="T"|Pathak_Rors_14_allSNPs_freq_table$A2[i]=="C")){
    Pathak_Rors_14_allSNPs_freq_table$sameallele_freq[i] <- Pathak_Rors_14_allSNPs_freq_table$MAF[i]
  } else if((Pathak_Rors_14_allSNPs_freq_table$A1[i]=="T"|Pathak_Rors_14_allSNPs_freq_table$A1[i]=="C") & (Pathak_Rors_14_allSNPs_freq_table$A2[i]=="A")){
    Pathak_Rors_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Pathak_Rors_14_allSNPs_freq_table$MAF[i]
    
  } else if(Pathak_Rors_14_allSNPs_freq_table$A1[i]=="T" & (Pathak_Rors_14_allSNPs_freq_table$A2[i]=="C")){
    Pathak_Rors_14_allSNPs_freq_table$sameallele_freq[i] <- Pathak_Rors_14_allSNPs_freq_table$MAF[i]
  } else if(Pathak_Rors_14_allSNPs_freq_table$A1[i]=="C" & (Pathak_Rors_14_allSNPs_freq_table$A2[i]=="T")){
    Pathak_Rors_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Pathak_Rors_14_allSNPs_freq_table$MAF[i]
    
  } else if(Pathak_Rors_14_allSNPs_freq_table$A1[i]=="0" & (Pathak_Rors_14_allSNPs_freq_table$A2[i]=="C"|Pathak_Rors_14_allSNPs_freq_table$A2[i]=="G"|Pathak_Rors_14_allSNPs_freq_table$A2[i]=="A"|Pathak_Rors_14_allSNPs_freq_table$A2[i]=="T")){
    Pathak_Rors_14_allSNPs_freq_table$sameallele_freq[i] <- Pathak_Rors_14_allSNPs_freq_table$MAF[i]
  } else if((Pathak_Rors_14_allSNPs_freq_table$A1[i]=="C"|Pathak_Rors_14_allSNPs_freq_table$A1[i]=="G"|Pathak_Rors_14_allSNPs_freq_table$A1[i]=="A"|Pathak_Rors_14_allSNPs_freq_table$A1[i]=="T") & (Pathak_Rors_14_allSNPs_freq_table$A2[i]=="0")){
    Pathak_Rors_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Pathak_Rors_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Pathak_Rors_14_allSNPs_freq_table, "Pathak_Rors_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Pathak_Rors_14_allSNPs_freq_table= read.table("Pathak_Rors_14_allSNPs_freq_table", sep=" ")

#Basu_Santal
for (i in 1:nrow(Basu_Santals_14_allSNPs_freq_table)){
  if (Basu_Santals_14_allSNPs_freq_table$A1[i]=="G" & (Basu_Santals_14_allSNPs_freq_table$A2[i]=="A"|Basu_Santals_14_allSNPs_freq_table$A2[i]=="T"|Basu_Santals_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Santals_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Santals_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Santals_14_allSNPs_freq_table$A1[i]=="A"|Basu_Santals_14_allSNPs_freq_table$A1[i]=="T"|Basu_Santals_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Santals_14_allSNPs_freq_table$A2[i]=="G")){
    Basu_Santals_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Santals_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Santals_14_allSNPs_freq_table$A1[i]=="A" & (Basu_Santals_14_allSNPs_freq_table$A2[i]=="T"|Basu_Santals_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Santals_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Santals_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Santals_14_allSNPs_freq_table$A1[i]=="T"|Basu_Santals_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Santals_14_allSNPs_freq_table$A2[i]=="A")){
    Basu_Santals_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Santals_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Santals_14_allSNPs_freq_table$A1[i]=="T" & (Basu_Santals_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Santals_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Santals_14_allSNPs_freq_table$MAF[i]
  } else if(Basu_Santals_14_allSNPs_freq_table$A1[i]=="C" & (Basu_Santals_14_allSNPs_freq_table$A2[i]=="T")){
    Basu_Santals_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Santals_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Basu_Santals_14_allSNPs_freq_table, "Basu_Santals_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Basu_Santals_14_allSNPs_freq_table= read.table("Basu_Santals_14_allSNPs_freq_table", sep=" ")


#Basu_Tripuri
for (i in 1:nrow(Basu_Tripuris_14_allSNPs_freq_table)){
  if (Basu_Tripuris_14_allSNPs_freq_table$A1[i]=="G" & (Basu_Tripuris_14_allSNPs_freq_table$A2[i]=="A"|Basu_Tripuris_14_allSNPs_freq_table$A2[i]=="T"|Basu_Tripuris_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Tripuris_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Tripuris_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Tripuris_14_allSNPs_freq_table$A1[i]=="A"|Basu_Tripuris_14_allSNPs_freq_table$A1[i]=="T"|Basu_Tripuris_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Tripuris_14_allSNPs_freq_table$A2[i]=="G")){
    Basu_Tripuris_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Tripuris_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Tripuris_14_allSNPs_freq_table$A1[i]=="A" & (Basu_Tripuris_14_allSNPs_freq_table$A2[i]=="T"|Basu_Tripuris_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Tripuris_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Tripuris_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Tripuris_14_allSNPs_freq_table$A1[i]=="T"|Basu_Tripuris_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Tripuris_14_allSNPs_freq_table$A2[i]=="A")){
    Basu_Tripuris_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Tripuris_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Tripuris_14_allSNPs_freq_table$A1[i]=="T" & (Basu_Tripuris_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Tripuris_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Tripuris_14_allSNPs_freq_table$MAF[i]
  } else if(Basu_Tripuris_14_allSNPs_freq_table$A1[i]=="C" & (Basu_Tripuris_14_allSNPs_freq_table$A2[i]=="T")){
    Basu_Tripuris_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Tripuris_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Basu_Tripuris_14_allSNPs_freq_table, "Basu_Tripuris_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Basu_Tripuris_14_allSNPs_freq_table= read.table("Basu_Tripuris_14_allSNPs_freq_table", sep=" ")


#Basu_Tharu
for (i in 1:nrow(Basu_Tharus_14_allSNPs_freq_table)){
  if (Basu_Tharus_14_allSNPs_freq_table$A1[i]=="G" & (Basu_Tharus_14_allSNPs_freq_table$A2[i]=="A"|Basu_Tharus_14_allSNPs_freq_table$A2[i]=="T"|Basu_Tharus_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Tharus_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Tharus_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Tharus_14_allSNPs_freq_table$A1[i]=="A"|Basu_Tharus_14_allSNPs_freq_table$A1[i]=="T"|Basu_Tharus_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Tharus_14_allSNPs_freq_table$A2[i]=="G")){
    Basu_Tharus_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Tharus_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Tharus_14_allSNPs_freq_table$A1[i]=="A" & (Basu_Tharus_14_allSNPs_freq_table$A2[i]=="T"|Basu_Tharus_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Tharus_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Tharus_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_Tharus_14_allSNPs_freq_table$A1[i]=="T"|Basu_Tharus_14_allSNPs_freq_table$A1[i]=="C") & (Basu_Tharus_14_allSNPs_freq_table$A2[i]=="A")){
    Basu_Tharus_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Tharus_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_Tharus_14_allSNPs_freq_table$A1[i]=="T" & (Basu_Tharus_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_Tharus_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_Tharus_14_allSNPs_freq_table$MAF[i]
  } else if(Basu_Tharus_14_allSNPs_freq_table$A1[i]=="C" & (Basu_Tharus_14_allSNPs_freq_table$A2[i]=="T")){
    Basu_Tharus_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_Tharus_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Basu_Tharus_14_allSNPs_freq_table, "Basu_Tharus_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Basu_Tharus_14_allSNPs_freq_table= read.table("Basu_Tharus_14_allSNPs_freq_table", sep=" ")


#Basu_WestBengalBrahmin
for (i in 1:nrow(Basu_WestBengalBrahmins_14_allSNPs_freq_table)){
  if (Basu_WestBengalBrahmins_14_allSNPs_freq_table$A1[i]=="G" & (Basu_WestBengalBrahmins_14_allSNPs_freq_table$A2[i]=="A"|Basu_WestBengalBrahmins_14_allSNPs_freq_table$A2[i]=="T"|Basu_WestBengalBrahmins_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_WestBengalBrahmins_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_WestBengalBrahmins_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_WestBengalBrahmins_14_allSNPs_freq_table$A1[i]=="A"|Basu_WestBengalBrahmins_14_allSNPs_freq_table$A1[i]=="T"|Basu_WestBengalBrahmins_14_allSNPs_freq_table$A1[i]=="C") & (Basu_WestBengalBrahmins_14_allSNPs_freq_table$A2[i]=="G")){
    Basu_WestBengalBrahmins_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_WestBengalBrahmins_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_WestBengalBrahmins_14_allSNPs_freq_table$A1[i]=="A" & (Basu_WestBengalBrahmins_14_allSNPs_freq_table$A2[i]=="T"|Basu_WestBengalBrahmins_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_WestBengalBrahmins_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_WestBengalBrahmins_14_allSNPs_freq_table$MAF[i]
  } else if((Basu_WestBengalBrahmins_14_allSNPs_freq_table$A1[i]=="T"|Basu_WestBengalBrahmins_14_allSNPs_freq_table$A1[i]=="C") & (Basu_WestBengalBrahmins_14_allSNPs_freq_table$A2[i]=="A")){
    Basu_WestBengalBrahmins_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_WestBengalBrahmins_14_allSNPs_freq_table$MAF[i]
    
  } else if(Basu_WestBengalBrahmins_14_allSNPs_freq_table$A1[i]=="T" & (Basu_WestBengalBrahmins_14_allSNPs_freq_table$A2[i]=="C")){
    Basu_WestBengalBrahmins_14_allSNPs_freq_table$sameallele_freq[i] <- Basu_WestBengalBrahmins_14_allSNPs_freq_table$MAF[i]
  } else if(Basu_WestBengalBrahmins_14_allSNPs_freq_table$A1[i]=="C" & (Basu_WestBengalBrahmins_14_allSNPs_freq_table$A2[i]=="T")){
    Basu_WestBengalBrahmins_14_allSNPs_freq_table$sameallele_freq[i] <- 1-Basu_WestBengalBrahmins_14_allSNPs_freq_table$MAF[i]
  }
}
write.table(Basu_WestBengalBrahmins_14_allSNPs_freq_table, "Basu_WestBengalBrahmins_14_allSNPs_freq_table", sep=" ", col.names = TRUE)
Basu_WestBengalBrahmins_14_allSNPs_freq_table= read.table("Basu_WestBengalBrahmins_14_allSNPs_freq_table", sep=" ")

#make sure all correct tables are loaded now
Basu_Birhors_14_allSNPs_freq_table= read.table("Basu_Birhors_14_allSNPs_freq_table", sep=" ")
Basu_Gonds_14_allSNPs_freq_table= read.table("Basu_Gonds_14_allSNPs_freq_table", sep=" ")
Basu_GujuratiBrahmins_14_allSNPs_freq_table= read.table("Basu_GujuratiBrahmins_14_allSNPs_freq_table", sep=" ")
Pathak_Gujjars_14_allSNPs_freq_table= read.table("Pathak_Gujjars_14_allSNPs_freq_table", sep=" ")
Basu_Hos_14_allSNPs_freq_table= read.table("Basu_Hos_14_allSNPs_freq_table", sep=" ")
Basu_Irulas_14_allSNPs_freq_table= read.table("Basu_Irulas_14_allSNPs_freq_table", sep=" ")
Basu_Iyers_14_allSNPs_freq_table= read.table("Basu_Iyers_14_allSNPs_freq_table", sep=" ")
Basu_Jamatias_14_allSNPs_freq_table= read.table("Basu_Jamatias_14_allSNPs_freq_table", sep=" ")
Basu_Jarawas_14_allSNPs_freq_table= read.table("Basu_Jarawas_14_allSNPs_freq_table", sep=" ")
Basu_Kadars_14_allSNPs_freq_table= read.table("Basu_Kadars_14_allSNPs_freq_table", sep=" ")
Pathak_Kambojs_14_allSNPs_freq_table= read.table("Pathak_Kambojs_14_allSNPs_freq_table", sep=" ")
Basu_Khatris_14_allSNPs_freq_table= read.table("Basu_Khatris_14_allSNPs_freq_table", sep=" ")
Basu_Korwas_14_allSNPs_freq_table= read.table("Basu_Korwas_14_allSNPs_freq_table", sep=" ")
Basu_ManipuriBrahmins_14_allSNPs_freq_table= read.table("Basu_ManipuriBrahmins_14_allSNPs_freq_table", sep=" ")
Basu_Onges_14_allSNPs_freq_table= read.table("Basu_Onges_14_allSNPs_freq_table", sep=" ")
Basu_Pallans_14_allSNPs_freq_table= read.table("Basu_Pallans_14_allSNPs_freq_table", sep=" ")
Basu_Paniyas_14_allSNPs_freq_table= read.table("Basu_Paniyas_14_allSNPs_freq_table", sep=" ")
Pathak_Rors_14_allSNPs_freq_table= read.table("Pathak_Rors_14_allSNPs_freq_table", sep=" ")
Basu_Santals_14_allSNPs_freq_table= read.table("Basu_Santals_14_allSNPs_freq_table", sep=" ")
Basu_Tripuris_14_allSNPs_freq_table= read.table("Basu_Tripuris_14_allSNPs_freq_table", sep=" ")
Basu_Tharus_14_allSNPs_freq_table= read.table("Basu_Tharus_14_allSNPs_freq_table", sep=" ")
Basu_WestBengalBrahmins_14_allSNPs_freq_table= read.table("Basu_WestBengalBrahmins_14_allSNPs_freq_table", sep=" ")

#drop columns in each subpop 
Basu_Birhors_14_allSNPs_freq_table=Basu_Birhors_14_allSNPs_freq_table[, c(1,2,7)]
Basu_Gonds_14_allSNPs_freq_table=Basu_Gonds_14_allSNPs_freq_table[, c(1,2,7)]
Basu_GujuratiBrahmins_14_allSNPs_freq_table=Basu_GujuratiBrahmins_14_allSNPs_freq_table[, c(1,2,7)]
Pathak_Gujjars_14_allSNPs_freq_table=Pathak_Gujjars_14_allSNPs_freq_table[, c(1,2,7)]
Basu_Hos_14_allSNPs_freq_table=Basu_Hos_14_allSNPs_freq_table[, c(1,2,7)]
Basu_Irulas_14_allSNPs_freq_table=Basu_Irulas_14_allSNPs_freq_table[, c(1,2,7)]
Basu_Iyers_14_allSNPs_freq_table=Basu_Iyers_14_allSNPs_freq_table[, c(1,2,7)]
Basu_Jamatias_14_allSNPs_freq_table=Basu_Jamatias_14_allSNPs_freq_table[, c(1,2,7)]
Basu_Jarawas_14_allSNPs_freq_table=Basu_Jarawas_14_allSNPs_freq_table[, c(1,2,7)]
Basu_Kadars_14_allSNPs_freq_table=Basu_Kadars_14_allSNPs_freq_table[, c(1,2,7)]
Pathak_Kambojs_14_allSNPs_freq_table=Pathak_Kambojs_14_allSNPs_freq_table[, c(1,2,7)]
Basu_Korwas_14_allSNPs_freq_table=Basu_Korwas_14_allSNPs_freq_table[, c(1,2,7)]
Basu_Khatris_14_allSNPs_freq_table=Basu_Khatris_14_allSNPs_freq_table[, c(1,2,7)]
Basu_ManipuriBrahmins_14_allSNPs_freq_table=Basu_ManipuriBrahmins_14_allSNPs_freq_table[, c(1,2,7)]
Basu_Onges_14_allSNPs_freq_table=Basu_Onges_14_allSNPs_freq_table[, c(1,2,7)]
Basu_Pallans_14_allSNPs_freq_table=Basu_Pallans_14_allSNPs_freq_table[, c(1,2,7)]
Basu_Paniyas_14_allSNPs_freq_table=Basu_Paniyas_14_allSNPs_freq_table[, c(1,2,7)]
Pathak_Rors_14_allSNPs_freq_table=Pathak_Rors_14_allSNPs_freq_table[, c(1,2,7)]
Basu_Santals_14_allSNPs_freq_table=Basu_Santals_14_allSNPs_freq_table[, c(1,2,7)]
Basu_Tripuris_14_allSNPs_freq_table=Basu_Tripuris_14_allSNPs_freq_table[, c(1,2,7)]
Basu_Tharus_14_allSNPs_freq_table=Basu_Tharus_14_allSNPs_freq_table[, c(1,2,7)]
Basu_WestBengalBrahmins_14_allSNPs_freq_table=Basu_WestBengalBrahmins_14_allSNPs_freq_table[, c(1,2,7)]
#rename columns in each subgroup
colnames(Basu_Birhors_14_allSNPs_freq_table)=c("CHR", "SNP", "Birhor_sameallele_freq")
colnames(Basu_Gonds_14_allSNPs_freq_table)=c("CHR", "SNP", "Gond_sameallele_freq")
colnames(Basu_GujuratiBrahmins_14_allSNPs_freq_table)=c("CHR", "SNP", "GujuratiBrahmin_sameallele_freq")
colnames(Pathak_Gujjars_14_allSNPs_freq_table)=c("CHR", "SNP", "Gujjar_sameallele_freq")
colnames(Basu_Hos_14_allSNPs_freq_table)=c("CHR", "SNP", "Ho_sameallele_freq")
colnames(Basu_Irulas_14_allSNPs_freq_table)=c("CHR", "SNP", "Irula_sameallele_freq")
colnames(Basu_Iyers_14_allSNPs_freq_table)=c("CHR", "SNP", "Iyer_sameallele_freq")
colnames(Basu_Jamatias_14_allSNPs_freq_table)=c("CHR", "SNP", "Jamatia_sameallele_freq")
colnames(Basu_Jarawas_14_allSNPs_freq_table)=c("CHR", "SNP", "Jarawa_sameallele_freq")
colnames(Basu_Kadars_14_allSNPs_freq_table)=c("CHR", "SNP", "Kadar_sameallele_freq")
colnames(Basu_Khatris_14_allSNPs_freq_table)=c("CHR", "SNP", "Khatri_sameallele_freq")
colnames(Basu_Korwas_14_allSNPs_freq_table)=c("CHR", "SNP", "Korwa_sameallele_freq")
colnames(Pathak_Kambojs_14_allSNPs_freq_table)=c("CHR", "SNP", "Kamboj_sameallele_freq")
colnames(Basu_ManipuriBrahmins_14_allSNPs_freq_table)=c("CHR", "SNP", "ManipuriBrahmin_sameallele_freq")
colnames(Basu_Onges_14_allSNPs_freq_table)=c("CHR", "SNP", "Onge_sameallele_freq")
colnames(Basu_Pallans_14_allSNPs_freq_table)=c("CHR", "SNP", "Pallan_sameallele_freq")
colnames(Basu_Paniyas_14_allSNPs_freq_table)=c("CHR", "SNP", "Paniya_sameallele_freq")
colnames(Pathak_Rors_14_allSNPs_freq_table)=c("CHR", "SNP", "Ror_sameallele_freq")
colnames(Basu_Santals_14_allSNPs_freq_table)=c("CHR", "SNP", "Santal_sameallele_freq")
colnames(Basu_Tripuris_14_allSNPs_freq_table)=c("CHR", "SNP", "Tripuri_sameallele_freq")
colnames(Basu_Tharus_14_allSNPs_freq_table)=c("CHR", "SNP", "Tharu_sameallele_freq")
colnames(Basu_WestBengalBrahmins_14_allSNPs_freq_table)=c("CHR", "SNP", "WestBengalBrahmin_sameallele_freq")

#merge all sub-populations' 14_allSNPs_freq_tables together first 
Allsubpops_allSNPs_merged = Reduce(function(x, y) merge(x,y, all=TRUE), list(Basu_Birhors_14_allSNPs_freq_table, Basu_Gonds_14_allSNPs_freq_table, Basu_GujuratiBrahmins_14_allSNPs_freq_table, Basu_Hos_14_allSNPs_freq_table, Basu_Irulas_14_allSNPs_freq_table, Basu_Iyers_14_allSNPs_freq_table, Basu_Jamatias_14_allSNPs_freq_table, Basu_Jarawas_14_allSNPs_freq_table, Basu_Kadars_14_allSNPs_freq_table, Basu_Khatris_14_allSNPs_freq_table, Basu_Korwas_14_allSNPs_freq_table, Basu_ManipuriBrahmins_14_allSNPs_freq_table, Basu_Onges_14_allSNPs_freq_table, Basu_Pallans_14_allSNPs_freq_table, Basu_Paniyas_14_allSNPs_freq_table, Basu_Santals_14_allSNPs_freq_table, Basu_Tharus_14_allSNPs_freq_table, Basu_Tripuris_14_allSNPs_freq_table, Basu_WestBengalBrahmins_14_allSNPs_freq_table, Pathak_Gujjars_14_allSNPs_freq_table, Pathak_Kambojs_14_allSNPs_freq_table, Pathak_Rors_14_allSNPs_freq_table))
#remove rows where not all populations have SNP
Allsubpops_allSNPs_merged = na.omit(Allsubpops_allSNPs_merged) 
#add column for sameallele_freq variance which shows allelefreq variance across subpopulations for each SNP
library(purrr)
Allsubpops_allSNPs_merged = Allsubpops_allSNPs_merged %>% dplyr:: mutate(sameallele_var = pmap_dbl(list(Birhor_sameallele_freq, Gond_sameallele_freq, GujuratiBrahmin_sameallele_freq, Ho_sameallele_freq, Irula_sameallele_freq, Iyer_sameallele_freq, Jamatia_sameallele_freq, Jarawa_sameallele_freq, Kadar_sameallele_freq, Khatri_sameallele_freq, Korwa_sameallele_freq, ManipuriBrahmin_sameallele_freq, Onge_sameallele_freq, Pallan_sameallele_freq, Paniya_sameallele_freq, Santal_sameallele_freq, Tharu_sameallele_freq, Tripuri_sameallele_freq, WestBengalBrahmin_sameallele_freq, Gujjar_sameallele_freq, Kamboj_sameallele_freq, Ror_sameallele_freq), ~ var(c(...))))
#merge Basu_Pathak_collective datatable with the Allsubpops table
Allsubpops_allSNPs_merged2=merge(Allsubpops_allSNPs_merged, Basu_Pathak_collective_samples_hweandfreq_table, all.x=TRUE, by="SNP")
#then add variance bins
max(Allsubpops_allSNPs_merged2$sameallele_var)
bins <- seq(0, 1, by=0.075)
Allsubpops_allSNPs_merged2$Bin_sameallelevar=cut(Allsubpops_allSNPs_merged2$sameallele_var, bins)
#add new MAF bin which is a bit larger to be more accommodating
#now create bins by MAF
bins <- seq(0, 1, by=0.1)
Allsubpops_allSNPs_merged2$Bin_MAF_new=cut(Allsubpops_allSNPs_merged2$MAF, bins)
#Add var to MAF bin to make joint bin
Allsubpops_allSNPs_merged2$Bin_MAF_W_sameallelevar=paste(Allsubpops_allSNPs_merged2$Bin_MAF_new, Allsubpops_allSNPs_merged2$Bin_sameallelevar)
#check enough other SNPs are in MAF with variance bin for rs1426654
Allsubpops_allSNPs_merged2$Bin_MAF_W_sameallelevar[Allsubpops_allSNPs_merged2$SNP=="rs1426654"]
nrow(subset(Allsubpops_allSNPs_merged2, Bin_MAF_W_sameallelevar =="(0.3,0.4] (0.075,0.15]"))
#find average F for each (joint allelefreq and variance) bin and add this column 
library(dplyr)
Allsubpops_allSNPs_merged2=Allsubpops_allSNPs_merged2 %>% dplyr:: group_by(Bin_MAF_W_sameallelevar) %>% dplyr:: mutate(Bin_MAF_W_sameallelevar_F_average = mean(F_score))
Allsubpops_allSNPs_merged2=Allsubpops_allSNPs_merged2 %>% dplyr:: group_by(Bin_MAF_W_sameallelevar) %>% dplyr:: mutate(Bin_MAF_W_sameallelevar_F_sd = sd(F_score))
#run Z scores 
Allsubpops_allSNPs_merged2$Z_score_new=(Allsubpops_allSNPs_merged2$F_score-Allsubpops_allSNPs_merged2$Bin_MAF_W_sameallelevar_F_average)/Allsubpops_allSNPs_merged2$Bin_MAF_W_sameallelevar_F_sd
#now see where rs1426654 SNP sits 
rs1426654_Zscore_new=Allsubpops_allSNPs_merged2$Z_score_new[Allsubpops_allSNPs_merged2$SNP=="rs1426654"]
rs1426654_Zscore_new
#--> Z score is 3.19 i.e. significantly high!!

#find prop of SNPS with Z score higher
nrow(subset(Allsubpops_allSNPs_merged2, Z_score_new>rs1426654_Zscore_new))/nrow(Allsubpops_allSNPs_merged2)
#--> only 0.4% of SNPs had higher Z score than rs1426654
#see what these SNPs are
SNPS_WgreaterZthanrs1426654=subset(Allsubpops_allSNPs_merged2, Z_score_new>rs1426654_Zscore_new)
#export these SNPs 
write.table(Allsubpops_allSNPs_merged2$SNP[Allsubpops_allSNPs_merged2$Z_score_new>rs1426654_Zscore_new], "SNPS_WgreaterZthanrs1426654.txt", sep=";", col.names = TRUE)
#check if all known skin pigmentation genes in list of SNPs with greater Z than rs1426654
listofSNPS_WgreaterZthanrs1426654=c("rs10409101","rs10492106","rs1049728","rs1058256","rs1105329","rs11202902","rs1136025","rs1150725","rs11541998","rs11603541","rs11683509","rs11768743","rs11772680","rs11858659","rs11929668","rs11981667","rs12027579","rs12070195","rs12151363","rs12185233","rs12189695","rs12208360","rs12303713","rs12340619","rs12373142","rs12894651","rs1318056","rs13301565","rs1395388","rs1438914","rs16858803","rs16862278","rs16924863","rs16943227","rs17152659","rs17162549","rs17190134","rs17219837","rs17220500","rs17236868","rs17540213","rs17784152","rs17882656","rs17885614","rs1801726","rs1802498","rs1822312","rs1995319","rs2071747","rs2114437","rs2230782","rs2234924","rs2276872","rs2295275","rs2295615","rs235679","rs2389604","rs2451713","rs2455408","rs2488429","rs2516676","rs2523505","rs2523979","rs2578652","rs262643","rs2675696","rs2708943","rs27625","rs2823742","rs2835288","rs2886296","rs3093547","rs3093668","rs3097645","rs310831","rs3129296","rs3129840","rs3130071","rs323043","rs34191521","rs34503701","rs35430780","rs3735351","rs3813867","rs3924194","rs3925027","rs4135268","rs4148392","rs4148556","rs4236655","rs4508546","rs4646296","rs4713504","rs4924454","rs5029939","rs592514","rs6012689","rs7221473","rs7252722","rs7316412","rs731780","rs760134","rs7630967","rs7689099","rs802986","rs8187929","rs8192837","rs8192903","rs8677","rs885913","rs8876","rs9267822","rs9275615","rs9276487","rs9296081","rs9332119","rs9391733","rs940611","rs9474433","rs948373","rs9692574","rs9965760")
listofskinpigSNPs=c("rs16891982","rs12203592","rs78160260","rs1042602","rs12203592","rs1126809","rs10099237","rs1805007","rs77688320","rs4268748","rs1800407","rs61310892","rs1042602","rs340417","rs12203592","rs74836424","rs10202963","rs12978395","rs79592764","rs2425025","rs427370","rs112332856","rs1805007","rs141457510","rs12056541","rs13296795","rs2240751","rs2921077","rs138420351","rs1805005","rs6060612","rs8015138","rs2921077","rs112725747","rs1667392","rs12203592","rs183671","rs12913832","rs12913832","rs1667392","rs2228479","rs6659601","rs76141549","rs16891982","rs16891982","rs12913832","rs31487","rs78378222","rs147430042","rs11198112","rs1034484","rs13192471","rs2425070","rs62211621","rs1800407","rs8178977","rs7326155","rs77437330","rs16891982","rs1426654","rs16891982","rs12203592","rs1800404","rs7528427","rs12913832","rs7528427","rs427370","rs12056541","rs61310892","rs10738605","rs1417965","rs7852515","rs16891982","rs31487","rs16891982","rs1667392","rs12913832","rs6082600","rs28649461","rs10849455","rs16891982","rs2899446","rs111930714","rs4778219","rs75653149","rs12202284","rs72928038","rs1426654","rs16891982","rs4778219","rs8033655","rs28649461","rs12913832","rs80252786","rs1129038","rs16891982","rs16891982","rs12203592","rs1805005","rs13192471","rs3769823","rs7625680","rs12203592","rs6059655","rs12203592","rs6142102","rs1667392","rs6709352","rs7326155","rs10042706","rs1805008","rs13192471","rs17401449","rs62210588","rs2470102","rs1667392","rs7118677","rs13192471","rs1667392","rs885479","rs16982256","rs16891982","rs1426654","rs10831496","rs1902910","rs6602666","rs16891982","rs16891982","rs183671","rs1805008","rs1805007","rs369805468","rs12913832","rs2675345","rs1805009","rs1667392","rs10133804","rs1805007","rs80030016","rs1110400","rs62385132","rs4778249","rs11858919","rs16891982","rs2470102","rs12925026","rs6441620","rs11170164","rs12203592","rs13192471","rs16891982","rs61435271","rs755107","rs1667392","rs77443641","rs16891982","rs6602665","rs12913832","rs62143248","rs12070203","rs35395","rs151165649","rs12913832","rs1800404","rs28753701","rs4911466","rs16891982","rs35397","rs16891982","rs73753762","rs73283869","rs2425241","rs13192471","rs12913832","rs16891982","rs12913832","rs1805007","rs35412","rs1667392","rs1800407","rs16891982","rs1042602","rs12913832","rs115105970","rs62029775","rs12931267","rs55768819","rs201429679","rs111256285","rs35395","rs2776353","rs1126809","rs77462788","rs11707890","rs1805007","rs7794780","rs1129038","rs479777","rs74445037","rs17437952","rs13192471","rs12913832","rs532282237","rs12120422","rs62170035","rs12913832","rs35397","rs115019323","rs1042602","rs1805009","rs16891982","rs62209647","rs1126809","rs11774568","rs11133982","rs2322031","rs76327975","rs10810657","rs3024737","rs16891982","rs4778249","rs1667392","rs17431641","rs28479566","rs2228479","rs6059655","rs1800407","rs1805007","rs1667392","rs79522206","rs1667392","rs364635","rs4911523","rs12913832","rs1800407","rs117307642","rs6013012","rs340417","rs1426654","rs12203592","rs115798498","rs12541402","rs2424952","rs157935","rs11774568","rs12913832","rs6788400","rs1667392","rs10996738","rs3900053","rs12203592","rs2294214","rs55767876","rs12931267","rs12913832","rs35063026","rs12913832","rs1667392","rs7923095","rs1885120","rs1800407","rs117382825","rs60061574","rs1834640","rs16891982","rs12913832","rs35202394","rs56238684","rs117744081","rs1532548","rs188019015","rs129138")
listofskinpigSNPs %in% listofSNPS_WgreaterZthanrs1426654
#-->no
#check if all known male fertility genes in list of SNPs with greater Z than rs1426654
listofmalefertSNPs=c("rs4936891","rs2423942","rs2991396","rs11144790","rs10966811","rs433406")
listofmalefertSNPs %in% listofSNPS_WgreaterZthanrs1426654
#-->no
#check if all known female pigmentation genes in list of SNPs with greater Z than rs1426654
listoffemalefertSNPs=c("rs10009124","rs1429606","rs1368013","rs1368011","rs16839793","rs12408261","rs696888","rs4295021","rs4295021","rs12991146","rs16872971","rs16872971","rs12408989","rs4712710","rs1808470","rs2813490","rs16841877","rs16841877","rs10843287","rs35320143","rs7239865","rs28588043","rs1105228","rs12406463","rs7869190","rs6711319","rs12408989","rs186478","rs696888","rs1105228","rs10009124","rs7775534")
listoffemalefertSNPs %in% listofSNPS_WgreaterZthanrs1426654
#-->no

#look if known SNPs of skin pigs, male and female fertility are in the subset of the table with Z_score_new over 1.96
SNPS_WgreaterZthan196=subset(Allsubpops_allSNPs_merged2, Z_score_new>1.96)
write.table(Allsubpops_allSNPs_merged2$SNP[Allsubpops_allSNPs_merged2$Z_score_new>1.96], "SNPS_WgreaterZthan196.txt", sep=";", col.names = TRUE)
#check skinpigs in SNPS over 1.96
checkskinpigs = listofskinpigSNPs %in% SNPS_WgreaterZthan196$SNP
which(checkskinpigs==TRUE)
#--> true at 60,88,120,123,239
listofskinpigSNPs[which(checkskinpigs==TRUE)]
#-->[1] "rs1426654" "rs1426654" "rs885479"  "rs1426654" "rs1426654"
#do male fertility
checkmalefert = listofmalefertSNPs %in% SNPS_WgreaterZthan196$SNP
which(checkmalefert==TRUE)
#--> none
#do female fertility 
checkfemalefert = listoffemalefertSNPs %in% SNPS_WgreaterZthan196$SNP
which(checkfemalefert==TRUE)
#--> none
#check Z score of rs885479
Allsubpops_allSNPs_merged2$Z_score_new[Allsubpops_allSNPs_merged2$SNP=="rs885479"]






#further analysis number 2
#look for evidence of an excess Wahlund effect within the region of the functional SNP
#first load each populations'freq and hwe tables with relevant info for all SNPs
#GONDS
Basu_Gonds_allSNPs_hwe_table=read.table("Basu_Gond_allSNPs_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Gonds_allSNPs_freq_table=read.table("Basu_Gond_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Gonds_allSNPs_freq_table=subset(Basu_Gonds_allSNPs_freq_table, MAF>0)
Basu_Gonds_allSNPs_freqandhwe_table=merge(Basu_Gonds_allSNPs_freq_table, Basu_Gonds_allSNPs_hwe_table, all.x=TRUE)
#then make F score column
Basu_Gonds_allSNPs_freqandhwe_table$F_score=1-(Basu_Gonds_allSNPs_freqandhwe_table$O.HET./Basu_Gonds_allSNPs_freqandhwe_table$E.HET.)
#add MAF freq bins
bins <- seq(0, 1, by=0.05)
Basu_Gonds_allSNPs_freqandhwe_table$Bin_MAF=cut(Basu_Gonds_allSNPs_freqandhwe_table$MAF, bins)
#now convert each SNP's F score to Z score 
library(dplyr)
Basu_Gonds_allSNPs_freqandhwe_table=Basu_Gonds_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_average = mean(F_score))
Basu_Gonds_allSNPs_freqandhwe_table=Basu_Gonds_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_sd = sd(F_score))
Basu_Gonds_allSNPs_freqandhwe_table$Z_score=(Basu_Gonds_allSNPs_freqandhwe_table$F_score-Basu_Gonds_allSNPs_freqandhwe_table$Bin_F_average)/Basu_Gonds_allSNPs_freqandhwe_table$Bin_F_sd
#load bim file to get genetic position of each SNP
Basu_Gonds_bim=read.table("Basu_Gonds.bim", sep="", stringsAsFactors = FALSE)
colnames(Basu_Gonds_bim)=c("CHR", "SNP", "position_cm", "bp_coordinate", "A1", "A2")
Basu_Gonds_allSNPs_freqandhwe_table=merge(Basu_Gonds_allSNPs_freqandhwe_table, Basu_Gonds_bim, all.x=TRUE, by.x="SNP",by.y = "SNP", memory.limit(size=2500000))
#make Bin_kb for 400kb windows over genome 
max(Basu_Gonds_allSNPs_freqandhwe_table$bp_coordinate)
bins <- seq(0, 400000000, by=400000)
Basu_Gonds_allSNPs_freqandhwe_table$Bin_kb=cut(Basu_Gonds_allSNPs_freqandhwe_table$bp_coordinate, bins)
#add chromosome number to each Bin_kb value too for precise locus 
Basu_Gonds_allSNPs_freqandhwe_table$Bin_kb_Wchromosome=paste(Basu_Gonds_allSNPs_freqandhwe_table$CHR.x, Basu_Gonds_allSNPs_freqandhwe_table$Bin_kb)
#make column with average F and Z value for each bp bin
library(dplyr)
Basu_Gonds_allSNPs_freqandhwe_table=Basu_Gonds_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr:: mutate(Bin_kb_Wchromosome_F_average = mean(F_score))
Basu_Gonds_allSNPs_freqandhwe_table=Basu_Gonds_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr:: mutate(Bin_kb_Wchromosome_Z_average = mean(Z_score))
#make freqtable
library(dplyr)
Basu_Gonds_allSNPs_freqandhwe_freq_table <- Basu_Gonds_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr:: summarise(Freq=n())
colnames(Basu_Gonds_allSNPs_freqandhwe_freq_table)=c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_freq")
#merge freqtable back with original 
Basu_Gonds_allSNPs_freqandhwe_table=merge(Basu_Gonds_allSNPs_freqandhwe_table, Basu_Gonds_allSNPs_freqandhwe_freq_table, all.x=TRUE, by="Bin_kb_Wchromosome")
#add proportion of Bin_kb with Z score over 1.96
Basu_Gonds_allSNPs_freqandhwe_table=Basu_Gonds_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_prop_sighigh = sum(Z_score>1.96)/Bin_kb_Wchromosome_freq)
#make aggregated table showing Gonds' bin_kb, bin_kb_F_avergae, bin_kb_Z_average, freq
Basu_Gonds_aggregated_table=unique(Basu_Gonds_allSNPs_freqandhwe_table[c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_F_average", "Bin_kb_Wchromosome_Z_average", "Bin_kb_Wchromosome_freq", "Bin_kb_Wchromosome_Z_prop_sighigh")])
#SLC24A5 is between base pair 46,200,461 to base pair 46,221,881 --> i.e. in bin 46,000,000 - 46,400,000 or 15 4.6e+07,4.64e+07, so find average F value for this genomic region
Gond_SLC24A5_averageF=Basu_Gonds_aggregated_table$Bin_kb_Wchromosome_F_average[which(Basu_Gonds_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Gond_SLC24A5_F_comparison=nrow(subset(Basu_Gonds_aggregated_table, Bin_kb_Wchromosome_F_average>Gond_SLC24A5_averageF))/nrow(Basu_Gonds_aggregated_table)
#-->no evidence of excess homo around SLC2415: 22.6% of genomic regions have higher F than Gonds
#find proportion of genomic regions with average Z value higher than that of SLC24A5 region
Gond_SLC24A5_averageZ=Basu_Gonds_aggregated_table$Bin_kb_Wchromosome_Z_average[which(Basu_Gonds_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average Z value higher than that of SLC24A5 region
Gond_SLC24A5_Z_comparison=nrow(subset(Basu_Gonds_aggregated_table, Bin_kb_Wchromosome_Z_average>Gond_SLC24A5_averageZ))/nrow(Basu_Gonds_aggregated_table)
#-->no evidence of excess homo around SLC2415: 23.5% of genomic regions have higher F than Gonds



#GujuratiBrahminS
Basu_GujuratiBrahmins_allSNPs_hwe_table=read.table("Basu_GujuratiBrahmin_allSNPs_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_GujuratiBrahmins_allSNPs_freq_table=read.table("Basu_GujuratiBrahmin_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_GujuratiBrahmins_allSNPs_freq_table=subset(Basu_GujuratiBrahmins_allSNPs_freq_table, MAF>0)
Basu_GujuratiBrahmins_allSNPs_freqandhwe_table=merge(Basu_GujuratiBrahmins_allSNPs_freq_table, Basu_GujuratiBrahmins_allSNPs_hwe_table, all.x=TRUE)
#then make F score column
Basu_GujuratiBrahmins_allSNPs_freqandhwe_table$F_score=1-(Basu_GujuratiBrahmins_allSNPs_freqandhwe_table$O.HET./Basu_GujuratiBrahmins_allSNPs_freqandhwe_table$E.HET.)
#add MAF freq bins
bins <- seq(0, 1, by=0.05)
Basu_GujuratiBrahmins_allSNPs_freqandhwe_table$Bin_MAF=cut(Basu_GujuratiBrahmins_allSNPs_freqandhwe_table$MAF, bins)
#now convert each SNP's F score to Z score 
library(dplyr)
Basu_GujuratiBrahmins_allSNPs_freqandhwe_table=Basu_GujuratiBrahmins_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_average = mean(F_score))
Basu_GujuratiBrahmins_allSNPs_freqandhwe_table=Basu_GujuratiBrahmins_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_sd = sd(F_score))
Basu_GujuratiBrahmins_allSNPs_freqandhwe_table$Z_score=(Basu_GujuratiBrahmins_allSNPs_freqandhwe_table$F_score-Basu_GujuratiBrahmins_allSNPs_freqandhwe_table$Bin_F_average)/Basu_GujuratiBrahmins_allSNPs_freqandhwe_table$Bin_F_sd
#load bim file to get genetic position of each SNP
Basu_GujuratiBrahmins_bim=read.table("Basu_GujuratiBrahmins.bim", sep="", stringsAsFactors = FALSE)
colnames(Basu_GujuratiBrahmins_bim)=c("CHR", "SNP", "position_cm", "bp_coordinate", "A1", "A2")
Basu_GujuratiBrahmins_allSNPs_freqandhwe_table=merge(Basu_GujuratiBrahmins_allSNPs_freqandhwe_table, Basu_GujuratiBrahmins_bim, all.x=TRUE, by.x="SNP",by.y = "SNP", memory.limit(size=2500000))
#make Bin_kb for 400kb windows over genome 
max(Basu_GujuratiBrahmins_allSNPs_freqandhwe_table$bp_coordinate)
bins <- seq(0, 400000000, by=400000)
Basu_GujuratiBrahmins_allSNPs_freqandhwe_table$Bin_kb=cut(Basu_GujuratiBrahmins_allSNPs_freqandhwe_table$bp_coordinate, bins)
#add chromosome number to each Bin_kb value too for precise locus 
Basu_GujuratiBrahmins_allSNPs_freqandhwe_table$Bin_kb_Wchromosome=paste(Basu_GujuratiBrahmins_allSNPs_freqandhwe_table$CHR.x, Basu_GujuratiBrahmins_allSNPs_freqandhwe_table$Bin_kb)
#make column with average F and Z value for each bp bin
library(dplyr)
Basu_GujuratiBrahmins_allSNPs_freqandhwe_table=Basu_GujuratiBrahmins_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_F_average = mean(F_score))
Basu_GujuratiBrahmins_allSNPs_freqandhwe_table=Basu_GujuratiBrahmins_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_average = mean(Z_score))
#make freqtable
library(dplyr)
Basu_GujuratiBrahmins_allSNPs_freqandhwe_freq_table <- Basu_GujuratiBrahmins_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr:: summarise(Freq=n())
colnames(Basu_GujuratiBrahmins_allSNPs_freqandhwe_freq_table)=c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_freq")
#merge freqtable back with original 
Basu_GujuratiBrahmins_allSNPs_freqandhwe_table=merge(Basu_GujuratiBrahmins_allSNPs_freqandhwe_table, Basu_GujuratiBrahmins_allSNPs_freqandhwe_freq_table, all.x=TRUE, by="Bin_kb_Wchromosome")
#add proportion of Bin_kb with Z score over 1.96
Basu_GujuratiBrahmins_allSNPs_freqandhwe_table=Basu_GujuratiBrahmins_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_prop_sighigh = sum(Z_score>1.96)/Bin_kb_Wchromosome_freq)
#make aggregated table showing GujuratiBrahmins' bin_kb, bin_kb_F_avergae, bin_kb_Z_average, freq
Basu_GujuratiBrahmins_aggregated_table=unique(Basu_GujuratiBrahmins_allSNPs_freqandhwe_table[c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_F_average", "Bin_kb_Wchromosome_Z_average", "Bin_kb_Wchromosome_freq", "Bin_kb_Wchromosome_Z_prop_sighigh")])
#SLC24A5 is between base pair 46,200,461 to base pair 46,221,881 --> i.e. in bin 46,000,000 - 46,400,000 or 15 4.6e+07,4.64e+07, so find average F value for this genomic region
GujuratiBrahmin_SLC24A5_averageF=Basu_GujuratiBrahmins_aggregated_table$Bin_kb_Wchromosome_F_average[which(Basu_GujuratiBrahmins_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
GujuratiBrahmin_SLC24A5_F_comparison=nrow(subset(Basu_GujuratiBrahmins_aggregated_table, Bin_kb_Wchromosome_F_average>GujuratiBrahmin_SLC24A5_averageF))/nrow(Basu_GujuratiBrahmins_aggregated_table)
#-->no evidence of excess homo around SLC2415: 89.2% of genomic regions have higher F than GujuratiBrahmins
#find proportion of genomic regions with average Z value higher than that of SLC24A5 region
GujuratiBrahmin_SLC24A5_averageZ=Basu_GujuratiBrahmins_aggregated_table$Bin_kb_Wchromosome_Z_average[which(Basu_GujuratiBrahmins_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average Z value lower than that of SLC24A5 region
GujuratiBrahmin_SLC24A5_Z_comparison=nrow(subset(Basu_GujuratiBrahmins_aggregated_table, Bin_kb_Wchromosome_Z_average<GujuratiBrahmin_SLC24A5_averageZ))/nrow(Basu_GujuratiBrahmins_aggregated_table)
#-->no evidence of excess homo around SLC2415: 89% have higher

#GujjarS
Pathak_Gujjars_allSNPs_hwe_table=read.table("Pathak_Gujjar_allSNPs_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header = TRUE)
Pathak_Gujjars_allSNPs_freq_table=read.table("Pathak_Gujjar_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Pathak_Gujjars_allSNPs_freq_table=subset(Pathak_Gujjars_allSNPs_freq_table, MAF>0)
Pathak_Gujjars_allSNPs_freqandhwe_table=merge(Pathak_Gujjars_allSNPs_freq_table, Pathak_Gujjars_allSNPs_hwe_table, all.x=TRUE)
#then make F score column
Pathak_Gujjars_allSNPs_freqandhwe_table$F_score=1-(Pathak_Gujjars_allSNPs_freqandhwe_table$O.HET./Pathak_Gujjars_allSNPs_freqandhwe_table$E.HET.)
#add MAF freq bins
bins <- seq(0, 1, by=0.05)
Pathak_Gujjars_allSNPs_freqandhwe_table$Bin_MAF=cut(Pathak_Gujjars_allSNPs_freqandhwe_table$MAF, bins)
#now convert each SNP's F score to Z score 
library(dplyr)
Pathak_Gujjars_allSNPs_freqandhwe_table=Pathak_Gujjars_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_average = mean(F_score))
Pathak_Gujjars_allSNPs_freqandhwe_table=Pathak_Gujjars_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_sd = sd(F_score))
Pathak_Gujjars_allSNPs_freqandhwe_table$Z_score=(Pathak_Gujjars_allSNPs_freqandhwe_table$F_score-Pathak_Gujjars_allSNPs_freqandhwe_table$Bin_F_average)/Pathak_Gujjars_allSNPs_freqandhwe_table$Bin_F_sd
#load bim file to get genetic position of each SNP
Pathak_Gujjars_bim=read.table("Pathak_Gujjars.bim", sep="", stringsAsFactors = FALSE)
colnames(Pathak_Gujjars_bim)=c("CHR", "SNP", "position_cm", "bp_coordinate", "A1", "A2")
Pathak_Gujjars_allSNPs_freqandhwe_table=merge(Pathak_Gujjars_allSNPs_freqandhwe_table, Pathak_Gujjars_bim, all.x=TRUE, by.x="SNP",by.y = "SNP", memory.limit(size=2500000))
#make Bin_kb for 400kb windows over genome 
max(Pathak_Gujjars_allSNPs_freqandhwe_table$bp_coordinate)
bins <- seq(0, 400000000, by=400000)
Pathak_Gujjars_allSNPs_freqandhwe_table$Bin_kb=cut(Pathak_Gujjars_allSNPs_freqandhwe_table$bp_coordinate, bins)
#add chromosome number to each Bin_kb value too for precise locus 
Pathak_Gujjars_allSNPs_freqandhwe_table$Bin_kb_Wchromosome=paste(Pathak_Gujjars_allSNPs_freqandhwe_table$CHR.x, Pathak_Gujjars_allSNPs_freqandhwe_table$Bin_kb)
#make column with average F and Z value for each bp bin
library(dplyr)
Pathak_Gujjars_allSNPs_freqandhwe_table=Pathak_Gujjars_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_F_average = mean(F_score))
Pathak_Gujjars_allSNPs_freqandhwe_table=Pathak_Gujjars_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_average = mean(Z_score))
#make freqtable
library(dplyr)
Pathak_Gujjars_allSNPs_freqandhwe_freq_table <- Pathak_Gujjars_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr:: summarise(Freq=n())
colnames(Pathak_Gujjars_allSNPs_freqandhwe_freq_table)=c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_freq")
#merge freqtable back with original 
Pathak_Gujjars_allSNPs_freqandhwe_table=merge(Pathak_Gujjars_allSNPs_freqandhwe_table, Pathak_Gujjars_allSNPs_freqandhwe_freq_table, all.x=TRUE, by="Bin_kb_Wchromosome")
#add proportion of Bin_kb with Z score over 1.96
Pathak_Gujjars_allSNPs_freqandhwe_table=Pathak_Gujjars_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_prop_sighigh = sum(Z_score>1.96)/Bin_kb_Wchromosome_freq)
#make aggregated table showing Gujjars' bin_kb, bin_kb_F_avergae, bin_kb_Z_average, freq
Pathak_Gujjars_aggregated_table=unique(Pathak_Gujjars_allSNPs_freqandhwe_table[c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_F_average", "Bin_kb_Wchromosome_Z_average", "Bin_kb_Wchromosome_freq", "Bin_kb_Wchromosome_Z_prop_sighigh")])
#SLC24A5 is between base pair 46,200,461 to base pair 46,221,881 --> i.e. in bin 46,000,000 - 46,400,000 or 15 4.6e+07,4.64e+07, so find average F value for this genomic region
Gujjar_SLC24A5_averageF=Pathak_Gujjars_aggregated_table$Bin_kb_Wchromosome_F_average[which(Pathak_Gujjars_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Gujjar_SLC24A5_F_comparison=nrow(subset(Pathak_Gujjars_aggregated_table, Bin_kb_Wchromosome_F_average>Gujjar_SLC24A5_averageF))/nrow(Pathak_Gujjars_aggregated_table)
#-->no evidence of excess homo around SLC2415: 31.2% of genomic regions have higher F than Gujjars
#find proportion of genomic regions with average Z value higher than that of SLC24A5 region
Gujjar_SLC24A5_averageZ=Pathak_Gujjars_aggregated_table$Bin_kb_Wchromosome_Z_average[which(Pathak_Gujjars_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Gujjar_SLC24A5_Z_comparison=nrow(subset(Pathak_Gujjars_aggregated_table, Bin_kb_Wchromosome_Z_average>Gujjar_SLC24A5_averageZ))/nrow(Pathak_Gujjars_aggregated_table)
#-->no evidence of excess homo around SLC2415: 29%


#HoS
Basu_Hos_allSNPs_hwe_table=read.table("Basu_Ho_allSNPs_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Hos_allSNPs_freq_table=read.table("Basu_Ho_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Hos_allSNPs_freq_table=subset(Basu_Hos_allSNPs_freq_table, MAF>0)
Basu_Hos_allSNPs_freqandhwe_table=merge(Basu_Hos_allSNPs_freq_table, Basu_Hos_allSNPs_hwe_table, all.x=TRUE)
#then make F score column
Basu_Hos_allSNPs_freqandhwe_table$F_score=1-(Basu_Hos_allSNPs_freqandhwe_table$O.HET./Basu_Hos_allSNPs_freqandhwe_table$E.HET.)
#add MAF freq bins
bins <- seq(0, 1, by=0.05)
Basu_Hos_allSNPs_freqandhwe_table$Bin_MAF=cut(Basu_Hos_allSNPs_freqandhwe_table$MAF, bins)
#now convert each SNP's F score to Z score 
library(dplyr)
Basu_Hos_allSNPs_freqandhwe_table=Basu_Hos_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_average = mean(F_score))
Basu_Hos_allSNPs_freqandhwe_table=Basu_Hos_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_sd = sd(F_score))
Basu_Hos_allSNPs_freqandhwe_table$Z_score=(Basu_Hos_allSNPs_freqandhwe_table$F_score-Basu_Hos_allSNPs_freqandhwe_table$Bin_F_average)/Basu_Hos_allSNPs_freqandhwe_table$Bin_F_sd
#load bim file to get genetic position of each SNP
Basu_Hos_bim=read.table("Basu_Hos.bim", sep="", stringsAsFactors = FALSE)
colnames(Basu_Hos_bim)=c("CHR", "SNP", "position_cm", "bp_coordinate", "A1", "A2")
Basu_Hos_allSNPs_freqandhwe_table=merge(Basu_Hos_allSNPs_freqandhwe_table, Basu_Hos_bim, all.x=TRUE, by.x="SNP",by.y = "SNP", memory.limit(size=2500000))
#make Bin_kb for 400kb windows over genome 
max(Basu_Hos_allSNPs_freqandhwe_table$bp_coordinate)
bins <- seq(0, 400000000, by=400000)
Basu_Hos_allSNPs_freqandhwe_table$Bin_kb=cut(Basu_Hos_allSNPs_freqandhwe_table$bp_coordinate, bins)
#add chromosome number to each Bin_kb value too for precise locus 
Basu_Hos_allSNPs_freqandhwe_table$Bin_kb_Wchromosome=paste(Basu_Hos_allSNPs_freqandhwe_table$CHR.x, Basu_Hos_allSNPs_freqandhwe_table$Bin_kb)
#make column with average F and Z value for each bp bin
library(dplyr)
Basu_Hos_allSNPs_freqandhwe_table=Basu_Hos_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_F_average = mean(F_score))
Basu_Hos_allSNPs_freqandhwe_table=Basu_Hos_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_average = mean(Z_score))
#make freqtable
library(dplyr)
Basu_Hos_allSNPs_freqandhwe_freq_table <- Basu_Hos_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr:: summarise(Freq=n())
colnames(Basu_Hos_allSNPs_freqandhwe_freq_table)=c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_freq")
#merge freqtable back with original 
Basu_Hos_allSNPs_freqandhwe_table=merge(Basu_Hos_allSNPs_freqandhwe_table, Basu_Hos_allSNPs_freqandhwe_freq_table, all.x=TRUE, by="Bin_kb_Wchromosome")
#add proportion of Bin_kb with Z score over 1.96
Basu_Hos_allSNPs_freqandhwe_table=Basu_Hos_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_prop_sighigh = sum(Z_score>1.96)/Bin_kb_Wchromosome_freq)
#make aggregated table showing Hos' bin_kb, bin_kb_F_avergae, bin_kb_Z_average, freq
Basu_Hos_aggregated_table=unique(Basu_Hos_allSNPs_freqandhwe_table[c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_F_average", "Bin_kb_Wchromosome_Z_average", "Bin_kb_Wchromosome_freq", "Bin_kb_Wchromosome_Z_prop_sighigh")])
#SLC24A5 is between base pair 46,200,461 to base pair 46,221,881 --> i.e. in bin 46,000,000 - 46,400,000 or 15 4.6e+07,4.64e+07, so find average F value for this genomic region
Ho_SLC24A5_averageF=Basu_Hos_aggregated_table$Bin_kb_Wchromosome_F_average[which(Basu_Hos_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Ho_SLC24A5_F_comparison=nrow(subset(Basu_Hos_aggregated_table, Bin_kb_Wchromosome_F_average>Ho_SLC24A5_averageF))/nrow(Basu_Hos_aggregated_table)
#-->no evidence of excess homo around SLC2415: 89.5% of genomic regions have higher F than Hos
#find proportion of genomic regions with average Z value higher than that of SLC24A5 region
Ho_SLC24A5_averageZ=Basu_Hos_aggregated_table$Bin_kb_Wchromosome_Z_average[which(Basu_Hos_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Ho_SLC24A5_Z_comparison=nrow(subset(Basu_Hos_aggregated_table, Bin_kb_Wchromosome_Z_average<Ho_SLC24A5_averageZ))/nrow(Basu_Hos_aggregated_table)
#-->no evidence of excess homo around SLC2415:88%


#IrulaS
Basu_Irulas_allSNPs_hwe_table=read.table("Basu_Irula_allSNPs_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Irulas_allSNPs_freq_table=read.table("Basu_Irula_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Irulas_allSNPs_freq_table=subset(Basu_Irulas_allSNPs_freq_table, MAF>0)
Basu_Irulas_allSNPs_freqandhwe_table=merge(Basu_Irulas_allSNPs_freq_table, Basu_Irulas_allSNPs_hwe_table, all.x=TRUE)
#then make F score column
Basu_Irulas_allSNPs_freqandhwe_table$F_score=1-(Basu_Irulas_allSNPs_freqandhwe_table$O.HET./Basu_Irulas_allSNPs_freqandhwe_table$E.HET.)
#add MAF freq bins
bins <- seq(0, 1, by=0.05)
Basu_Irulas_allSNPs_freqandhwe_table$Bin_MAF=cut(Basu_Irulas_allSNPs_freqandhwe_table$MAF, bins)
#now convert each SNP's F score to Z score 
library(dplyr)
Basu_Irulas_allSNPs_freqandhwe_table=Basu_Irulas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_average = mean(F_score))
Basu_Irulas_allSNPs_freqandhwe_table=Basu_Irulas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_sd = sd(F_score))
Basu_Irulas_allSNPs_freqandhwe_table$Z_score=(Basu_Irulas_allSNPs_freqandhwe_table$F_score-Basu_Irulas_allSNPs_freqandhwe_table$Bin_F_average)/Basu_Irulas_allSNPs_freqandhwe_table$Bin_F_sd
#load bim file to get genetic position of each SNP
Basu_Irulas_bim=read.table("Basu_Irulas.bim", sep="", stringsAsFactors = FALSE)
colnames(Basu_Irulas_bim)=c("CHR", "SNP", "position_cm", "bp_coordinate", "A1", "A2")
Basu_Irulas_allSNPs_freqandhwe_table=merge(Basu_Irulas_allSNPs_freqandhwe_table, Basu_Irulas_bim, all.x=TRUE, by.x="SNP",by.y = "SNP", memory.limit(size=2500000))
#make Bin_kb for 400kb windows over genome 
max(Basu_Irulas_allSNPs_freqandhwe_table$bp_coordinate)
bins <- seq(0, 400000000, by=400000)
Basu_Irulas_allSNPs_freqandhwe_table$Bin_kb=cut(Basu_Irulas_allSNPs_freqandhwe_table$bp_coordinate, bins)
#add chromosome number to each Bin_kb value too for precise locus 
Basu_Irulas_allSNPs_freqandhwe_table$Bin_kb_Wchromosome=paste(Basu_Irulas_allSNPs_freqandhwe_table$CHR.x, Basu_Irulas_allSNPs_freqandhwe_table$Bin_kb)
#make column with average F and Z value for each bp bin
library(dplyr)
Basu_Irulas_allSNPs_freqandhwe_table=Basu_Irulas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_F_average = mean(F_score))
Basu_Irulas_allSNPs_freqandhwe_table=Basu_Irulas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_average = mean(Z_score))
#make freqtable
library(dplyr)
Basu_Irulas_allSNPs_freqandhwe_freq_table <- Basu_Irulas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr:: summarise(Freq=n())
colnames(Basu_Irulas_allSNPs_freqandhwe_freq_table)=c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_freq")
#merge freqtable back with original 
Basu_Irulas_allSNPs_freqandhwe_table=merge(Basu_Irulas_allSNPs_freqandhwe_table, Basu_Irulas_allSNPs_freqandhwe_freq_table, all.x=TRUE, by="Bin_kb_Wchromosome")
#add proportion of Bin_kb with Z score over 1.96
Basu_Irulas_allSNPs_freqandhwe_table=Basu_Irulas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_prop_sighigh = sum(Z_score>1.96)/Bin_kb_Wchromosome_freq)
#make aggregated table sIrulawing Irulas' bin_kb, bin_kb_F_avergae, bin_kb_Z_average, freq
Basu_Irulas_aggregated_table=unique(Basu_Irulas_allSNPs_freqandhwe_table[c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_F_average", "Bin_kb_Wchromosome_Z_average", "Bin_kb_Wchromosome_freq", "Bin_kb_Wchromosome_Z_prop_sighigh")])
#SLC24A5 is between base pair 46,200,461 to base pair 46,221,881 --> i.e. in bin 46,000,000 - 46,400,000 or 15 4.6e+07,4.64e+07, so find average F value for this genomic region
Irula_SLC24A5_averageF=Basu_Irulas_aggregated_table$Bin_kb_Wchromosome_F_average[which(Basu_Irulas_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Irula_SLC24A5_F_comparison=nrow(subset(Basu_Irulas_aggregated_table, Bin_kb_Wchromosome_F_average>Irula_SLC24A5_averageF))/nrow(Basu_Irulas_aggregated_table)
#-->no evidence of excess Irulamo around SLC2415: 22.6% of genomic regions have higher F than Irulas
#find proportion of genomic regions with average Z value higher than that of SLC24A5 region
Irula_SLC24A5_averageZ=Basu_Irulas_aggregated_table$Bin_kb_Wchromosome_Z_average[which(Basu_Irulas_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Irula_SLC24A5_Z_comparison=nrow(subset(Basu_Irulas_aggregated_table, Bin_kb_Wchromosome_Z_average>Irula_SLC24A5_averageZ))/nrow(Basu_Irulas_aggregated_table)
#-->no evidence of excess homo around SLC2415:35%

#IyerS
Basu_Iyers_allSNPs_hwe_table=read.table("Basu_Iyer_allSNPs_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Iyers_allSNPs_freq_table=read.table("Basu_Iyer_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Iyers_allSNPs_freq_table=subset(Basu_Iyers_allSNPs_freq_table, MAF>0)
Basu_Iyers_allSNPs_freqandhwe_table=merge(Basu_Iyers_allSNPs_freq_table, Basu_Iyers_allSNPs_hwe_table, all.x=TRUE)
#then make F score column
Basu_Iyers_allSNPs_freqandhwe_table$F_score=1-(Basu_Iyers_allSNPs_freqandhwe_table$O.HET./Basu_Iyers_allSNPs_freqandhwe_table$E.HET.)
#add MAF freq bins
bins <- seq(0, 1, by=0.05)
Basu_Iyers_allSNPs_freqandhwe_table$Bin_MAF=cut(Basu_Iyers_allSNPs_freqandhwe_table$MAF, bins)
#now convert each SNP's F score to Z score 
library(dplyr)
Basu_Iyers_allSNPs_freqandhwe_table=Basu_Iyers_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_average = mean(F_score))
Basu_Iyers_allSNPs_freqandhwe_table=Basu_Iyers_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_sd = sd(F_score))
Basu_Iyers_allSNPs_freqandhwe_table$Z_score=(Basu_Iyers_allSNPs_freqandhwe_table$F_score-Basu_Iyers_allSNPs_freqandhwe_table$Bin_F_average)/Basu_Iyers_allSNPs_freqandhwe_table$Bin_F_sd
#load bim file to get genetic position of each SNP
Basu_Iyers_bim=read.table("Basu_Iyers.bim", sep="", stringsAsFactors = FALSE)
colnames(Basu_Iyers_bim)=c("CHR", "SNP", "position_cm", "bp_coordinate", "A1", "A2")
Basu_Iyers_allSNPs_freqandhwe_table=merge(Basu_Iyers_allSNPs_freqandhwe_table, Basu_Iyers_bim, all.x=TRUE, by.x="SNP",by.y = "SNP", memory.limit(size=2500000))
#make Bin_kb for 400kb windows over genome 
max(Basu_Iyers_allSNPs_freqandhwe_table$bp_coordinate)
bins <- seq(0, 400000000, by=400000)
Basu_Iyers_allSNPs_freqandhwe_table$Bin_kb=cut(Basu_Iyers_allSNPs_freqandhwe_table$bp_coordinate, bins)
#add chromosome number to each Bin_kb value too for precise locus 
Basu_Iyers_allSNPs_freqandhwe_table$Bin_kb_Wchromosome=paste(Basu_Iyers_allSNPs_freqandhwe_table$CHR.x, Basu_Iyers_allSNPs_freqandhwe_table$Bin_kb)
#make column with average F and Z value for each bp bin
library(dplyr)
Basu_Iyers_allSNPs_freqandhwe_table=Basu_Iyers_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_F_average = mean(F_score))
Basu_Iyers_allSNPs_freqandhwe_table=Basu_Iyers_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_average = mean(Z_score))
#make freqtable
library(dplyr)
Basu_Iyers_allSNPs_freqandhwe_freq_table <- Basu_Iyers_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr:: summarise(Freq=n())
colnames(Basu_Iyers_allSNPs_freqandhwe_freq_table)=c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_freq")
#merge freqtable back with original 
Basu_Iyers_allSNPs_freqandhwe_table=merge(Basu_Iyers_allSNPs_freqandhwe_table, Basu_Iyers_allSNPs_freqandhwe_freq_table, all.x=TRUE, by="Bin_kb_Wchromosome")
#add proportion of Bin_kb with Z score over 1.96
Basu_Iyers_allSNPs_freqandhwe_table=Basu_Iyers_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_prop_sighigh = sum(Z_score>1.96)/Bin_kb_Wchromosome_freq)
#make aggregated table sIyerwing Iyers' bin_kb, bin_kb_F_avergae, bin_kb_Z_average, freq
Basu_Iyers_aggregated_table=unique(Basu_Iyers_allSNPs_freqandhwe_table[c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_F_average", "Bin_kb_Wchromosome_Z_average", "Bin_kb_Wchromosome_freq", "Bin_kb_Wchromosome_Z_prop_sighigh")])
#SLC24A5 is between base pair 46,200,461 to base pair 46,221,881 --> i.e. in bin 46,000,000 - 46,400,000 or 15 4.6e+07,4.64e+07, so find average F value for this genomic region
Iyer_SLC24A5_averageF=Basu_Iyers_aggregated_table$Bin_kb_Wchromosome_F_average[which(Basu_Iyers_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Iyer_SLC24A5_F_comparison=nrow(subset(Basu_Iyers_aggregated_table, Bin_kb_Wchromosome_F_average>Iyer_SLC24A5_averageF))/nrow(Basu_Iyers_aggregated_table)
#-->no evidence of excess Iyermo around SLC2415: 34.8% of genomic regions have higher F than Iyers
#find proportion of genomic regions with average Z value higher than that of SLC24A5 region
Iyer_SLC24A5_averageZ=Basu_Iyers_aggregated_table$Bin_kb_Wchromosome_Z_average[which(Basu_Iyers_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average Z value less than that of SLC24A5 region
Iyer_SLC24A5_Z_comparison=nrow(subset(Basu_Iyers_aggregated_table, Bin_kb_Wchromosome_Z_average<Iyer_SLC24A5_averageZ))/nrow(Basu_Iyers_aggregated_table)
#-->no evidence of excess Iyermo around SLC2415:91%


#JamatiaS
Basu_Jamatias_allSNPs_hwe_table=read.table("Basu_Jamatia_allSNPs_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Jamatias_allSNPs_freq_table=read.table("Basu_Jamatia_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Jamatias_allSNPs_freq_table=subset(Basu_Jamatias_allSNPs_freq_table, MAF>0)
Basu_Jamatias_allSNPs_freqandhwe_table=merge(Basu_Jamatias_allSNPs_freq_table, Basu_Jamatias_allSNPs_hwe_table, all.x=TRUE)
#then make F score column
Basu_Jamatias_allSNPs_freqandhwe_table$F_score=1-(Basu_Jamatias_allSNPs_freqandhwe_table$O.HET./Basu_Jamatias_allSNPs_freqandhwe_table$E.HET.)
#add MAF freq bins
bins <- seq(0, 1, by=0.05)
Basu_Jamatias_allSNPs_freqandhwe_table$Bin_MAF=cut(Basu_Jamatias_allSNPs_freqandhwe_table$MAF, bins)
#now convert each SNP's F score to Z score 
library(dplyr)
Basu_Jamatias_allSNPs_freqandhwe_table=Basu_Jamatias_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_average = mean(F_score))
Basu_Jamatias_allSNPs_freqandhwe_table=Basu_Jamatias_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_sd = sd(F_score))
Basu_Jamatias_allSNPs_freqandhwe_table$Z_score=(Basu_Jamatias_allSNPs_freqandhwe_table$F_score-Basu_Jamatias_allSNPs_freqandhwe_table$Bin_F_average)/Basu_Jamatias_allSNPs_freqandhwe_table$Bin_F_sd
#load bim file to get genetic position of each SNP
Basu_Jamatias_bim=read.table("Basu_Jamatias.bim", sep="", stringsAsFactors = FALSE)
colnames(Basu_Jamatias_bim)=c("CHR", "SNP", "position_cm", "bp_coordinate", "A1", "A2")
Basu_Jamatias_allSNPs_freqandhwe_table=merge(Basu_Jamatias_allSNPs_freqandhwe_table, Basu_Jamatias_bim, all.x=TRUE, by.x="SNP",by.y = "SNP", memory.limit(size=2500000))
#make Bin_kb for 400kb windows over genome 
max(Basu_Jamatias_allSNPs_freqandhwe_table$bp_coordinate)
bins <- seq(0, 400000000, by=400000)
Basu_Jamatias_allSNPs_freqandhwe_table$Bin_kb=cut(Basu_Jamatias_allSNPs_freqandhwe_table$bp_coordinate, bins)
#add chromosome number to each Bin_kb value too for precise locus 
Basu_Jamatias_allSNPs_freqandhwe_table$Bin_kb_Wchromosome=paste(Basu_Jamatias_allSNPs_freqandhwe_table$CHR.x, Basu_Jamatias_allSNPs_freqandhwe_table$Bin_kb)
#make column with average F and Z value for each bp bin
library(dplyr)
Basu_Jamatias_allSNPs_freqandhwe_table=Basu_Jamatias_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_F_average = mean(F_score))
Basu_Jamatias_allSNPs_freqandhwe_table=Basu_Jamatias_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_average = mean(Z_score))
#make freqtable
library(dplyr)
Basu_Jamatias_allSNPs_freqandhwe_freq_table <- Basu_Jamatias_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr:: summarise(Freq=n())
colnames(Basu_Jamatias_allSNPs_freqandhwe_freq_table)=c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_freq")
#merge freqtable back with original 
Basu_Jamatias_allSNPs_freqandhwe_table=merge(Basu_Jamatias_allSNPs_freqandhwe_table, Basu_Jamatias_allSNPs_freqandhwe_freq_table, all.x=TRUE, by="Bin_kb_Wchromosome")
#add proportion of Bin_kb with Z score over 1.96
Basu_Jamatias_allSNPs_freqandhwe_table=Basu_Jamatias_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_prop_sighigh = sum(Z_score>1.96)/Bin_kb_Wchromosome_freq)
#make aggregated table sJamatiawing Jamatias' bin_kb, bin_kb_F_avergae, bin_kb_Z_average, freq
Basu_Jamatias_aggregated_table=unique(Basu_Jamatias_allSNPs_freqandhwe_table[c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_F_average", "Bin_kb_Wchromosome_Z_average", "Bin_kb_Wchromosome_freq", "Bin_kb_Wchromosome_Z_prop_sighigh")])
#SLC24A5 is between base pair 46,200,461 to base pair 46,221,881 --> i.e. in bin 46,000,000 - 46,400,000 or 15 4.6e+07,4.64e+07, so find average F value for this genomic region
Jamatia_SLC24A5_averageF=Basu_Jamatias_aggregated_table$Bin_kb_Wchromosome_F_average[which(Basu_Jamatias_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Jamatia_SLC24A5_F_comparison=nrow(subset(Basu_Jamatias_aggregated_table, Bin_kb_Wchromosome_F_average>Jamatia_SLC24A5_averageF))/nrow(Basu_Jamatias_aggregated_table)
#-->no evidence of excess Jamatiamo around SLC2415: 45.6% of genomic regions have higher F than Jamatias
#find proportion of genomic regions with average Z value higher than that of SLC24A5 region
Jamatia_SLC24A5_averageZ=Basu_Jamatias_aggregated_table$Bin_kb_Wchromosome_Z_average[which(Basu_Jamatias_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Jamatia_SLC24A5_Z_comparison=nrow(subset(Basu_Jamatias_aggregated_table, Bin_kb_Wchromosome_Z_average>Jamatia_SLC24A5_averageZ))/nrow(Basu_Jamatias_aggregated_table)
#-->no evidence of excess homo around SLC2415:44%


#KadarS
Basu_Kadars_allSNPs_hwe_table=read.table("Basu_Kadar_allSNPs_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Kadars_allSNPs_freq_table=read.table("Basu_Kadar_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Kadars_allSNPs_freq_table=subset(Basu_Kadars_allSNPs_freq_table, MAF>0)
Basu_Kadars_allSNPs_freqandhwe_table=merge(Basu_Kadars_allSNPs_freq_table, Basu_Kadars_allSNPs_hwe_table, all.x=TRUE)
#then make F score column
Basu_Kadars_allSNPs_freqandhwe_table$F_score=1-(Basu_Kadars_allSNPs_freqandhwe_table$O.HET./Basu_Kadars_allSNPs_freqandhwe_table$E.HET.)
#add MAF freq bins
bins <- seq(0, 1, by=0.05)
Basu_Kadars_allSNPs_freqandhwe_table$Bin_MAF=cut(Basu_Kadars_allSNPs_freqandhwe_table$MAF, bins)
#now convert each SNP's F score to Z score 
library(dplyr)
Basu_Kadars_allSNPs_freqandhwe_table=Basu_Kadars_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_average = mean(F_score))
Basu_Kadars_allSNPs_freqandhwe_table=Basu_Kadars_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_sd = sd(F_score))
Basu_Kadars_allSNPs_freqandhwe_table$Z_score=(Basu_Kadars_allSNPs_freqandhwe_table$F_score-Basu_Kadars_allSNPs_freqandhwe_table$Bin_F_average)/Basu_Kadars_allSNPs_freqandhwe_table$Bin_F_sd
#load bim file to get genetic position of each SNP
Basu_Kadars_bim=read.table("Basu_Kadars.bim", sep="", stringsAsFactors = FALSE)
colnames(Basu_Kadars_bim)=c("CHR", "SNP", "position_cm", "bp_coordinate", "A1", "A2")
Basu_Kadars_allSNPs_freqandhwe_table=merge(Basu_Kadars_allSNPs_freqandhwe_table, Basu_Kadars_bim, all.x=TRUE, by.x="SNP",by.y = "SNP", memory.limit(size=2500000))
#make Bin_kb for 400kb windows over genome 
max(Basu_Kadars_allSNPs_freqandhwe_table$bp_coordinate)
bins <- seq(0, 400000000, by=400000)
Basu_Kadars_allSNPs_freqandhwe_table$Bin_kb=cut(Basu_Kadars_allSNPs_freqandhwe_table$bp_coordinate, bins)
#add chromosome number to each Bin_kb value too for precise locus 
Basu_Kadars_allSNPs_freqandhwe_table$Bin_kb_Wchromosome=paste(Basu_Kadars_allSNPs_freqandhwe_table$CHR.x, Basu_Kadars_allSNPs_freqandhwe_table$Bin_kb)
#make column with average F and Z value for each bp bin
library(dplyr)
Basu_Kadars_allSNPs_freqandhwe_table=Basu_Kadars_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_F_average = mean(F_score))
Basu_Kadars_allSNPs_freqandhwe_table=Basu_Kadars_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_average = mean(Z_score))
#make freqtable
library(dplyr)
Basu_Kadars_allSNPs_freqandhwe_freq_table <- Basu_Kadars_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr:: summarise(Freq=n())
colnames(Basu_Kadars_allSNPs_freqandhwe_freq_table)=c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_freq")
#merge freqtable back with original 
Basu_Kadars_allSNPs_freqandhwe_table=merge(Basu_Kadars_allSNPs_freqandhwe_table, Basu_Kadars_allSNPs_freqandhwe_freq_table, all.x=TRUE, by="Bin_kb_Wchromosome")
#add proportion of Bin_kb with Z score over 1.96
Basu_Kadars_allSNPs_freqandhwe_table=Basu_Kadars_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_prop_sighigh = sum(Z_score>1.96)/Bin_kb_Wchromosome_freq)
#make aggregated table sKadarwing Kadars' bin_kb, bin_kb_F_avergae, bin_kb_Z_average, freq
Basu_Kadars_aggregated_table=unique(Basu_Kadars_allSNPs_freqandhwe_table[c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_F_average", "Bin_kb_Wchromosome_Z_average", "Bin_kb_Wchromosome_freq", "Bin_kb_Wchromosome_Z_prop_sighigh")])
#SLC24A5 is between base pair 46,200,461 to base pair 46,221,881 --> i.e. in bin 46,000,000 - 46,400,000 or 15 4.6e+07,4.64e+07, so find average F value for this genomic region
Kadar_SLC24A5_averageF=Basu_Kadars_aggregated_table$Bin_kb_Wchromosome_F_average[which(Basu_Kadars_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Kadar_SLC24A5_F_comparison=nrow(subset(Basu_Kadars_aggregated_table, Bin_kb_Wchromosome_F_average>Kadar_SLC24A5_averageF))/nrow(Basu_Kadars_aggregated_table)
#-->no evidence of excess Kadarmo around SLC2415: 92.1% of genomic regions have higher F than Kadars
#find proportion of genomic regions with average Z value higher than that of SLC24A5 region
Kadar_SLC24A5_averageZ=Basu_Kadars_aggregated_table$Bin_kb_Wchromosome_Z_average[which(Basu_Kadars_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Kadar_SLC24A5_Z_comparison=nrow(subset(Basu_Kadars_aggregated_table, Bin_kb_Wchromosome_Z_average>Kadar_SLC24A5_averageZ))/nrow(Basu_Kadars_aggregated_table)
#-->no evidence of excess homo around SLC2415: 92%


#KhatriS
Basu_Khatris_allSNPs_hwe_table=read.table("Basu_Khatri_allSNPs_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Khatris_allSNPs_freq_table=read.table("Basu_Khatri_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Khatris_allSNPs_freq_table=subset(Basu_Khatris_allSNPs_freq_table, MAF>0)
Basu_Khatris_allSNPs_freqandhwe_table=merge(Basu_Khatris_allSNPs_freq_table, Basu_Khatris_allSNPs_hwe_table, all.x=TRUE)
#then make F score column
Basu_Khatris_allSNPs_freqandhwe_table$F_score=1-(Basu_Khatris_allSNPs_freqandhwe_table$O.HET./Basu_Khatris_allSNPs_freqandhwe_table$E.HET.)
#add MAF freq bins
bins <- seq(0, 1, by=0.05)
Basu_Khatris_allSNPs_freqandhwe_table$Bin_MAF=cut(Basu_Khatris_allSNPs_freqandhwe_table$MAF, bins)
#now convert each SNP's F score to Z score 
library(dplyr)
Basu_Khatris_allSNPs_freqandhwe_table=Basu_Khatris_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_average = mean(F_score))
Basu_Khatris_allSNPs_freqandhwe_table=Basu_Khatris_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_sd = sd(F_score))
Basu_Khatris_allSNPs_freqandhwe_table$Z_score=(Basu_Khatris_allSNPs_freqandhwe_table$F_score-Basu_Khatris_allSNPs_freqandhwe_table$Bin_F_average)/Basu_Khatris_allSNPs_freqandhwe_table$Bin_F_sd
#load bim file to get genetic position of each SNP
Basu_Khatris_bim=read.table("Basu_Khatris.bim", sep="", stringsAsFactors = FALSE)
colnames(Basu_Khatris_bim)=c("CHR", "SNP", "position_cm", "bp_coordinate", "A1", "A2")
Basu_Khatris_allSNPs_freqandhwe_table=merge(Basu_Khatris_allSNPs_freqandhwe_table, Basu_Khatris_bim, all.x=TRUE, by.x="SNP",by.y = "SNP", memory.limit(size=2500000))
#make Bin_kb for 400kb windows over genome 
max(Basu_Khatris_allSNPs_freqandhwe_table$bp_coordinate)
bins <- seq(0, 400000000, by=400000)
Basu_Khatris_allSNPs_freqandhwe_table$Bin_kb=cut(Basu_Khatris_allSNPs_freqandhwe_table$bp_coordinate, bins)
#add chromosome number to each Bin_kb value too for precise locus 
Basu_Khatris_allSNPs_freqandhwe_table$Bin_kb_Wchromosome=paste(Basu_Khatris_allSNPs_freqandhwe_table$CHR.x, Basu_Khatris_allSNPs_freqandhwe_table$Bin_kb)
#make column with average F and Z value for each bp bin
library(dplyr)
Basu_Khatris_allSNPs_freqandhwe_table=Basu_Khatris_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_F_average = mean(F_score))
Basu_Khatris_allSNPs_freqandhwe_table=Basu_Khatris_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_average = mean(Z_score))
#make freqtable
library(dplyr)
Basu_Khatris_allSNPs_freqandhwe_freq_table <- Basu_Khatris_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr:: summarise(Freq=n())
colnames(Basu_Khatris_allSNPs_freqandhwe_freq_table)=c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_freq")
#merge freqtable back with original 
Basu_Khatris_allSNPs_freqandhwe_table=merge(Basu_Khatris_allSNPs_freqandhwe_table, Basu_Khatris_allSNPs_freqandhwe_freq_table, all.x=TRUE, by="Bin_kb_Wchromosome")
#add proportion of Bin_kb with Z score over 1.96
Basu_Khatris_allSNPs_freqandhwe_table=Basu_Khatris_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_prop_sighigh = sum(Z_score>1.96)/Bin_kb_Wchromosome_freq)
#make aggregated table sKhatriwing Khatris' bin_kb, bin_kb_F_avergae, bin_kb_Z_average, freq
Basu_Khatris_aggregated_table=unique(Basu_Khatris_allSNPs_freqandhwe_table[c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_F_average", "Bin_kb_Wchromosome_Z_average", "Bin_kb_Wchromosome_freq", "Bin_kb_Wchromosome_Z_prop_sighigh")])
#SLC24A5 is between base pair 46,200,461 to base pair 46,221,881 --> i.e. in bin 46,000,000 - 46,400,000 or 15 4.6e+07,4.64e+07, so find average F value for this genomic region
Khatri_SLC24A5_averageF=Basu_Khatris_aggregated_table$Bin_kb_Wchromosome_F_average[which(Basu_Khatris_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Khatri_SLC24A5_F_comparison=nrow(subset(Basu_Khatris_aggregated_table, Bin_kb_Wchromosome_F_average>Khatri_SLC24A5_averageF))/nrow(Basu_Khatris_aggregated_table)
#-->no evidence of excess homo around SLC2415: 39% of genomic regions have higher F than Khatris
#find proportion of genomic regions with average Z value higher than that of SLC24A5 region
Khatri_SLC24A5_averageZ=Basu_Khatris_aggregated_table$Bin_kb_Wchromosome_Z_average[which(Basu_Khatris_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Khatri_SLC24A5_Z_comparison=nrow(subset(Basu_Khatris_aggregated_table, Bin_kb_Wchromosome_Z_average>Khatri_SLC24A5_averageZ))/nrow(Basu_Khatris_aggregated_table)
#-->no evidence of excess homo around SLC2415:33%


#KambojS
Pathak_Kambojs_allSNPs_hwe_table=read.table("Pathak_Kamboj_allSNPs_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header = TRUE)
Pathak_Kambojs_allSNPs_freq_table=read.table("Pathak_Kamboj_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Pathak_Kambojs_allSNPs_freq_table=subset(Pathak_Kambojs_allSNPs_freq_table, MAF>0)
Pathak_Kambojs_allSNPs_freqandhwe_table=merge(Pathak_Kambojs_allSNPs_freq_table, Pathak_Kambojs_allSNPs_hwe_table, all.x=TRUE)
#then make F score column
Pathak_Kambojs_allSNPs_freqandhwe_table$F_score=1-(Pathak_Kambojs_allSNPs_freqandhwe_table$O.HET./Pathak_Kambojs_allSNPs_freqandhwe_table$E.HET.)
#add MAF freq bins
bins <- seq(0, 1, by=0.05)
Pathak_Kambojs_allSNPs_freqandhwe_table$Bin_MAF=cut(Pathak_Kambojs_allSNPs_freqandhwe_table$MAF, bins)
#now convert each SNP's F score to Z score 
library(dplyr)
Pathak_Kambojs_allSNPs_freqandhwe_table=Pathak_Kambojs_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_average = mean(F_score))
Pathak_Kambojs_allSNPs_freqandhwe_table=Pathak_Kambojs_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_sd = sd(F_score))
Pathak_Kambojs_allSNPs_freqandhwe_table$Z_score=(Pathak_Kambojs_allSNPs_freqandhwe_table$F_score-Pathak_Kambojs_allSNPs_freqandhwe_table$Bin_F_average)/Pathak_Kambojs_allSNPs_freqandhwe_table$Bin_F_sd
#load bim file to get genetic position of each SNP
Pathak_Kambojs_bim=read.table("Pathak_Kambojs.bim", sep="", stringsAsFactors = FALSE)
colnames(Pathak_Kambojs_bim)=c("CHR", "SNP", "position_cm", "bp_coordinate", "A1", "A2")
Pathak_Kambojs_allSNPs_freqandhwe_table=merge(Pathak_Kambojs_allSNPs_freqandhwe_table, Pathak_Kambojs_bim, all.x=TRUE, by.x="SNP",by.y = "SNP", memory.limit(size=2500000))
#make Bin_kb for 400kb windows over genome 
max(Pathak_Kambojs_allSNPs_freqandhwe_table$bp_coordinate)
bins <- seq(0, 400000000, by=400000)
Pathak_Kambojs_allSNPs_freqandhwe_table$Bin_kb=cut(Pathak_Kambojs_allSNPs_freqandhwe_table$bp_coordinate, bins)
#add chromosome number to each Bin_kb value too for precise locus 
Pathak_Kambojs_allSNPs_freqandhwe_table$Bin_kb_Wchromosome=paste(Pathak_Kambojs_allSNPs_freqandhwe_table$CHR.x, Pathak_Kambojs_allSNPs_freqandhwe_table$Bin_kb)
#make column with average F and Z value for each bp bin
library(dplyr)
Pathak_Kambojs_allSNPs_freqandhwe_table=Pathak_Kambojs_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_F_average = mean(F_score))
Pathak_Kambojs_allSNPs_freqandhwe_table=Pathak_Kambojs_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_average = mean(Z_score))
#make freqtable
library(dplyr)
Pathak_Kambojs_allSNPs_freqandhwe_freq_table <- Pathak_Kambojs_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr:: summarise(Freq=n())
colnames(Pathak_Kambojs_allSNPs_freqandhwe_freq_table)=c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_freq")
#merge freqtable back with original 
Pathak_Kambojs_allSNPs_freqandhwe_table=merge(Pathak_Kambojs_allSNPs_freqandhwe_table, Pathak_Kambojs_allSNPs_freqandhwe_freq_table, all.x=TRUE, by="Bin_kb_Wchromosome")
#add proportion of Bin_kb with Z score over 1.96
Pathak_Kambojs_allSNPs_freqandhwe_table=Pathak_Kambojs_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_prop_sighigh = sum(Z_score>1.96)/Bin_kb_Wchromosome_freq)
#make aggregated table showing Kambojs' bin_kb, bin_kb_F_avergae, bin_kb_Z_average, freq
Pathak_Kambojs_aggregated_table=unique(Pathak_Kambojs_allSNPs_freqandhwe_table[c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_F_average", "Bin_kb_Wchromosome_Z_average", "Bin_kb_Wchromosome_freq", "Bin_kb_Wchromosome_Z_prop_sighigh")])
#SLC24A5 is between base pair 46,200,461 to base pair 46,221,881 --> i.e. in bin 46,000,000 - 46,400,000 or 15 4.6e+07,4.64e+07, so find average F value for this genomic region
Kamboj_SLC24A5_averageF=Pathak_Kambojs_aggregated_table$Bin_kb_Wchromosome_F_average[which(Pathak_Kambojs_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Kamboj_SLC24A5_F_comparison=nrow(subset(Pathak_Kambojs_aggregated_table, Bin_kb_Wchromosome_F_average>Kamboj_SLC24A5_averageF))/nrow(Pathak_Kambojs_aggregated_table)
#-->no evidence of excess homo around SLC2415: 94.7% of genomic regions have higher F than Kambojs
#find proportion of genomic regions with average Z value higher than that of SLC24A5 region
Kamboj_SLC24A5_averageZ=Pathak_Kambojs_aggregated_table$Bin_kb_Wchromosome_Z_average[which(Pathak_Kambojs_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average Z value less than that of SLC24A5 region
Kamboj_SLC24A5_Z_comparison=nrow(subset(Pathak_Kambojs_aggregated_table, Bin_kb_Wchromosome_Z_average<Kamboj_SLC24A5_averageZ))/nrow(Pathak_Kambojs_aggregated_table)
#-->no evidence of excess homo around SLC2415:93%

#KorwaS
Basu_Korwas_allSNPs_hwe_table=read.table("Basu_Korwa_allSNPs_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Korwas_allSNPs_freq_table=read.table("Basu_Korwa_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Korwas_allSNPs_freq_table=subset(Basu_Korwas_allSNPs_freq_table, MAF>0)
Basu_Korwas_allSNPs_freqandhwe_table=merge(Basu_Korwas_allSNPs_freq_table, Basu_Korwas_allSNPs_hwe_table, all.x=TRUE)
#then make F score column
Basu_Korwas_allSNPs_freqandhwe_table$F_score=1-(Basu_Korwas_allSNPs_freqandhwe_table$O.HET./Basu_Korwas_allSNPs_freqandhwe_table$E.HET.)
#add MAF freq bins
bins <- seq(0, 1, by=0.05)
Basu_Korwas_allSNPs_freqandhwe_table$Bin_MAF=cut(Basu_Korwas_allSNPs_freqandhwe_table$MAF, bins)
#now convert each SNP's F score to Z score 
library(dplyr)
Basu_Korwas_allSNPs_freqandhwe_table=Basu_Korwas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_average = mean(F_score))
Basu_Korwas_allSNPs_freqandhwe_table=Basu_Korwas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_sd = sd(F_score))
Basu_Korwas_allSNPs_freqandhwe_table$Z_score=(Basu_Korwas_allSNPs_freqandhwe_table$F_score-Basu_Korwas_allSNPs_freqandhwe_table$Bin_F_average)/Basu_Korwas_allSNPs_freqandhwe_table$Bin_F_sd
#load bim file to get genetic position of each SNP
Basu_Korwas_bim=read.table("Basu_Korwas.bim", sep="", stringsAsFactors = FALSE)
colnames(Basu_Korwas_bim)=c("CHR", "SNP", "position_cm", "bp_coordinate", "A1", "A2")
Basu_Korwas_allSNPs_freqandhwe_table=merge(Basu_Korwas_allSNPs_freqandhwe_table, Basu_Korwas_bim, all.x=TRUE, by.x="SNP",by.y = "SNP", memory.limit(size=2500000))
#make Bin_kb for 400kb windows over genome 
max(Basu_Korwas_allSNPs_freqandhwe_table$bp_coordinate)
bins <- seq(0, 400000000, by=400000)
Basu_Korwas_allSNPs_freqandhwe_table$Bin_kb=cut(Basu_Korwas_allSNPs_freqandhwe_table$bp_coordinate, bins)
#add chromosome number to each Bin_kb value too for precise locus 
Basu_Korwas_allSNPs_freqandhwe_table$Bin_kb_Wchromosome=paste(Basu_Korwas_allSNPs_freqandhwe_table$CHR.x, Basu_Korwas_allSNPs_freqandhwe_table$Bin_kb)
#make column with average F and Z value for each bp bin
library(dplyr)
Basu_Korwas_allSNPs_freqandhwe_table=Basu_Korwas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_F_average = mean(F_score))
Basu_Korwas_allSNPs_freqandhwe_table=Basu_Korwas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_average = mean(Z_score))
#make freqtable
library(dplyr)
Basu_Korwas_allSNPs_freqandhwe_freq_table <- Basu_Korwas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr:: summarise(Freq=n())
colnames(Basu_Korwas_allSNPs_freqandhwe_freq_table)=c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_freq")
#merge freqtable back with original 
Basu_Korwas_allSNPs_freqandhwe_table=merge(Basu_Korwas_allSNPs_freqandhwe_table, Basu_Korwas_allSNPs_freqandhwe_freq_table, all.x=TRUE, by="Bin_kb_Wchromosome")
#add proportion of Bin_kb with Z score over 1.96
Basu_Korwas_allSNPs_freqandhwe_table=Basu_Korwas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_prop_sighigh = sum(Z_score>1.96)/Bin_kb_Wchromosome_freq)
#make aggregated table sKorwawing Korwas' bin_kb, bin_kb_F_avergae, bin_kb_Z_average, freq
Basu_Korwas_aggregated_table=unique(Basu_Korwas_allSNPs_freqandhwe_table[c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_F_average", "Bin_kb_Wchromosome_Z_average", "Bin_kb_Wchromosome_freq", "Bin_kb_Wchromosome_Z_prop_sighigh")])
#SLC24A5 is between base pair 46,200,461 to base pair 46,221,881 --> i.e. in bin 46,000,000 - 46,400,000 or 15 4.6e+07,4.64e+07, so find average F value for this genomic region
Korwa_SLC24A5_averageF=Basu_Korwas_aggregated_table$Bin_kb_Wchromosome_F_average[which(Basu_Korwas_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Korwa_SLC24A5_F_comparison=nrow(subset(Basu_Korwas_aggregated_table, Bin_kb_Wchromosome_F_average>Korwa_SLC24A5_averageF))/nrow(Basu_Korwas_aggregated_table)
#-->no evidence of excess homo around SLC2415: 16.3% of genomic regions have higher F than Korwas
#find proportion of genomic regions with average Z value higher than that of SLC24A5 region
Korwa_SLC24A5_averageZ=Basu_Korwas_aggregated_table$Bin_kb_Wchromosome_Z_average[which(Basu_Korwas_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Korwa_SLC24A5_Z_comparison=nrow(subset(Basu_Korwas_aggregated_table, Bin_kb_Wchromosome_Z_average>Korwa_SLC24A5_averageZ))/nrow(Basu_Korwas_aggregated_table)
#-->no evidence of excess homo around SLC2415:17%


#ManipuriBrahminS
Basu_ManipuriBrahmins_allSNPs_hwe_table=read.table("Basu_ManipuriBrahmin_allSNPs_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_ManipuriBrahmins_allSNPs_freq_table=read.table("Basu_ManipuriBrahmin_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_ManipuriBrahmins_allSNPs_freq_table=subset(Basu_ManipuriBrahmins_allSNPs_freq_table, MAF>0)
Basu_ManipuriBrahmins_allSNPs_freqandhwe_table=merge(Basu_ManipuriBrahmins_allSNPs_freq_table, Basu_ManipuriBrahmins_allSNPs_hwe_table, all.x=TRUE)
#then make F score column
Basu_ManipuriBrahmins_allSNPs_freqandhwe_table$F_score=1-(Basu_ManipuriBrahmins_allSNPs_freqandhwe_table$O.HET./Basu_ManipuriBrahmins_allSNPs_freqandhwe_table$E.HET.)
#add MAF freq bins
bins <- seq(0, 1, by=0.05)
Basu_ManipuriBrahmins_allSNPs_freqandhwe_table$Bin_MAF=cut(Basu_ManipuriBrahmins_allSNPs_freqandhwe_table$MAF, bins)
#now convert each SNP's F score to Z score 
library(dplyr)
Basu_ManipuriBrahmins_allSNPs_freqandhwe_table=Basu_ManipuriBrahmins_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_average = mean(F_score))
Basu_ManipuriBrahmins_allSNPs_freqandhwe_table=Basu_ManipuriBrahmins_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_sd = sd(F_score))
Basu_ManipuriBrahmins_allSNPs_freqandhwe_table$Z_score=(Basu_ManipuriBrahmins_allSNPs_freqandhwe_table$F_score-Basu_ManipuriBrahmins_allSNPs_freqandhwe_table$Bin_F_average)/Basu_ManipuriBrahmins_allSNPs_freqandhwe_table$Bin_F_sd
#load bim file to get genetic position of each SNP
Basu_ManipuriBrahmins_bim=read.table("Basu_ManipuriBrahmins.bim", sep="", stringsAsFactors = FALSE)
colnames(Basu_ManipuriBrahmins_bim)=c("CHR", "SNP", "position_cm", "bp_coordinate", "A1", "A2")
Basu_ManipuriBrahmins_allSNPs_freqandhwe_table=merge(Basu_ManipuriBrahmins_allSNPs_freqandhwe_table, Basu_ManipuriBrahmins_bim, all.x=TRUE, by.x="SNP",by.y = "SNP", memory.limit(size=2500000))
#make Bin_kb for 400kb windows over genome 
max(Basu_ManipuriBrahmins_allSNPs_freqandhwe_table$bp_coordinate)
bins <- seq(0, 400000000, by=400000)
Basu_ManipuriBrahmins_allSNPs_freqandhwe_table$Bin_kb=cut(Basu_ManipuriBrahmins_allSNPs_freqandhwe_table$bp_coordinate, bins)
#add chromosome number to each Bin_kb value too for precise locus 
Basu_ManipuriBrahmins_allSNPs_freqandhwe_table$Bin_kb_Wchromosome=paste(Basu_ManipuriBrahmins_allSNPs_freqandhwe_table$CHR.x, Basu_ManipuriBrahmins_allSNPs_freqandhwe_table$Bin_kb)
#make column with average F and Z value for each bp bin
library(dplyr)
Basu_ManipuriBrahmins_allSNPs_freqandhwe_table=Basu_ManipuriBrahmins_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_F_average = mean(F_score))
Basu_ManipuriBrahmins_allSNPs_freqandhwe_table=Basu_ManipuriBrahmins_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_average = mean(Z_score))
#make freqtable
library(dplyr)
Basu_ManipuriBrahmins_allSNPs_freqandhwe_freq_table <- Basu_ManipuriBrahmins_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr:: summarise(Freq=n())
colnames(Basu_ManipuriBrahmins_allSNPs_freqandhwe_freq_table)=c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_freq")
#merge freqtable back with original 
Basu_ManipuriBrahmins_allSNPs_freqandhwe_table=merge(Basu_ManipuriBrahmins_allSNPs_freqandhwe_table, Basu_ManipuriBrahmins_allSNPs_freqandhwe_freq_table, all.x=TRUE, by="Bin_kb_Wchromosome")
#add proportion of Bin_kb with Z score over 1.96
Basu_ManipuriBrahmins_allSNPs_freqandhwe_table=Basu_ManipuriBrahmins_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_prop_sighigh = sum(Z_score>1.96)/Bin_kb_Wchromosome_freq)
#make aggregated table sManipuriBrahminwing ManipuriBrahmins' bin_kb, bin_kb_F_avergae, bin_kb_Z_average, freq
Basu_ManipuriBrahmins_aggregated_table=unique(Basu_ManipuriBrahmins_allSNPs_freqandhwe_table[c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_F_average", "Bin_kb_Wchromosome_Z_average", "Bin_kb_Wchromosome_freq", "Bin_kb_Wchromosome_Z_prop_sighigh")])
#SLC24A5 is between base pair 46,200,461 to base pair 46,221,881 --> i.e. in bin 46,000,000 - 46,400,000 or 15 4.6e+07,4.64e+07, so find average F value for this genomic region
ManipuriBrahmin_SLC24A5_averageF=Basu_ManipuriBrahmins_aggregated_table$Bin_kb_Wchromosome_F_average[which(Basu_ManipuriBrahmins_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
ManipuriBrahmin_SLC24A5_F_comparison=nrow(subset(Basu_ManipuriBrahmins_aggregated_table, Bin_kb_Wchromosome_F_average>ManipuriBrahmin_SLC24A5_averageF))/nrow(Basu_ManipuriBrahmins_aggregated_table)
#-->no evidence of excess ManipuriBrahminmo around SLC2415: 43.6% of genomic regions have higher F than ManipuriBrahmins
#find proportion of genomic regions with average Z value higher than that of SLC24A5 region
ManipuriBrahmin_SLC24A5_averageZ=Basu_ManipuriBrahmins_aggregated_table$Bin_kb_Wchromosome_Z_average[which(Basu_ManipuriBrahmins_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
ManipuriBrahmin_SLC24A5_Z_comparison=nrow(subset(Basu_ManipuriBrahmins_aggregated_table, Bin_kb_Wchromosome_Z_average>ManipuriBrahmin_SLC24A5_averageZ))/nrow(Basu_ManipuriBrahmins_aggregated_table)
#-->no evidence of excess homo around SLC2415:45%


#MarathaS
Basu_Marathas_allSNPs_hwe_table=read.table("Basu_Maratha_allSNPs_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Marathas_allSNPs_freq_table=read.table("Basu_Maratha_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Marathas_allSNPs_freq_table=subset(Basu_Marathas_allSNPs_freq_table, MAF>0)
Basu_Marathas_allSNPs_freqandhwe_table=merge(Basu_Marathas_allSNPs_freq_table, Basu_Marathas_allSNPs_hwe_table, all.x=TRUE)
#then make F score column
Basu_Marathas_allSNPs_freqandhwe_table$F_score=1-(Basu_Marathas_allSNPs_freqandhwe_table$O.HET./Basu_Marathas_allSNPs_freqandhwe_table$E.HET.)
#add MAF freq bins
bins <- seq(0, 1, by=0.05)
Basu_Marathas_allSNPs_freqandhwe_table$Bin_MAF=cut(Basu_Marathas_allSNPs_freqandhwe_table$MAF, bins)
#now convert each SNP's F score to Z score 
library(dplyr)
Basu_Marathas_allSNPs_freqandhwe_table=Basu_Marathas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_average = mean(F_score))
Basu_Marathas_allSNPs_freqandhwe_table=Basu_Marathas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_sd = sd(F_score))
Basu_Marathas_allSNPs_freqandhwe_table$Z_score=(Basu_Marathas_allSNPs_freqandhwe_table$F_score-Basu_Marathas_allSNPs_freqandhwe_table$Bin_F_average)/Basu_Marathas_allSNPs_freqandhwe_table$Bin_F_sd
#load bim file to get genetic position of each SNP
Basu_Marathas_bim=read.table("Basu_Marathas.bim", sep="", stringsAsFactors = FALSE)
colnames(Basu_Marathas_bim)=c("CHR", "SNP", "position_cm", "bp_coordinate", "A1", "A2")
Basu_Marathas_allSNPs_freqandhwe_table=merge(Basu_Marathas_allSNPs_freqandhwe_table, Basu_Marathas_bim, all.x=TRUE, by.x="SNP",by.y = "SNP", memory.limit(size=2500000))
#make Bin_kb for 400kb windows over genome 
max(Basu_Marathas_allSNPs_freqandhwe_table$bp_coordinate)
bins <- seq(0, 400000000, by=400000)
Basu_Marathas_allSNPs_freqandhwe_table$Bin_kb=cut(Basu_Marathas_allSNPs_freqandhwe_table$bp_coordinate, bins)
#add chromosome number to each Bin_kb value too for precise locus 
Basu_Marathas_allSNPs_freqandhwe_table$Bin_kb_Wchromosome=paste(Basu_Marathas_allSNPs_freqandhwe_table$CHR.x, Basu_Marathas_allSNPs_freqandhwe_table$Bin_kb)
#make column with average F and Z value for each bp bin
library(dplyr)
Basu_Marathas_allSNPs_freqandhwe_table=Basu_Marathas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_F_average = mean(F_score))
Basu_Marathas_allSNPs_freqandhwe_table=Basu_Marathas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_average = mean(Z_score))
#make freqtable
library(dplyr)
Basu_Marathas_allSNPs_freqandhwe_freq_table <- Basu_Marathas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr:: summarise(Freq=n())
colnames(Basu_Marathas_allSNPs_freqandhwe_freq_table)=c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_freq")
#merge freqtable back with original 
Basu_Marathas_allSNPs_freqandhwe_table=merge(Basu_Marathas_allSNPs_freqandhwe_table, Basu_Marathas_allSNPs_freqandhwe_freq_table, all.x=TRUE, by="Bin_kb_Wchromosome")
#add proportion of Bin_kb with Z score over 1.96
Basu_Marathas_allSNPs_freqandhwe_table=Basu_Marathas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_prop_sighigh = sum(Z_score>1.96)/Bin_kb_Wchromosome_freq)
#make aggregated table sMarathawing Marathas' bin_kb, bin_kb_F_avergae, bin_kb_Z_average, freq
Basu_Marathas_aggregated_table=unique(Basu_Marathas_allSNPs_freqandhwe_table[c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_F_average", "Bin_kb_Wchromosome_Z_average", "Bin_kb_Wchromosome_freq", "Bin_kb_Wchromosome_Z_prop_sighigh")])
#SLC24A5 is between base pair 46,200,461 to base pair 46,221,881 --> i.e. in bin 46,000,000 - 46,400,000 or 15 4.6e+07,4.64e+07, so find average F value for this genomic region
Maratha_SLC24A5_averageF=Basu_Marathas_aggregated_table$Bin_kb_Wchromosome_F_average[which(Basu_Marathas_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Maratha_SLC24A5_F_comparison=nrow(subset(Basu_Marathas_aggregated_table, Bin_kb_Wchromosome_F_average>Maratha_SLC24A5_averageF))/nrow(Basu_Marathas_aggregated_table)
#-->no evidence of excess homo around SLC2415: 50.1% of genomic regions have higher F than Marathas
#find proportion of genomic regions with average Z value higher than that of SLC24A5 region
Maratha_SLC24A5_averageZ=Basu_Marathas_aggregated_table$Bin_kb_Wchromosome_Z_average[which(Basu_Marathas_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Maratha_SLC24A5_Z_comparison=nrow(subset(Basu_Marathas_aggregated_table, Bin_kb_Wchromosome_Z_average>Maratha_SLC24A5_averageZ))/nrow(Basu_Marathas_aggregated_table)
#-->no evidence of excess homo around SLC2415:44%

#PallanS
Basu_Pallans_allSNPs_hwe_table=read.table("Basu_Pallan_allSNPs_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Pallans_allSNPs_freq_table=read.table("Basu_Pallan_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Pallans_allSNPs_freq_table=subset(Basu_Pallans_allSNPs_freq_table, MAF>0)
Basu_Pallans_allSNPs_freqandhwe_table=merge(Basu_Pallans_allSNPs_freq_table, Basu_Pallans_allSNPs_hwe_table, all.x=TRUE)
#then make F score column
Basu_Pallans_allSNPs_freqandhwe_table$F_score=1-(Basu_Pallans_allSNPs_freqandhwe_table$O.HET./Basu_Pallans_allSNPs_freqandhwe_table$E.HET.)
#add MAF freq bins
bins <- seq(0, 1, by=0.05)
Basu_Pallans_allSNPs_freqandhwe_table$Bin_MAF=cut(Basu_Pallans_allSNPs_freqandhwe_table$MAF, bins)
#now convert each SNP's F score to Z score 
library(dplyr)
Basu_Pallans_allSNPs_freqandhwe_table=Basu_Pallans_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_average = mean(F_score))
Basu_Pallans_allSNPs_freqandhwe_table=Basu_Pallans_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_sd = sd(F_score))
Basu_Pallans_allSNPs_freqandhwe_table$Z_score=(Basu_Pallans_allSNPs_freqandhwe_table$F_score-Basu_Pallans_allSNPs_freqandhwe_table$Bin_F_average)/Basu_Pallans_allSNPs_freqandhwe_table$Bin_F_sd
#load bim file to get genetic position of each SNP
Basu_Pallans_bim=read.table("Basu_Pallans.bim", sep="", stringsAsFactors = FALSE)
colnames(Basu_Pallans_bim)=c("CHR", "SNP", "position_cm", "bp_coordinate", "A1", "A2")
Basu_Pallans_allSNPs_freqandhwe_table=merge(Basu_Pallans_allSNPs_freqandhwe_table, Basu_Pallans_bim, all.x=TRUE, by.x="SNP",by.y = "SNP", memory.limit(size=2500000))
#make Bin_kb for 400kb windows over genome 
max(Basu_Pallans_allSNPs_freqandhwe_table$bp_coordinate)
bins <- seq(0, 400000000, by=400000)
Basu_Pallans_allSNPs_freqandhwe_table$Bin_kb=cut(Basu_Pallans_allSNPs_freqandhwe_table$bp_coordinate, bins)
#add chromosome number to each Bin_kb value too for precise locus 
Basu_Pallans_allSNPs_freqandhwe_table$Bin_kb_Wchromosome=paste(Basu_Pallans_allSNPs_freqandhwe_table$CHR.x, Basu_Pallans_allSNPs_freqandhwe_table$Bin_kb)
#make column with average F and Z value for each bp bin
library(dplyr)
Basu_Pallans_allSNPs_freqandhwe_table=Basu_Pallans_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_F_average = mean(F_score))
Basu_Pallans_allSNPs_freqandhwe_table=Basu_Pallans_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_average = mean(Z_score))
#make freqtable
library(dplyr)
Basu_Pallans_allSNPs_freqandhwe_freq_table <- Basu_Pallans_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr:: summarise(Freq=n())
colnames(Basu_Pallans_allSNPs_freqandhwe_freq_table)=c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_freq")
#merge freqtable back with original 
Basu_Pallans_allSNPs_freqandhwe_table=merge(Basu_Pallans_allSNPs_freqandhwe_table, Basu_Pallans_allSNPs_freqandhwe_freq_table, all.x=TRUE, by="Bin_kb_Wchromosome")
#add proportion of Bin_kb with Z score over 1.96
Basu_Pallans_allSNPs_freqandhwe_table=Basu_Pallans_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_prop_sighigh = sum(Z_score>1.96)/Bin_kb_Wchromosome_freq)
#make aggregated table sPallanwing Pallans' bin_kb, bin_kb_F_avergae, bin_kb_Z_average, freq
Basu_Pallans_aggregated_table=unique(Basu_Pallans_allSNPs_freqandhwe_table[c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_F_average", "Bin_kb_Wchromosome_Z_average", "Bin_kb_Wchromosome_freq", "Bin_kb_Wchromosome_Z_prop_sighigh")])
#SLC24A5 is between base pair 46,200,461 to base pair 46,221,881 --> i.e. in bin 46,000,000 - 46,400,000 or 15 4.6e+07,4.64e+07, so find average F value for this genomic region
Pallan_SLC24A5_averageF=Basu_Pallans_aggregated_table$Bin_kb_Wchromosome_F_average[which(Basu_Pallans_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Pallan_SLC24A5_F_comparison=nrow(subset(Basu_Pallans_aggregated_table, Bin_kb_Wchromosome_F_average>Pallan_SLC24A5_averageF))/nrow(Basu_Pallans_aggregated_table)
#-->no evidence of excess homo around SLC2415: 48.2% of genomic regions have higher F than Pallans
#find proportion of genomic regions with average Z value higher than that of SLC24A5 region
Pallan_SLC24A5_averageZ=Basu_Pallans_aggregated_table$Bin_kb_Wchromosome_Z_average[which(Basu_Pallans_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average Z value less than that of SLC24A5 region
Pallan_SLC24A5_Z_comparison=nrow(subset(Basu_Pallans_aggregated_table, Bin_kb_Wchromosome_Z_average<Pallan_SLC24A5_averageZ))/nrow(Basu_Pallans_aggregated_table)
#-->no evidence of excess homo around SLC2415:49%


#PaniyaS
Basu_Paniyas_allSNPs_hwe_table=read.table("Basu_Paniya_allSNPs_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Paniyas_allSNPs_freq_table=read.table("Basu_Paniya_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Paniyas_allSNPs_freq_table=subset(Basu_Paniyas_allSNPs_freq_table, MAF>0)
Basu_Paniyas_allSNPs_freqandhwe_table=merge(Basu_Paniyas_allSNPs_freq_table, Basu_Paniyas_allSNPs_hwe_table, all.x=TRUE)
#then make F score column
Basu_Paniyas_allSNPs_freqandhwe_table$F_score=1-(Basu_Paniyas_allSNPs_freqandhwe_table$O.HET./Basu_Paniyas_allSNPs_freqandhwe_table$E.HET.)
#add MAF freq bins
bins <- seq(0, 1, by=0.05)
Basu_Paniyas_allSNPs_freqandhwe_table$Bin_MAF=cut(Basu_Paniyas_allSNPs_freqandhwe_table$MAF, bins)
#now convert each SNP's F score to Z score 
library(dplyr)
Basu_Paniyas_allSNPs_freqandhwe_table=Basu_Paniyas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_average = mean(F_score))
Basu_Paniyas_allSNPs_freqandhwe_table=Basu_Paniyas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_sd = sd(F_score))
Basu_Paniyas_allSNPs_freqandhwe_table$Z_score=(Basu_Paniyas_allSNPs_freqandhwe_table$F_score-Basu_Paniyas_allSNPs_freqandhwe_table$Bin_F_average)/Basu_Paniyas_allSNPs_freqandhwe_table$Bin_F_sd
#load bim file to get genetic position of each SNP
Basu_Paniyas_bim=read.table("Basu_Paniyas.bim", sep="", stringsAsFactors = FALSE)
colnames(Basu_Paniyas_bim)=c("CHR", "SNP", "position_cm", "bp_coordinate", "A1", "A2")
Basu_Paniyas_allSNPs_freqandhwe_table=merge(Basu_Paniyas_allSNPs_freqandhwe_table, Basu_Paniyas_bim, all.x=TRUE, by.x="SNP",by.y = "SNP", memory.limit(size=2500000))
#make Bin_kb for 400kb windows over genome 
max(Basu_Paniyas_allSNPs_freqandhwe_table$bp_coordinate)
bins <- seq(0, 400000000, by=400000)
Basu_Paniyas_allSNPs_freqandhwe_table$Bin_kb=cut(Basu_Paniyas_allSNPs_freqandhwe_table$bp_coordinate, bins)
#add chromosome number to each Bin_kb value too for precise locus 
Basu_Paniyas_allSNPs_freqandhwe_table$Bin_kb_Wchromosome=paste(Basu_Paniyas_allSNPs_freqandhwe_table$CHR.x, Basu_Paniyas_allSNPs_freqandhwe_table$Bin_kb)
#make column with average F and Z value for each bp bin
library(dplyr)
Basu_Paniyas_allSNPs_freqandhwe_table=Basu_Paniyas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_F_average = mean(F_score))
Basu_Paniyas_allSNPs_freqandhwe_table=Basu_Paniyas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_average = mean(Z_score))
#make freqtable
library(dplyr)
Basu_Paniyas_allSNPs_freqandhwe_freq_table <- Basu_Paniyas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr:: summarise(Freq=n())
colnames(Basu_Paniyas_allSNPs_freqandhwe_freq_table)=c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_freq")
#merge freqtable back with original 
Basu_Paniyas_allSNPs_freqandhwe_table=merge(Basu_Paniyas_allSNPs_freqandhwe_table, Basu_Paniyas_allSNPs_freqandhwe_freq_table, all.x=TRUE, by="Bin_kb_Wchromosome")
#add proportion of Bin_kb with Z score over 1.96
Basu_Paniyas_allSNPs_freqandhwe_table=Basu_Paniyas_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_prop_sighigh = sum(Z_score>1.96)/Bin_kb_Wchromosome_freq)
#make aggregated table sPaniyawing Paniyas' bin_kb, bin_kb_F_avergae, bin_kb_Z_average, freq
Basu_Paniyas_aggregated_table=unique(Basu_Paniyas_allSNPs_freqandhwe_table[c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_F_average", "Bin_kb_Wchromosome_Z_average", "Bin_kb_Wchromosome_freq", "Bin_kb_Wchromosome_Z_prop_sighigh")])
#SLC24A5 is between base pair 46,200,461 to base pair 46,221,881 --> i.e. in bin 46,000,000 - 46,400,000 or 15 4.6e+07,4.64e+07, so find average F value for this genomic region
Paniya_SLC24A5_averageF=Basu_Paniyas_aggregated_table$Bin_kb_Wchromosome_F_average[which(Basu_Paniyas_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Paniya_SLC24A5_F_comparison=nrow(subset(Basu_Paniyas_aggregated_table, Bin_kb_Wchromosome_F_average>Paniya_SLC24A5_averageF))/nrow(Basu_Paniyas_aggregated_table)
#-->no evidence of excess homo around SLC2415: 29.8% of genomic regions have higher F than Paniyas
#find proportion of genomic regions with average Z value higher than that of SLC24A5 region
Paniya_SLC24A5_averageZ=Basu_Paniyas_aggregated_table$Bin_kb_Wchromosome_Z_average[which(Basu_Paniyas_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Paniya_SLC24A5_Z_comparison=nrow(subset(Basu_Paniyas_aggregated_table, Bin_kb_Wchromosome_Z_average>Paniya_SLC24A5_averageZ))/nrow(Basu_Paniyas_aggregated_table)
#-->no evidence of excess homo around SLC2415:27%


#TharuS
Basu_Tharus_allSNPs_hwe_table=read.table("Basu_Tharu_allSNPs_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Tharus_allSNPs_freq_table=read.table("Basu_Tharu_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Tharus_allSNPs_freq_table=subset(Basu_Tharus_allSNPs_freq_table, MAF>0)
Basu_Tharus_allSNPs_freqandhwe_table=merge(Basu_Tharus_allSNPs_freq_table, Basu_Tharus_allSNPs_hwe_table, all.x=TRUE)
#then make F score column
Basu_Tharus_allSNPs_freqandhwe_table$F_score=1-(Basu_Tharus_allSNPs_freqandhwe_table$O.HET./Basu_Tharus_allSNPs_freqandhwe_table$E.HET.)
#add MAF freq bins
bins <- seq(0, 1, by=0.05)
Basu_Tharus_allSNPs_freqandhwe_table$Bin_MAF=cut(Basu_Tharus_allSNPs_freqandhwe_table$MAF, bins)
#now convert each SNP's F score to Z score 
library(dplyr)
Basu_Tharus_allSNPs_freqandhwe_table=Basu_Tharus_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_average = mean(F_score))
Basu_Tharus_allSNPs_freqandhwe_table=Basu_Tharus_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_sd = sd(F_score))
Basu_Tharus_allSNPs_freqandhwe_table$Z_score=(Basu_Tharus_allSNPs_freqandhwe_table$F_score-Basu_Tharus_allSNPs_freqandhwe_table$Bin_F_average)/Basu_Tharus_allSNPs_freqandhwe_table$Bin_F_sd
#load bim file to get genetic position of each SNP
Basu_Tharus_bim=read.table("Basu_Tharus.bim", sep="", stringsAsFactors = FALSE)
colnames(Basu_Tharus_bim)=c("CHR", "SNP", "position_cm", "bp_coordinate", "A1", "A2")
Basu_Tharus_allSNPs_freqandhwe_table=merge(Basu_Tharus_allSNPs_freqandhwe_table, Basu_Tharus_bim, all.x=TRUE, by.x="SNP",by.y = "SNP", memory.limit(size=2500000))
#make Bin_kb for 400kb windows over genome 
max(Basu_Tharus_allSNPs_freqandhwe_table$bp_coordinate)
bins <- seq(0, 400000000, by=400000)
Basu_Tharus_allSNPs_freqandhwe_table$Bin_kb=cut(Basu_Tharus_allSNPs_freqandhwe_table$bp_coordinate, bins)
#add chromosome number to each Bin_kb value too for precise locus 
Basu_Tharus_allSNPs_freqandhwe_table$Bin_kb_Wchromosome=paste(Basu_Tharus_allSNPs_freqandhwe_table$CHR.x, Basu_Tharus_allSNPs_freqandhwe_table$Bin_kb)
#make column with average F and Z value for each bp bin
library(dplyr)
Basu_Tharus_allSNPs_freqandhwe_table=Basu_Tharus_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_F_average = mean(F_score))
Basu_Tharus_allSNPs_freqandhwe_table=Basu_Tharus_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_average = mean(Z_score))
#make freqtable
library(dplyr)
Basu_Tharus_allSNPs_freqandhwe_freq_table <- Basu_Tharus_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr:: summarise(Freq=n())
colnames(Basu_Tharus_allSNPs_freqandhwe_freq_table)=c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_freq")
#merge freqtable back with original 
Basu_Tharus_allSNPs_freqandhwe_table=merge(Basu_Tharus_allSNPs_freqandhwe_table, Basu_Tharus_allSNPs_freqandhwe_freq_table, all.x=TRUE, by="Bin_kb_Wchromosome")
#add proportion of Bin_kb with Z score over 1.96
Basu_Tharus_allSNPs_freqandhwe_table=Basu_Tharus_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_prop_sighigh = sum(Z_score>1.96)/Bin_kb_Wchromosome_freq)
#make aggregated table sTharuwing Tharus' bin_kb, bin_kb_F_avergae, bin_kb_Z_average, freq
Basu_Tharus_aggregated_table=unique(Basu_Tharus_allSNPs_freqandhwe_table[c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_F_average", "Bin_kb_Wchromosome_Z_average", "Bin_kb_Wchromosome_freq", "Bin_kb_Wchromosome_Z_prop_sighigh")])
#SLC24A5 is between base pair 46,200,461 to base pair 46,221,881 --> i.e. in bin 46,000,000 - 46,400,000 or 15 4.6e+07,4.64e+07, so find average F value for this genomic region
Tharu_SLC24A5_averageF=Basu_Tharus_aggregated_table$Bin_kb_Wchromosome_F_average[which(Basu_Tharus_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Tharu_SLC24A5_F_comparison=nrow(subset(Basu_Tharus_aggregated_table, Bin_kb_Wchromosome_F_average>Tharu_SLC24A5_averageF))/nrow(Basu_Tharus_aggregated_table)
#-->no evidence of excess homo around SLC2415: 56.0% of genomic regions have higher F than Tharus
#find proportion of genomic regions with average Z value higher than that of SLC24A5 region
Tharu_SLC24A5_averageZ=Basu_Tharus_aggregated_table$Bin_kb_Wchromosome_Z_average[which(Basu_Tharus_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Tharu_SLC24A5_Z_comparison=nrow(subset(Basu_Tharus_aggregated_table, Bin_kb_Wchromosome_Z_average>Tharu_SLC24A5_averageZ))/nrow(Basu_Tharus_aggregated_table)
#-->no evidence of excess homo around SLC2415:56%


#TripuriS
Basu_Tripuris_allSNPs_hwe_table=read.table("Basu_Tripuri_allSNPs_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Tripuris_allSNPs_freq_table=read.table("Basu_Tripuri_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_Tripuris_allSNPs_freq_table=subset(Basu_Tripuris_allSNPs_freq_table, MAF>0)
Basu_Tripuris_allSNPs_freqandhwe_table=merge(Basu_Tripuris_allSNPs_freq_table, Basu_Tripuris_allSNPs_hwe_table, all.x=TRUE)
#then make F score column
Basu_Tripuris_allSNPs_freqandhwe_table$F_score=1-(Basu_Tripuris_allSNPs_freqandhwe_table$O.HET./Basu_Tripuris_allSNPs_freqandhwe_table$E.HET.)
#add MAF freq bins
bins <- seq(0, 1, by=0.05)
Basu_Tripuris_allSNPs_freqandhwe_table$Bin_MAF=cut(Basu_Tripuris_allSNPs_freqandhwe_table$MAF, bins)
#now convert each SNP's F score to Z score 
library(dplyr)
Basu_Tripuris_allSNPs_freqandhwe_table=Basu_Tripuris_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_average = mean(F_score))
Basu_Tripuris_allSNPs_freqandhwe_table=Basu_Tripuris_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_sd = sd(F_score))
Basu_Tripuris_allSNPs_freqandhwe_table$Z_score=(Basu_Tripuris_allSNPs_freqandhwe_table$F_score-Basu_Tripuris_allSNPs_freqandhwe_table$Bin_F_average)/Basu_Tripuris_allSNPs_freqandhwe_table$Bin_F_sd
#load bim file to get genetic position of each SNP
Basu_Tripuris_bim=read.table("Basu_Tripuris.bim", sep="", stringsAsFactors = FALSE)
colnames(Basu_Tripuris_bim)=c("CHR", "SNP", "position_cm", "bp_coordinate", "A1", "A2")
Basu_Tripuris_allSNPs_freqandhwe_table=merge(Basu_Tripuris_allSNPs_freqandhwe_table, Basu_Tripuris_bim, all.x=TRUE, by.x="SNP",by.y = "SNP", memory.limit(size=2500000))
#make Bin_kb for 400kb windows over genome 
max(Basu_Tripuris_allSNPs_freqandhwe_table$bp_coordinate)
bins <- seq(0, 400000000, by=400000)
Basu_Tripuris_allSNPs_freqandhwe_table$Bin_kb=cut(Basu_Tripuris_allSNPs_freqandhwe_table$bp_coordinate, bins)
#add chromosome number to each Bin_kb value too for precise locus 
Basu_Tripuris_allSNPs_freqandhwe_table$Bin_kb_Wchromosome=paste(Basu_Tripuris_allSNPs_freqandhwe_table$CHR.x, Basu_Tripuris_allSNPs_freqandhwe_table$Bin_kb)
#make column with average F and Z value for each bp bin
library(dplyr)
Basu_Tripuris_allSNPs_freqandhwe_table=Basu_Tripuris_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_F_average = mean(F_score))
Basu_Tripuris_allSNPs_freqandhwe_table=Basu_Tripuris_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_average = mean(Z_score))
#make freqtable
library(dplyr)
Basu_Tripuris_allSNPs_freqandhwe_freq_table <- Basu_Tripuris_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr:: summarise(Freq=n())
colnames(Basu_Tripuris_allSNPs_freqandhwe_freq_table)=c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_freq")
#merge freqtable back with original 
Basu_Tripuris_allSNPs_freqandhwe_table=merge(Basu_Tripuris_allSNPs_freqandhwe_table, Basu_Tripuris_allSNPs_freqandhwe_freq_table, all.x=TRUE, by="Bin_kb_Wchromosome")
#add proportion of Bin_kb with Z score over 1.96
Basu_Tripuris_allSNPs_freqandhwe_table=Basu_Tripuris_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_prop_sighigh = sum(Z_score>1.96)/Bin_kb_Wchromosome_freq)
#make aggregated table sTripuriwing Tripuris' bin_kb, bin_kb_F_avergae, bin_kb_Z_average, freq
Basu_Tripuris_aggregated_table=unique(Basu_Tripuris_allSNPs_freqandhwe_table[c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_F_average", "Bin_kb_Wchromosome_Z_average", "Bin_kb_Wchromosome_freq", "Bin_kb_Wchromosome_Z_prop_sighigh")])
#SLC24A5 is between base pair 46,200,461 to base pair 46,221,881 --> i.e. in bin 46,000,000 - 46,400,000 or 15 4.6e+07,4.64e+07, so find average F value for this genomic region
Tripuri_SLC24A5_averageF=Basu_Tripuris_aggregated_table$Bin_kb_Wchromosome_F_average[which(Basu_Tripuris_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Tripuri_SLC24A5_F_comparison=nrow(subset(Basu_Tripuris_aggregated_table, Bin_kb_Wchromosome_F_average>Tripuri_SLC24A5_averageF))/nrow(Basu_Tripuris_aggregated_table)
#-->no evidence of excess homo around SLC2415: 38.1% of genomic regions have higher F than Tripuris
#find proportion of genomic regions with average Z value higher than that of SLC24A5 region
Tripuri_SLC24A5_averageZ=Basu_Tripuris_aggregated_table$Bin_kb_Wchromosome_Z_average[which(Basu_Tripuris_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
Tripuri_SLC24A5_Z_comparison=nrow(subset(Basu_Tripuris_aggregated_table, Bin_kb_Wchromosome_Z_average>Tripuri_SLC24A5_averageZ))/nrow(Basu_Tripuris_aggregated_table)
#-->no evidence of excess homo around SLC2415:38%


#WestBengalBrahminS
Basu_WestBengalBrahmins_allSNPs_hwe_table=read.table("Basu_WestBengalBrahmin_allSNPs_HWE_output.hwe", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_WestBengalBrahmins_allSNPs_freq_table=read.table("Basu_WestBengalBrahmin_allSNPs_freq_output.frq", sep="", stringsAsFactors = FALSE, header = TRUE)
Basu_WestBengalBrahmins_allSNPs_freq_table=subset(Basu_WestBengalBrahmins_allSNPs_freq_table, MAF>0)
Basu_WestBengalBrahmins_allSNPs_freqandhwe_table=merge(Basu_WestBengalBrahmins_allSNPs_freq_table, Basu_WestBengalBrahmins_allSNPs_hwe_table, all.x=TRUE)
#then make F score column
Basu_WestBengalBrahmins_allSNPs_freqandhwe_table$F_score=1-(Basu_WestBengalBrahmins_allSNPs_freqandhwe_table$O.HET./Basu_WestBengalBrahmins_allSNPs_freqandhwe_table$E.HET.)
#add MAF freq bins
bins <- seq(0, 1, by=0.05)
Basu_WestBengalBrahmins_allSNPs_freqandhwe_table$Bin_MAF=cut(Basu_WestBengalBrahmins_allSNPs_freqandhwe_table$MAF, bins)
#now convert each SNP's F score to Z score 
library(dplyr)
Basu_WestBengalBrahmins_allSNPs_freqandhwe_table=Basu_WestBengalBrahmins_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_average = mean(F_score))
Basu_WestBengalBrahmins_allSNPs_freqandhwe_table=Basu_WestBengalBrahmins_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_MAF) %>% dplyr::  mutate (Bin_F_sd = sd(F_score))
Basu_WestBengalBrahmins_allSNPs_freqandhwe_table$Z_score=(Basu_WestBengalBrahmins_allSNPs_freqandhwe_table$F_score-Basu_WestBengalBrahmins_allSNPs_freqandhwe_table$Bin_F_average)/Basu_WestBengalBrahmins_allSNPs_freqandhwe_table$Bin_F_sd
#load bim file to get genetic position of each SNP
Basu_WestBengalBrahmins_bim=read.table("Basu_WestBengalBrahmins.bim", sep="", stringsAsFactors = FALSE)
colnames(Basu_WestBengalBrahmins_bim)=c("CHR", "SNP", "position_cm", "bp_coordinate", "A1", "A2")
Basu_WestBengalBrahmins_allSNPs_freqandhwe_table=merge(Basu_WestBengalBrahmins_allSNPs_freqandhwe_table, Basu_WestBengalBrahmins_bim, all.x=TRUE, by.x="SNP",by.y = "SNP", memory.limit(size=2500000))
#make Bin_kb for 400kb windows over genome 
max(Basu_WestBengalBrahmins_allSNPs_freqandhwe_table$bp_coordinate)
bins <- seq(0, 400000000, by=400000)
Basu_WestBengalBrahmins_allSNPs_freqandhwe_table$Bin_kb=cut(Basu_WestBengalBrahmins_allSNPs_freqandhwe_table$bp_coordinate, bins)
#add chromosome number to each Bin_kb value too for precise locus 
Basu_WestBengalBrahmins_allSNPs_freqandhwe_table$Bin_kb_Wchromosome=paste(Basu_WestBengalBrahmins_allSNPs_freqandhwe_table$CHR.x, Basu_WestBengalBrahmins_allSNPs_freqandhwe_table$Bin_kb)
#make column with average F and Z value for each bp bin
library(dplyr)
Basu_WestBengalBrahmins_allSNPs_freqandhwe_table=Basu_WestBengalBrahmins_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_F_average = mean(F_score))
Basu_WestBengalBrahmins_allSNPs_freqandhwe_table=Basu_WestBengalBrahmins_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_average = mean(Z_score))
#make freqtable
library(dplyr)
Basu_WestBengalBrahmins_allSNPs_freqandhwe_freq_table <- Basu_WestBengalBrahmins_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr:: summarise(Freq=n())
colnames(Basu_WestBengalBrahmins_allSNPs_freqandhwe_freq_table)=c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_freq")
#merge freqtable back with original 
Basu_WestBengalBrahmins_allSNPs_freqandhwe_table=merge(Basu_WestBengalBrahmins_allSNPs_freqandhwe_table, Basu_WestBengalBrahmins_allSNPs_freqandhwe_freq_table, all.x=TRUE, by="Bin_kb_Wchromosome")
#add proportion of Bin_kb with Z score over 1.96
Basu_WestBengalBrahmins_allSNPs_freqandhwe_table=Basu_WestBengalBrahmins_allSNPs_freqandhwe_table %>% dplyr:: group_by(Bin_kb_Wchromosome) %>% dplyr::  mutate (Bin_kb_Wchromosome_Z_prop_sighigh = sum(Z_score>1.96)/Bin_kb_Wchromosome_freq)
#make aggregated table sWestBengalBrahminwing WestBengalBrahmins' bin_kb, bin_kb_F_avergae, bin_kb_Z_average, freq
Basu_WestBengalBrahmins_aggregated_table=unique(Basu_WestBengalBrahmins_allSNPs_freqandhwe_table[c("Bin_kb_Wchromosome", "Bin_kb_Wchromosome_F_average", "Bin_kb_Wchromosome_Z_average", "Bin_kb_Wchromosome_freq", "Bin_kb_Wchromosome_Z_prop_sighigh")])
#SLC24A5 is between base pair 46,200,461 to base pair 46,221,881 --> i.e. in bin 46,000,000 - 46,400,000 or 15 4.6e+07,4.64e+07, so find average F value for this genomic region
WestBengalBrahmin_SLC24A5_averageF=Basu_WestBengalBrahmins_aggregated_table$Bin_kb_Wchromosome_F_average[which(Basu_WestBengalBrahmins_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average F value higher than that of SLC24A5 region
WestBengalBrahmin_SLC24A5_F_comparison=nrow(subset(Basu_WestBengalBrahmins_aggregated_table, Bin_kb_Wchromosome_F_average>WestBengalBrahmin_SLC24A5_averageF))/nrow(Basu_WestBengalBrahmins_aggregated_table)
#-->no evidence of excess homo around SLC2415: 97.1% of genomic regions have higher F than WestBengalBrahmins
#find proportion of genomic regions with average Z value higher than that of SLC24A5 region
WestBengalBrahmin_SLC24A5_averageZ=Basu_WestBengalBrahmins_aggregated_table$Bin_kb_Wchromosome_Z_average[which(Basu_WestBengalBrahmins_aggregated_table$Bin_kb_Wchromosome=="15 (4.6e+07,4.64e+07]")]
#find proportion of genomic regions with average Z value less than that of SLC24A5 region
WestBengalBrahmin_SLC24A5_Z_comparison=nrow(subset(Basu_WestBengalBrahmins_aggregated_table, Bin_kb_Wchromosome_Z_average<WestBengalBrahmin_SLC24A5_averageZ))/nrow(Basu_WestBengalBrahmins_aggregated_table)
#-->no evidence of excess homo around SLC2415:97%

#export table of Z comparisons for results section of diss
Population=c("Gond", "Gujurati Brahmin", "Gujjar", "Ho", "Irula","Iyer", "Jamatia", "Khatri", "Kadar", "Kamboj", "Korwa", "Manipuri Brahmin", "Maratha", "Pallan", "Paniya", "Tripuri", "West Bengal Brahmin")
Z_comp_score=c(Gond_SLC24A5_Z_comparison, GujuratiBrahmin_SLC24A5_Z_comparison, Gujjar_SLC24A5_Z_comparison, Ho_SLC24A5_Z_comparison, Irula_SLC24A5_Z_comparison,Iyer_SLC24A5_Z_comparison, Jamatia_SLC24A5_Z_comparison, Khatri_SLC24A5_Z_comparison, Kadar_SLC24A5_Z_comparison, Kamboj_SLC24A5_Z_comparison, Korwa_SLC24A5_Z_comparison, ManipuriBrahmin_SLC24A5_Z_comparison, Maratha_SLC24A5_Z_comparison, Pallan_SLC24A5_Z_comparison, Paniya_SLC24A5_Z_comparison, Tripuri_SLC24A5_Z_comparison, WestBengalBrahmin_SLC24A5_Z_comparison)
Z_score=c(Gond_SLC24A5_averageZ, GujuratiBrahmin_SLC24A5_averageZ, Gujjar_SLC24A5_averageZ, Ho_SLC24A5_averageZ, Irula_SLC24A5_averageZ,Iyer_SLC24A5_averageZ, Jamatia_SLC24A5_averageZ, Khatri_SLC24A5_averageZ, Kadar_SLC24A5_averageZ, Kamboj_SLC24A5_averageZ, Korwa_SLC24A5_averageZ, ManipuriBrahmin_SLC24A5_averageZ, Maratha_SLC24A5_averageZ, Pallan_SLC24A5_averageZ, Paniya_SLC24A5_averageZ, Tripuri_SLC24A5_averageZ, WestBengalBrahmin_SLC24A5_averageZ)
inbreedingaroundSLCtable=data.frame(Population, Z_score, Z_comp_score)
colnames(inbreedingaroundSLCtable)=c("Population", "Z score of genomic region containing SLC24A5", "Proportion of genomic regions with Z score greater than that of region containing SLC24A5")
write.csv(inbreedingaroundSLCtable, "inbreedingaroundSLCtable.csv")


#further analyses number 3: testing how much each variant is associated with population structure using pcadapt 
library(pcadapt)
#load bim file of Basu and Pathak collective samples 
Basu_Pathak_collective_full_samples_bed=read.pcadapt("Basu_Pathak_merged3.bed", type="bed")
Basu_Pathak_collective_popstruct_analysis=pcadapt(input=Basu_Pathak_collective_full_samples_bed, K=20)
#plot and export screeplot pre SNPthinning
plot(Basu_Pathak_collective_popstruct_analysis, option="screeplot")
#--> looks like 4 first principal components ascertain population structure 
#do score plot 
poplist.names = c(rep("Maratha", 7), rep("West Bengal Brahmin", 18), rep("Birhor", 16), rep("Gujurati Brahmin", 20), rep("Gond", 20), rep("Ho",18), rep("Iyer",20), rep("Jamatia", 18), rep("Jarawa", 19), rep("Kadar", 20), rep("Paniya",18), rep("Pallan",20), rep("Khatri",19), rep("Tripuri",19), rep("Onge", 17), rep("Korwa", 18), rep("Tharu",20), rep("Santal", 20), rep("Irula",20), rep("Manipuri Brahmin",20), rep("Gujjar",15), rep("Kamboj",14), rep("Ror", 15))
plot(Basu_Pathak_collective_popstruct_analysis, option = "scores", pop = poplist.names)
plot(Basu_Pathak_collective_popstruct_analysis, option = "scores", i=3, j=4, pop = poplist.names)
plot(Basu_Pathak_collective_popstruct_analysis, option = "scores", i=4, j=5, pop = poplist.names)
plot(Basu_Pathak_collective_popstruct_analysis, option = "scores", i=5, j=6, pop = poplist.names)
plot(Basu_Pathak_collective_popstruct_analysis, option = "scores", i=7, j=8, pop = poplist.names)
plot(Basu_Pathak_collective_popstruct_analysis, option = "scores", i=16, j=17, pop = poplist.names)
plot(Basu_Pathak_collective_popstruct_analysis, option = "scores", i=22, j=23, pop = poplist.names)
#-->shows that after K=4, much less explanation of population structure, but still certain clusters do show 
#now compute test statistic 
Basu_Pathak_collective_popstruct_analysis = pcadapt(Basu_Pathak_collective_full_samples_bed, K=5, min.maf = 0.01)
summary(Basu_Pathak_collective_popstruct_analysis)
#hist of p values
hist(Basu_Pathak_collective_popstruct_analysis$pvalues, xlab = "p-values", main = NULL, breaks = 50, col = "orange")
#--> shows excess of small and large p values: evidence of outliers 
#finding cut-off for outlier with Bonferroni correction
padj <- p.adjust(Basu_Pathak_collective_popstruct_analysis$pvalues,method="bonferroni")
alpha <- 0.1
outliers <- which(padj < alpha)
length(outliers)
#evaluate if LD may be issue in dataset by displaying loadings through visualisations
Loadingsplot1=plot(Basu_Pathak_collective_popstruct_analysis$loadings[, 1], pch=19, cex=0.3, ylab = paste0("Loadings PC 1"), ylim=c(-0.015, 0.015))
Loadingsplot2=plot(Basu_Pathak_collective_popstruct_analysis$loadings[, 2], pch=19, cex=0.3, ylab = paste0("Loadings PC 2"), ylim=c(-0.015, 0.015))
Loadingsplot3=plot(Basu_Pathak_collective_popstruct_analysis$loadings[, 3], pch=19, cex=0.3, ylab = paste0("Loadings PC 3"), ylim=c(-0.015, 0.015))
tiff("Loadingsplot4.tff", units = "px", compression="lzw", res=96)
#--> Loadingsplot1 demonstrates that PC1 is determined by single genomic region, which is likely to be due to LD, so should undertake LD pruning
Basu_Pathak_collective_popstruct_analysis <- pcadapt(Basu_Pathak_collective_full_samples_bed, K = 20, LD.clumping = list(size = 500, thr = 0.1))
#plot and export post SNP thinning plot
plot(Basu_Pathak_collective_popstruct_analysis, option = "screeplot")
#--> after SNP thinning, choose K=3

#with new pcadapt post-thinning, run analyses again:
Basu_Pathak_collective_popstruct_analysis <- pcadapt(Basu_Pathak_collective_full_samples_bed, K = 3, LD.clumping = list(size = 500, thr = 0.1))
Loadingsplot_postthinning_1=plot(Basu_Pathak_collective_popstruct_analysis$loadings[, 1], pch=19, cex=0.3, ylab = paste0("Loadings PC 1"), ylim=c(-0.015, 0.015))
Loadingsplot_postthinning_2=plot(Basu_Pathak_collective_popstruct_analysis$loadings[, 2], pch=19, cex=0.3, ylab = paste0("Loadings PC 2"), ylim=c(-0.015, 0.015))
Loadingsplot_postthinning_3=plot(Basu_Pathak_collective_popstruct_analysis$loadings[, 3], pch=19, cex=0.3, ylab = paste0("Loadings PC 3"), ylim=c(-0.015, 0.015))
#run summary stats:
summary(Basu_Pathak_collective_popstruct_analysis)
Basu_Pathak_collective_popstruct_analysis$logpvalues=-log10(Basu_Pathak_collective_popstruct_analysis$pvalues)
#finding cut-off for outliers with Bonferroni correction again
padj <- p.adjust(Basu_Pathak_collective_popstruct_analysis$pvalues,method="bonferroni")
alpha <- 0.1
outliers <- which(padj < alpha)
length(outliers)
#can now redo and plot PCA now we've done LD pruning and chose K=4
poplist.names = c(rep("Maratha", 7), rep("West Bengal Brahmin", 18), rep("Birhor", 16), rep("Gujurati Brahmin", 20), rep("Gond", 20), rep("Ho",18), rep("Iyer",20), rep("Jamatia", 18), rep("Jarawa", 19), rep("Kadar", 20), rep("Paniya",18), rep("Pallan",20), rep("Khatri",19), rep("Tripuri",19), rep("Onge", 17), rep("Korwa", 18), rep("Tharu",20), rep("Santal", 20), rep("Irula",20), rep("Manipuri Brahmin",20), rep("Gujjar",15), rep("Kamboj",14), rep("Ror", 15))
PC1and2_postthinning=plot(Basu_Pathak_collective_popstruct_analysis, option = "scores", pop = poplist.names)
PC3and4_postthinnnig=plot(Basu_Pathak_collective_popstruct_analysis, option = "scores", i=3, j=4, pop = poplist.names)
#rs1426654 is SNP 732761, check if this SNP is an "outlier"
732761 %in% outliers
##see association between PCs and outliers
PC_outlier = get.pc(Basu_Pathak_collective_popstruct_analysis, outliers)

#now look at and export the manhatten plot which shows regions involved in adaptation
outliernumbers=c(outliers) 
outlierpvalues=c(-log10(Basu_Pathak_collective_popstruct_analysis$pvalues[outliers]))
outliercoords=cbind(outliernumbers, outlierpvalues)
library(tidyverse)
library(lubridate)
outliercoords<-as.data.frame(outliercoords)

Basu_Pathak_passvector=Basu_Pathak_collective_popstruct_analysis$pass
Basu_Pathak_pvaluesvector=Basu_Pathak_collective_popstruct_analysis$logpvalues[Basu_Pathak_collective_popstruct_analysis$pass]
Basu_Pathak_pass_pvalues=cbind(Basu_Pathak_passvector, Basu_Pathak_pvaluesvector)
library(tidyverse)
library(lubridate)
Basu_Pathak_pass_pvalues<-as.data.frame(Basu_Pathak_pass_pvalues)

library(ggplot2)
Manhattanplot_final = plot(Basu_Pathak_collective_popstruct_analysis, option="manhattan", plt.pkg = "ggplot") 
Manhattanplot_final + geom_point(data=Basu_Pathak_pass_pvalues, x=Basu_Pathak_passvector, y=Basu_Pathak_pvaluesvector, aes(color="Non-significant SNPs")) + geom_point(data=outliercoords,x=outliernumbers, y=outlierpvalues, aes(color="Significant SNPs")) + geom_point(data=outliercoords[outliercoords$outliernumbers=="732761",],x=732761, y=21.61494, aes(color="rs1426654 SNP"))










